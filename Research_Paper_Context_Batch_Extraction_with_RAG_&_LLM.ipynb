{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODYYXWM1MMOeT8i/A97TvH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuHsin-Chen1221/Large-Language-Model-Agents/blob/main/Research_Paper_Context_Batch_Extraction_with_RAG_%26_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JZvpd7ix8W3",
        "outputId": "fea3cc21-8d46-4992-d507-d2fd13023be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U langchain-community langchain_google_genai pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5631DBYTyfrn",
        "outputId": "34c3f43d-d9a2-45e4-9010-15d2968107fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.1.8)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.70)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.6.18)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "pdf_folder = '/content/drive/MyDrive/PNPL'\n",
        "docs = []\n",
        "\n",
        "for filename in os.listdir(pdf_folder):\n",
        "    if filename.endswith('.pdf'):\n",
        "        loader = PyMuPDFLoader(os.path.join(pdf_folder, filename))\n",
        "        docs.extend(loader.load())\n"
      ],
      "metadata": {
        "id": "G2NUoTl_yPTM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR-ys7Dizw7d",
        "outputId": "5ccc3b5c-0090-4bf7-ec41-4b33167144cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 0}, page_content='ARTIFICIAL NEURAL NETWORKS FOR\\nMAGNETOENCEPHALOGRAPHY: A REVIEW OF AN EMERGING\\nFIELD\\nA PREPRINT\\nArthur Dehgan\\nMILA, CoCo Lab\\nUniversité de Montreal\\ndehganar@mila.quebec\\n(Corresponding author)\\nHamza Abdelhedi\\nMILA, CoCo Lab\\nUniversité de Montreal\\nhamza.abdelhedi@umontreal.ca\\nVanessa Hadid\\nMcGill University Health Centre\\nMcGill University\\nvanessa.hadid@mcgill.ca\\nIrina Rish\\nMILA\\nUniversité de Montreal\\nrish@iro.umontreal.ca\\nKarim Jerbi\\nMILA, CoCo Lab\\nUniversité de Montreal\\nkarim.jerbi@umontreal.ca\\nMay 20, 2025\\nABSTRACT\\nObjective: Magnetoencephalography (MEG) is a cutting-edge neuroimaging technique that measures\\nthe intricate brain dynamics underlying cognitive processes with an unparalleled combination of\\nhigh temporal and spatial precision. While MEG data analytics have traditionally relied on advanced\\nsignal processing and mathematical and statistical tools, the recent surge in artificial intelligence\\n(AI) has led to the growing use of machine learning (ML) methods for MEG data classification.\\nAn emerging trend in this field is the use of artificial neural networks (ANNs) to address various\\nMEG-related tasks. This review aims to provide a comprehensive overview of the state of the art in\\nthis area. Approach: This topical review included studies that applied ANNs to MEG data. Studies\\nwere sourced from PubMed, Google Scholar, arXiv, and bioRxiv using targeted search queries. The\\nincluded studies were categorized into three groups: ’Classification’, ’Modeling’, and ’Other’. Key\\nfindings and trends were summarized to provide a comprehensive assessment of the field.Main results:\\nWe identified 119 relevant studies, with 70 focused on ’Classification’, 16 on ’Modeling’, and 33\\nin the ’Other’ category. ’Classification’ studies addressed tasks such as brain decoding, clinical\\ndiagnostics, and BCI implementations, often achieving high predictive accuracy. ’Modeling’ studies\\nexplored the alignment between ANN activations and brain processes, offering insights into the\\nneural representations captured by these networks. The ’Other’ category demonstrated innovative\\nuses of ANNs for artifact correction, preprocessing, and neural source localization. Significance: By\\nestablishing a detailed portrait of the current state of the field, this review highlights the strengths and\\ncurrent limitations of ANNs in MEG research. It also provides practical recommendations for future\\nwork, offering a helpful reference for seasoned researchers and newcomers interested in using ANNs\\nto explore the complex dynamics of the human brain with MEG.\\nKeywords Artificial neural networks (ANNs)- Magnetoencephalography (MEG) - Deep learning (DL) - Brain imaging,\\nMachine learning (ML) - Brain decoding\\narXiv:2501.11566v4  [q-bio.NC]  19 May 2025'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 1}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging field\\nA PREPRINT\\n1\\nIntroduction\\nArtificial neural networks (ANNs) have had an enormous impact on most research fields, especially in computer vision\\nand natural language processing ([1]). In recent years, ANNs have become increasingly used for brain data analyses and\\nmodeling, yielding ample evidence for their added value in neuroscience research ([2, 3]). Deep learning (DL) has been\\napplied in the analysis of various types of brain imaging modalities, including structural magnetic iesonance imaging\\n(MRI), functional magnetic resonance imaging (fMRI), electroencephalography (EEG), and magnetoencephalography\\n(MEG). While using ANNs specifically with EEG has been the subject of comprehensive reviews (e.g. [4, 5]), using\\nANNs with MEG data is less common, and the current state-of-the-art has yet to be surveyed. This combination,\\nhowever, is of growing interest. MEG provides direct, time-resolved recordings of brain activity, which can be used to\\ncompare the dynamics and internal representations of ANN models to those observed in the brain. For ANN researchers,\\nthis opens up possibilities for testing model outputs against high-temporal-resolution biological signals and for training\\nmodels that simulate aspects of cognitive processing. For MEG researchers, ANNs offer powerful tools to classify\\ndifferent brain states, uncover hidden patterns in brain activity, and model the complex structure of MEG datasets. These\\nshared opportunities have led to an increasing number of studies exploring how MEG and ANNs can be integrated in\\npractice. The present review seeks to fill in this gap by providing a detailed survey of the main applications of ANNs in\\nMEG research to date. The strengths and limitations of the approaches reviewed allow us to highlight the potential of\\nANNs in future MEG research and the main challenges in this field.\\n1.1\\nMagnetoencephalography and brain imaging\\nMEG is a non-invasive electrophysiological brain imaging technique that detects magnetic signals generated by\\nsynchronized neuronal ensembles in the brain, typically recorded with approximately 100–300 sensors depending on\\nthe system. Although it shares similarities with EEG, such as electrophysiological signal origins and millisecond-range\\ntemporal resolution, MEG offers distinct advantages over EEG and other modalities like fMRI. Unlike EEG, MEG\\nis less susceptible to spatial smearing from variations in conductivity across brain tissues and the skull, due to its\\nsensitivity to magnetic permittivity changes, which are relatively homogeneous in the brain ([6]). This makes MEG\\nparticularly well-suited for source reconstruction and analyzing spatio-temporal brain dynamics in cortical source space\\n([7, 8]). In contrast to fMRI, which measures hemodynamic responses only indirectly reflecting neural activity with a\\ntemporal resolution of several seconds, MEG directly captures electrophysiological activity on a millisecond scale ([9]).\\nOverall, with its high temporal resolution and improved spatial precision over EEG, MEG excels in studying complex\\nbrain dynamics, despite challenges like field spread that source reconstruction mitigates but does not fully resolve ([10]).\\nBoth EEG and MEG produce sensor-level data that are highly intercorrelated and have limited spatial interpretability.\\nThese ambiguities can be reduced with MEG by applying source localization techniques ([8, 10]). Compared to fMRI,\\nanother widely used functional neuroimaging technique, MEG’s millisecond-scale resolution provides a significant\\nadvantage for capturing rapid neural events, whereas fMRI’s lower sampling rate limits its temporal precision ([9]).\\nAlthough source reconstruction does not entirely resolve the field spread problem or linear mixing issue, it facilitates\\nexploration of the functional role of specific brain areas with more anatomical precision than sensor-level analysis\\n([11]).\\nGiven its high temporal and spatial resolution, MEG has become an established imaging modality particularly suited\\nfor addressing neuroscientific inquiries involving intricate spatial, temporal, and spectral patterns of distributed brain\\ndynamics. These include the real-time integration of information from one or multiple modalities and the neural\\ndynamics underpinning advanced cognitive functions such as language processing and decision-making. For a complete\\noverview of the advantages and drawbacks of MEG in neuroscience, see [12].\\n1.2\\nStandard M/EEG analysis pipeline\\nGenerally, the M/EEG analysis pipeline consists of a sequence of processing steps that start from the sensor-level\\nraw data and, after a series of manipulations, ultimately yield observations that address a research question. Standard\\npreprocessing pipelines often include data cleaning and artifact rejection procedures, including channel rejection,\\nnotch-filtering, and independent component analysis (ICA [13]).\\nM/EEG recordings yield rich and high-dimensional data, and typically, the analysis pipeline involves computing a set of\\nvariables from the raw data, also known as hand-crafted feature extraction in the case of ML analysis. Computing these\\nvariables (or features) relies on processing pipelines that can be broadly divided into two main categories: time-domain\\nor frequency-domain analyses (which can also be jointly explored using time-frequency representations of the data\\n([14, 15]).\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 2}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging field\\nA PREPRINT\\nTime domain analyses typically involve the computation of features such as event-related potentials (ERPs [16]),\\nlong-range temporal correlations (LRTC) using detrended fluctuation analysis (DFA [17] ), complexity measures (e.g.\\nHjorth parameters [18], Hurst exponent [19], fractality measures (fractal dimension) or entropy measures (approximate\\nentropy [20], sample entropy [21], permutation entropy [22]). Frequency-domain analyses typically employ spectral\\nanalysis methods, such as the wavelet ([23]) or Fourier transform ([24, 14]), to derive features like the spectral power\\nacross canonical frequency bands (e.g. delta, theta, alpha, beta, gamma).\\nIn time- or frequency-domain analyses, connectivity features can be calculated using multi-channel or whole-brain\\nsource data. Typical connectivity metrics assess different associations between signals from various sources or sensors.\\nThese metrics include magnitude-squared coherence (MSCoh [25]), phase locking value (PLV [26]), phase-lag index\\n(PLI [27]), and weighted PLI (wPLI [28]), to name but a few. Different features capture complementary -although\\nsometimes partly overlapping- aspects of the neural dynamics associated with various behavioral states.\\nSensor-level analyses in neuroscience are informative, but source reconstruction is essential for improving the inter-\\npretation of the spatial distribution of significant effects. This step involves defining a forward model and solving the\\ninverse problem ( [29] ). Effective source reconstruction allows for precise identification of neural markers in specific\\nbrain regions, enhancing our understanding of complex brain functions and informing both hypothesis validation and\\nexploratory data-driven research.\\n1.3\\nMachine learning versus inferential statistics\\nA typical M/EEG neuroscience study employs either inferential statistics, using hypothesis testing methods such\\nas t-tests or ANOVA, or statistical learning approaches, notably machine learning (ML). The choice between these\\napproaches depends on the specific research questions, with inferential statistics often used to test predefined hypotheses\\nabout neural activity, while ML is typically applied to explore complex patterns and predictions from high-dimensional\\ndatasets. Both approaches have significantly contributed to progress in neuroscience, as detailed in [30]. Additionally,\\nthe bayesian framework has emerged as a complementary avenue in M/EEG analysis ([31]), allowing hypothesis testing\\nfrom a probabilistic perspective. While hypothesis testing, whether conducted through traditional methods or bayesian\\napproaches, has long been a staple in neuroscience, it remains the subject of ongoing debate and scrutiny ([32]). Despite\\nits undeniable strength and utility, null hypothesis significance testing (NHST) has several limitations. First, it heavily\\nrelies on a priori choices guided by literature and expert domain knowledge. Additionally, it assumes generalization\\nof findings based solely on the representativeness of the studied sample to the target population. As an alternative\\napproach to examining hand-picked variables informed by the literature, MEG researchers have begun to use a variety\\nof data-driven approaches. These broadly include two types of approaches to relevant feature identification with hardly\\nany a priori information: (a) Feeding a large number of hand-crafted features into a decoding framework and assessing\\ntheir relative contributions to model performance, and (b) Learning the relevant features from the data themselves, a\\nform of data-driven automatic feature extraction known as representation learning. Unlike simpler algorithms such\\nas support vector machines (SVM), linear discriminant analysis (LDA), or decision trees which generally rely on\\nhand-crafted features, ANNs can automatically learn and extract relevant features from data. Despite its conceptual\\nappeal, representation learning comes with its limitations, in particular when it comes to interpreting the learned features.\\nANNs are often criticized for being ’black boxes’ compared to more classical ML algorithms, which use hand-crafted\\nfeatures as inputs. Enhancing the feature interpretability is an active research topic in deep representation learning.\\n1.4\\nArtificial neural networks in a nutshell\\nUnderstanding ANNs begins with grasping the concept of linear regression, where the output y is given by y = wx + b,\\nwith w representing the weight, x the input, and b the bias. In ANNs, this equation gets extended by an activation\\nfunction f, transforming it to y = f(wx + b) ([33, 34]). As the neural network gains depth through additional layers,\\nthese equations combine to form increasingly complex mathematical models. A neural network’s complexity, or capacity\\n([35]), indicates its ability to model intricate functions, but this can be a double-edged sword. High capacity often leads\\nto overfitting ([35]), where the model learns the training data too well, capturing noise and inaccuracies, thus performing\\npoorly on new data. To mitigate overfitting, regularization techniques such as maxpooling ([36]), which reduces\\nfeature map dimensions, dropout ([37]), which nullifies a subset of neurons during training, and batch normalization\\n([38]), which standardizes layer outputs, can be applied. Another regularization method comes in the form of data\\naugmentation ([39]), where the initial dataset can be enlarged by introducing noise to the data, rotating, or cropping it.\\nLearning in ANNs occurs through optimization, typically employing stochastic gradient descent (SGD [40]). During\\neach training iteration, the model’s parameters are adjusted based on computed gradients from a loss function, facilitated\\nby a technique known as backpropagation ([41]). As we progress to more advanced ANNs, we encounter convolutional\\nneural networks ([42]) optimized for image data, recurrent neural networks ([43]) ideal for sequences, and attention\\nmodels ([44]) that allocate varying degrees of focus on different parts of the input. Another important class includes\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 3}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging field\\nA PREPRINT\\nautoencoders, which are typically unsupervised networks designed to learn efficient data codings. They consist of an\\nencoder compressing the input to a latent representation and a decoder reconstructing the input from this representation,\\naiming to capture essential data features. Autoencoders can utilize various architectures like CNNs or RNNs. Sparse\\nautoencoders (SAEs) are a variant that adds sparsity constraints to encourage compact representations. Overall, the\\ndomain of ANNs extends from fundamental mathematical principles to intricate architectures and sophisticated methods\\nfor effective training.\\nWhile the ANNs described in this review originate from early inspiration in neuroscience, it is important to distinguish\\nthem from biologically detailed models used in computational neuroscience. Models such as integrate-and-fire neurons,\\nHebbian learning mechanisms, spiking neural networks, and neural mass models aim to capture the physiological\\nbehavior of neural systems with varying levels of abstraction. In contrast, the ANNs considered in this review are\\nprimarily optimized for predictive performance and scalability in ML applications. This distinction highlights the\\ndivergence between biologically motivated modeling and task-driven computational approaches — both valuable, but\\ngrounded in different research priorities ([45]).\\n1.5\\nANNs for neuroimaging data\\nRecently, ML has garnered much interest from the neuroscience research community ([46, 47, 48]). While deep learning\\nhas predominantly been applied to fMRI and MRI data due to their compatibility with existing architectures, its use in\\nEEG analysis is also growing, partly due to EEG’s wider availability. However, the unique strengths of MEG, such\\nas its high temporal and spatial resolution, present a compelling case for its integration with advanced techniques\\nlike ANNs. These networks are well-suited to handle MEG’s complex, high-dimensional data. They apply to various\\ntasks, including classification, data modeling, representation learning, data cleaning, and source estimation, thereby\\ncontributing to a more nuanced understanding of brain functions. The use of ANNs to analyze MEG data is gaining\\ntraction, offering novel perspectives and data analyses that standard MEG methodologies typically do not provide.\\nAlthough arguably still in the early stages, exciting progress has already been reported in this emerging field.\\n1.6\\nGoal of this review\\nA growing number of AI researchers are turning to MEG as a rich source of non-invasive brain-wide neural data,\\nrecognizing its high spatio-temporal resolution and ability to capture complex cognitive dynamics as key assets for\\nadvancing more efficient, robust, and brain-inspired neural network models. This review provides a comprehensive\\nsurvey of ANN applications in MEG data analysis, highlighting their benefits and potential added value. These\\ntechniques have proven to be shown to be useful in various neuroscience contexts. By providing examples of\\narchitectures that emulate aspects of visual processing and techniques, we aim to provide an up-to-date overview of\\nthe demonstrated the potential of this approach in ANN-based MEG studies. We hope to expedite researchers’ path to\\ndeveloping their own implementations. More globally, we hope this review will encourage researchers well-versed with\\nMEG to integrate deep learning techniques into their work, and conversely encourage experts in deep learning with an\\ninterest in neuroscience to consider MEG as a particularly promising brain measurement modality in the context of\\nNeuro-AI research applications.\\n2\\nMethods\\n2.1\\nLitterature research\\nFor this review, we collected English-language conference and journal papers using PubMed, Google Scholar, arXiv,\\nand bioRxiv. We used the following query to find the papers referenced in this review: (\"deep learning\" OR \"artificial\\nneural network*\" OR \"Convolutional neural net*\" OR CNN OR \"Recurrent neural net*\" OR RNN) AND (\"MEG\" OR\\nMagnetoencephalogra*). Relevant papers were selected by examining the title first, followed by the abstract. Finally, we\\nused a PDF search for the terms from the query to better understand how they are used. This study includes conference\\npapers, journal papers, and pre-prints. The final inclusion criterion requires papers to be primary research articles (i.e.,\\nnot reviews), possess a valid digital object identifier (DOI) and a publication date preceding November 2024. Figure 1\\nprovides a schematic overview of the literature search and selection process employed in this review.\\nTables 2, 3, 4, 5 provide an overview of the key information extracted from the included studies for this survey.\\nMore detailed information on each paper was collected in a table we have made available online as a Google sheet:\\nhttps://tinyurl.com/ub3s5mr. See table 1 for details on each information category extracted from the papers.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 4}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging field\\nA PREPRINT\\nFigure 1: Schematic overview of the literature review process. An initial search query across specified databases yielded\\nover 10,000 results. These were screened based on language, DOI availability, publication date (< Nov 2024), and\\nexclusion of reviews, resulting in 119 included studies. Following data extraction, studies were categorized into three\\nmain groups: ’Classification’ (N=70; further subdivided into decoding, BCI, clinical, and event detection), ’Modeling’\\n(N=16; subdivided by cortical focus), and ’Other’ methodological applications (N=33; subdivided into method, source\\nlocalization, and preprocessing).\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 5}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging field\\nA PREPRINT\\nItem\\nDescription\\nOnline table\\nIncluded table\\nAuthor\\nName of the main author(s).\\nPublication year\\nThe year the paper/pre-print was published.\\nTitle\\nThe title of the paper.\\nDOI\\nThe DOI of the paper.\\nCategory\\nThe paper category (classification, modeling, or other).\\nSubcategory\\nSubcategory of paper.\\nGoal\\nThe goal of the study.\\nSubjects (N)\\nThe number of subjects for the study.\\nNotes on subjects\\nInformation and details on subjects\\nSample size (trials)\\nThe total number of samples used.\\nNotes on sample size\\nInformation and details on the number of samples.\\nInput data\\nThe type of input data given to the neural network.\\nDetailed input data\\nA more detailed version of the input data column.\\nData augmentation\\nData augmentation techniques used, if any.\\nValidation\\nThe type of validation or model selection technique used.\\nSensors\\nThe number of sensors used.\\nSF\\nThe sampling frequency used.\\nPerformance\\nThe performances that the paper was able to reach.\\nTrial length\\nLength of a single data point given to the neural network.\\nBaseline\\nPreviously attained/baseline performances.\\nArchitecture (depth)\\nThe type of architecture used and its depth.\\nDetailed architecture\\nThe detailed architecture of the network used.\\npreprocessing\\nThe detailed preprocessing steps used.\\nProcessing\\nData processing/feature extraction process.\\nSpecificities\\nFurther details on how the study was led.\\nInterpretation\\nvisualization techniques or interpretation tools, if any.\\nTraining params\\nDetails on the neural network training, when available.\\nCode availability\\nWhether the code is publicly available or not.\\nTable 1: Description of table items, and in which table they can be found. Full table and information can be found in\\nhttps://tinyurl.com/ub3s5mr\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 6}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging field\\nA PREPRINT\\nYear\\nAuthors\\nSubcategory\\nN\\ntrials\\nArchitecture (depth)\\nValidation\\nCode\\n2016\\nYu et al. [49]\\nDecoding\\n15\\n2000\\nCNN (3)\\ntrain/test split\\n2017\\nWang et al. [50]\\nDecoding\\n2\\n410\\nMLP (1)\\n4Fold on train +\\nholdout test set\\n2018\\nHramov et al. [51]\\nDecoding\\n5\\n150000\\nMLP (1)\\ntrain/valid split\\n2018\\nFrolov\\nand Pisarchik [52]\\nDecoding\\n5\\n10000\\nMLP (1)\\ntrain/test split\\n2019\\nDash et al[53]\\nDecoding\\n4\\nN/M\\nMLP (3)\\nLSTM\\ntrain/valid/test split\\n2019\\nGarry et al. [54]\\nDecoding\\n605\\n75396\\nCNN (3)\\ntrain/valid/test split\\n2019\\nKim et al. [55]\\nDecoding\\n27\\n5400\\nCNN + Attention (6)\\n5Fold\\n2019\\nKostas et al. [56]\\nDecoding\\n101\\n3900\\nCNN\\nand\\nRCNN\\n(N/A)\\n5Fold\\n2019\\nDash et al. [57]\\nDecoding\\n8\\n2400\\nMLP (1)\\ntrain/test split\\n2019\\nHuang and Yu [58]\\nDecoding\\n16\\n588\\nCNN (4)\\ntrain/test split\\n2020\\nAbdellaoui et al. [59]\\nDecoding\\n18\\nN/M\\nCNN + Attention +\\nLSTM (10)\\ntrain/test split\\n2021\\nDash et al. [60]\\nDecoding\\n7\\n300\\nMLP (1)\\n5Fold\\n2020\\nDash et al. [61]\\nDecoding\\n10\\n180\\nMLP (1)\\n5Fold\\n2021\\nLi et al. [62]\\nDecoding\\n16\\n9414\\n2 x GRU + FC\\nLOO\\n2021\\nFeng et al. [63]\\nDecoding\\n200\\n397\\nGoogLeNet\\n-\\nCNN\\n(144)\\ntrain/valid split\\n2021\\nChang et al. [64]\\nDecoding\\n8\\n108\\nGAN-like (5)\\nHoldout set\\n2021\\nPilyugina et al. [65]\\nDecoding\\n17\\n17\\nCNN (1)\\ntrain/test split\\n2022\\nEngemann et al. [66]\\nDecoding\\n646\\nN/M\\nShallowFBCSPNet +\\nDeep4Net\\n5Fold\\n2021\\nShi et al. [67]\\nDecoding\\n17\\n10880\\nEEGNet (4)\\nnested LOSO\\n2023\\nZhang et al. [68]\\nDecoding\\n20\\nN/M\\nN/M\\nLeave One Out\\n2023\\nCsaky et al. [69]\\nDecoding\\n15\\n53100\\nWavenet Classifier (9)\\nLOSO\\n2023\\nÖzer et al. [70]\\nDecoding\\n18\\n9414\\nLSTM, GRU and CNN\\n10Fold\\n2023\\nBu et al. [71]\\nDecoding\\n12\\n8640\\nCNN (4)\\n5Fold + train/test split\\n2024\\nBoyko et al. [72]\\nDecoding\\n96\\n2304000\\nN/M\\ntrain/valid/test split\\n2024\\nYang et al. [73]\\nDecoding\\n27\\n179977\\nN/A\\ntrain/valid/test split\\n2024\\nZubarev et al. [74]\\nDecoding\\n12\\n3600\\nLF-CNN (3)\\n9Fold\\n2024\\nYang et al. [75]\\nDecoding\\n27\\n179977\\nNeuGPT\\ntrain/valid/test\\n2024\\nJayalath et al. [76]\\nDecoding\\n900\\n2296800\\nCNN + LSTM\\ntrain/valid/test\\n2020\\nShu and Fyshe [77]\\nDecoding\\n9\\n60\\nSAE\\nLOO\\nTable 2: Part 1 of 4 of the table containing condensed information about the included papers. This part includes all papers\\ncategorized as ’Classification’ papers and subcategorized as ’Decoding’ papers. More info can be found in the complete table:\\nhttps://tinyurl.com/ub3s5mr. N is the number of subjects included in the study. ’N/M’ is used when specific information was\\neither not mentioned or when it was impossible to infer from other information. N/A means the information does not apply to the\\nstudy.\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 7}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging field\\nA PREPRINT\\nYear\\nAuthors\\nSubcategory\\nN\\ntrials\\nArchitecture (depth)\\nValidation\\nCode\\n2018\\nMeng et al. [78]\\nClinical\\n45\\nN/M\\nCNN (4)\\n4Fold\\n2019\\nAoe et al. [79]\\nClinical\\n233\\n70000\\nCNN (12)\\n10Fold\\n2020\\nGu et al. [80]\\nClinical\\n32\\nN/M\\nMLP (4) + Self-attention\\nhead\\ncross-validation\\n2020\\nZhang et al. [81]\\nClinical\\n32\\nN/M\\nLSTM (2)\\n10Fold + holdout test set\\n2021\\nXu et al. [82]\\nClinical\\n129\\nN/M\\nG2G network (1)\\ntrain/valid split\\n2021\\nGiovannetti et al. [83]\\nClinical\\n87\\n6525\\nAlexNet - CNN (5)\\nLOSO\\n2021\\nWu and Huang [84]\\nClinical\\n38\\n2280\\nCNN (4)\\n2Fold\\n2021\\nHuang et al. [85]\\nClinical\\n95\\n95\\nEEGNet (4)\\ntrain/valid/test split\\n2022\\nHuang et al. [86]\\nClinical\\n190\\n33500\\nResNet (5)\\n5Fold + train/val/test split\\n2022\\nFujita et al. [87]\\nClinical\\n180\\n21000\\nMLP (1)\\nMNet (12)\\n10Fold + holdout test set\\n2023\\nBarik et al. [88]\\nClinical\\n60\\nN/M\\nMLP (1)\\n5Fold nested\\n2024\\nAnand et al. [89]\\nClinical\\nN/M\\n1080000\\nN/M\\ntrain/test split\\n2024\\nAchterberg et al. [90]\\nClinical\\n8\\n2400\\nDNN (4)\\ntrain/valid/test split\\n2018\\nDash et al. [91]\\nBCI\\n4\\n1635\\nMLP (1)\\ntrain/valid/test split\\n2018\\nDash et al. [92]\\nBCI\\n3\\n1225\\nCNN (8)\\n5Fold (trials)\\n2019\\nHramov et al. [93]\\nBCI\\n10\\nN/M\\nMLP\\nN/M\\n2019\\nHramov et al. [94]\\nBCI\\n7\\nN/M\\nMLP (2)\\nN/M\\n2019\\nZubarev et al. [95]\\nBCI\\n7\\n11354\\nLF-CNN (3)\\nLOO\\n2019\\nDash et al. [96]\\nBCI\\n4\\n1635\\nCNN (5)\\ntrain/valid/test split\\n2020\\nDash et al. [97]\\nBCI\\n8\\n3046\\nAlexNet (7)\\nResNet101 (101)\\nInception-ResNet-v2 (164)\\ntrain/valid/test split\\n2020\\nDash et al. [98]\\nBCI\\n8\\n3046\\nLSTM (3)\\nN/M\\n2020\\nYeom et al. [99]\\nBCI\\n9\\n2160\\nLSTM\\n5Fold\\n2020\\nDash et al. [100]\\nBCI\\n4\\n1500\\nLSTM*\\ntrain/valid/test split\\n2020\\nLopopolo\\nand\\nvan\\nden\\nBosch [101]\\nBCI\\n15\\n1377\\nCNN (4)\\nhouldout test set\\n2021\\nOvchinnikova et al. [102]\\nBCI\\n32\\n422\\nLF-CNN (3)\\nnested 4Fold in a 5Fold\\n2022\\nFan et al. [103]\\nBCI\\n61\\n39040\\nCNN (3)\\ntrain/test split\\n2018\\nGuo et al. [104]\\nEvent\\n10\\n102\\nSSAE (3)\\n5Fold\\n2019\\nZheng et al. [105]\\nEvent\\n20\\n4000\\nCNN (8)\\nmultiple KFold\\n2020\\nLiu et al. [106]\\nEvent\\n20\\n150\\nCNN*\\nKFold\\n2021\\nZhang et al. [107]\\nEvent\\n10\\nN/M\\nEEGNet (4)\\ncross-validation\\n2022\\nHirano et al. [108]\\nEvent\\n348\\n23177\\nResNet (26) and AE\\n5Fold and 10Fold\\n2022\\nBhanot et al. [109]\\nEvent\\n15\\n11000\\nN/M\\n5Fold\\n2022\\nZhao et al. [110]\\nEvent\\n20\\n1320\\nDANN\\nLeave One Out\\n2022\\nGuo et al. [111]\\nEvent\\n20\\n202\\nTransformer\\nKFold\\n2022\\nZhang et al. [112]\\nEvent\\n20\\n150\\nCADNet\\nDendriteNet\\nN/M\\n2023\\nZheng et al. [113]\\nEvent\\n48\\nN/M\\nCNN (5)\\n7Fold\\n2023\\nMouches et al. [114]\\nEvent\\n95\\n40662\\nN/M\\n10Fold\\n2024\\nWei et al. [115]\\nEvent\\n277\\n230325\\nN/M\\ntrain/valid split\\n2024\\nHe et al. [116]\\nEvent\\n11\\n35013\\nCNN + Attention (5)\\nLOSO\\n2024\\nDev et al. [117]\\nEvent\\n7\\n7060\\nN/M\\ntrain/valid/test split\\n2024\\nHirano et al. [118]\\nEvent\\n1782\\n60139\\n26 Layer scSE-ResNet\\n5Fold subject-wise\\nTable 3: Part 2 of 4. Includes all papers categorized as ’Classification’ papers and subcategorized as ’clinical’, ’BCI’, or ’event detection’\\npapers.\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 8}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging field\\nA PREPRINT\\nYear\\nAuthors\\nSubcategory\\nN\\ntrials\\nArchitecture (depth)\\nValidation\\nCode\\n2016\\nCichy et al. [119]\\nVisual\\n15\\n3540\\nSuperVision - CNN ()\\nN/M\\n2017\\nCichy et al. [120]\\nVisual\\n15\\nN/M\\nCNN (8)\\nBootsrapping\\nresampling\\n2018\\nSeeliger et al. [121]\\nVisual\\n15\\n1000\\nVGG-S - CNN (10)\\n10Fold\\n2018\\nDima et al. [122]\\nVisual\\n19\\n720\\nCNN (7)\\nAlexNet - CNN (8)\\n5Fold\\n2018\\nBankson et al. [123]\\nVisual\\n32\\n32\\nVGG-F - CNN (7)\\nLOSO\\n2019\\nRajaei et al. [124]\\nVisual\\n15\\n1536\\nAlexNet - CNN (7)\\nHRRN (152)\\nLOO\\n2019\\nKietzmann et al. [125]\\nVisual\\n15\\n380\\nCNN\\nRCNN (6)\\n2Fold\\nbootstrapping\\n2020\\nGiari et al. [126]\\nVisual\\n25\\n3525\\nAlexNet - CNN (5)\\nN/A\\n2022\\nvan Vliet et al. [127]\\nWords\\n15\\n560\\nVGG (11)\\nN/A\\n2023\\nvon Seth et al. [128]\\nVisual\\n36\\nN/M\\nN/M\\nN/M\\n2020\\nDonhauser et al.[129]\\nSpeech\\n11\\n77\\nLSTM*\\n7Fold\\n2022\\nCaucheteux and King\\n[130]\\nAuditory\\n92\\nN/M\\nTransformer\\nCNN (4, 8 or 12)\\nN/A\\n2022\\nWingfield et al. [131]\\nSpeech\\n16\\n66300\\nMLP (6)\\nN/A\\n2023\\nDesbordes et al. [132]\\nSpeech\\n11\\n3630\\nLSTM (2)\\nTransformer (11)\\n10Fold\\n2024\\nBrodbeck et al. [133]\\nSpeech\\nRecognition\\n18\\nN/M\\nN/M\\nN/M\\n2024\\nLyu et al. [134]\\nSpeech\\nRecognition\\n16\\n5760\\nN/M\\nN/M\\nTable 4: Part 3 of 4. Includes all papers that were categorized as ’Modeling’ papers.\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 9}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging field\\nA PREPRINT\\nYear\\nAuthors\\nSubcategory\\nN\\nTrials\\nArchitecture (Depth)\\nValidation\\nCode\\n2019\\nDinh et al. [135]\\nSL\\n1\\nN/M\\nLSTM\\ntrain/valid split\\n2021\\nPantazis\\nand Adler\\nSL\\nN/A\\nN/M\\nCNN (5) and MLP (4)\\nN/A\\n2021\\nDinh et al. [136]\\nSL\\n1\\n1653\\nLSTM\\ntrain/valid split\\n2022\\nSun et al. [137]\\nSL\\n29\\n620256\\nResNet (4) LSTM (3)\\ntrain/test split\\n2023\\nSun et al. [138]\\nSL\\n29\\n620256\\nN/M\\nN/M\\n2023\\nO’Reilly et al. [139]\\nSL\\n1\\n446\\nSimpleRNN (4)\\nBootstrapping\\n2024\\nSanchez-Bort\\net al\\nSL\\n1\\nN/M\\nMPSS (4)\\nN/M\\n2024\\nJiao et al. [140]\\nSL\\n1\\nN/M\\nCNN + Attention\\nN/M\\n2024\\nYokoyama et al. [141]\\nSL\\n3\\nN/M\\n4LCNN (4)\\ntrain/valid sets\\n2016\\nHyvärinen\\nand Morioka[142]\\nPreprocessing\\n9\\nN/M\\nFC (4)\\nM/M\\n2017\\nGarg et al. [143]\\nPreprocessing\\n49\\n980\\nCNN (6)\\ntrain/test split\\n2017\\nGarg et al. [144]\\nPreprocessing\\n44\\n880\\nCNN (9)\\nLOO + train/test split\\n2018\\nCroce et al. [145]\\nPreprocessing\\n67\\n4038\\nCNN (6); FC (6)\\nCNN+FC\\n10Fold (trials)\\n2018\\nHasasneh et al.[146]\\nPreprocessing\\n48\\n1112\\ntemporal. CNN (3)\\nspatial. CNN (2)\\nsCNN + tCNN\\n50Fold\\n2021\\nFeng et al. [147]\\nPreprocessing\\n4\\n66780\\nGoogLeNet - CNN\\n(144)\\ntrain/valid/test split\\n2021\\nTreacher et al. [148]\\nPreprocessing\\n217\\n294\\nCNN (11)\\n10Fold\\n2023\\nHamdan et al. [149]\\nPreprocessing\\nN/M\\nN/M\\nAE (9)\\nN/M\\n2017\\nGuo et al. [150]\\nMethod\\n3\\nN/M\\nCNN (5)\\n10Fold\\n2019\\nHarper et al. [151]\\nMethod\\n72\\nN/M\\nLRCN\\ntrain/valid/test split\\n2021\\nAbdellaoui et al. [152]\\nMethod\\n18\\nN/M\\nCNN + Attention\\ntrain/test split\\n2022\\nPriya and Jayalakshmy\\n[153]\\nMethod\\n23\\n13472\\nGoogLeNet\\nKFold with\\nholdout test set\\n2023\\nGosti et al. [154]\\nMethod\\n10\\nN/M\\nRHoMM\\n-\\nRNN\\n(N/M)\\nN/M\\n2023\\nElshafei et al. [155]\\nMethod\\n1\\n166800\\nDNN (4)\\nCNN (1)\\ntrain/test split\\n2023\\nFan et al. [156]\\nMethod\\n669\\n70000\\nLFCNN\\nVARCNN\\nHGRN\\ntrain/valid split\\n2023\\nCsaky et al. [157]\\nMethod\\n37\\n53100\\nWaveNet-based\\ntrain/valid split\\n2023\\nZhu et al. [158]\\nMethod\\n676\\n100000\\nMLP (1)\\nNICA(TCL) (3)\\nNICA(IIA) (3)\\nN/A\\n2024\\nSolana et al. [159]\\nMethod\\n16\\nN/M\\nROCKET-based\\nmodels (3)\\ntrain/valid split\\n2024\\nFan et al. [160]\\nMethod\\n250\\n29238\\nEEGNet\\nResNet\\nShuffleNet\\nN/M\\n2024\\nGallard et al. [161]\\nMethod\\n44\\nN/M\\nCycleGAN\\nN/A\\n2024\\nChou et al. [162]\\nMethod\\n17\\nN/M\\nCNN (3)\\ntrain/valid/test split\\n2024\\nCsaky et al. [163]\\nMethod\\n15\\n3540\\nGPT\\ntrain/valid/test split\\n2024\\nGideoni et al. [164]\\nMethod\\n6\\nN/M\\nMLP and CNN\\ntrain/valid/test split\\n2024\\nFerrante et al. [165]\\nMethod\\n4\\nN/M\\nCNN and CLIP\\nN/A\\nTable 5: Part 4 of 4. Includes all papers that were classified as ’Other’ papers. SL is an abbreviation of source localization.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 10}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nCategory\\nSubcategory\\nStudy\\nClassification\\nDecoding\\n[56], [64], [63], [57], [61], [53], [60], [52], [51], [55], [77]\\n[62], [50], [49], [58], [70], [54], [74], [65], [71],\\n[72], [59], [68], [73], [69], [66], [67], [75], [76]\\nBCI\\n[91], [97], [100], [98], [96], [92], [94], [93],\\n[103], [102], [99], [95], [101], [116]\\nClinical\\n[79], [78], [83], [85], [86], [81], [80], [84], [82], [89],\\n[90], [87], [88], [112], [110], [109], [108]\\nEvent detection\\n[113], [105], [111], [106], [107], [104], [114], [115], [117][118]\\nModeling\\nVisual cortex\\n[122], [123], [119], [120], [125], [121], [126], [127] [124], [128]\\nAuditory cortex\\n[129], [130], [132], [131], [133], [134]\\nOther\\nPreprocessing\\n[145], [144], [147], [146], [143], [142], [148], [149],\\nMethods\\n[157], [152], [150], [151], [153], [156], [154], [161],\\n[155], [163], [159], [160], [162], [165], [164], [158]\\nSource localization\\n[135], [166], [137], [139], [136], [138], [167],\\n[140], [165]\\nTable 6: The categories and subcategories classification for all reviewed papers.\\n2.2\\nTypes of studies\\nIn conducting this review, we opted to categorize the included studies into three distinct groups: (1) ’Clas-\\nsification’, (2) ’Modeling’, and (3) ’Other’. The ’Classification’ category was split into four sub-groups:\\ndecoding, brain-computer interfaces (BCI), clinical applications or computer-aided diagnosis (CAD), and\\nevent detection. Studies under ’decoding’ primarily leverage classification as a statistical tool to identify the\\ninvolvement of specific brain features in certain tasks. The BCI subcategory includes studies that incorporate\\nANNs in their BCI framework. Meanwhile, the ’clinical’ subcategory includes studies that utilize ANNs\\nto enhance clinical diagnosis or prognosis, and the ’event detection’ subcategory regroups studies using\\nANNs to continuously detect events in MEG data. Studies in the ’Modeling’ category primarily focus on\\ncomparing activations across ANNs and the brain. The ’Other’ category gathers the studies that do not fit\\nthe aforementioned groups and includes research utilizing ANNs for preprocessing tasks such as artifact\\ndetection and removal or source localization. The distribution across more specific subcategories is given in 6\\nand illustrated in figure 2d.\\n2.3\\nNotes on terminology\\nSome of the ML nomenclature overlaps with terminology used in M/EEG research, which can sometimes\\nlead to confusion. In particular, the term ’epoch’ in deep learning generally describes a complete pass of the\\ntraining data, where the entire dataset is used once to update the model parameters. Meanwhile, in the field\\nof M/EEG, it usually describes a segment of data (or a single trial). In addition, the term ’samples’ in ML\\nusually refers to individual examples from the dataset, which should not be confused with the use of the same\\nterm to refer to a data point in a M/EEG time series, where the parameter known as ’sampling rate’ (Hz)\\nrefers to the rate at which the data is assessed, i.e., data points per second.\\n3\\nResults\\nIn the following, we present a comprehensive analysis of the corpus of papers selected for this review. After a\\ngeneral overview highlighting key trends and patterns across various categories, we dive into a more detailed\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 11}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n(a)\\n(b)\\n(c)\\n(d)\\nFigure 2: Quantitative overview of the reviewed studies (n=119). (a) The evolution of the number of MEG\\npublications using ANNs across categories (classification, modeling, and other). (b) Size of the dataset\\n(number of samples) used for training and evaluation of the ANNs across the reviewed studies. ’N/M’\\naccounts for the studies that did not mention the number of samples used. (c) Overall distribution across\\ncategories of all the reviewed publications. (d) Overall distribution across subcategories categories of all the\\nreviewed publications.\\nexamination of the methods, data, and ANN architectures used across each of the three main categories of\\npapers.\\nThe methodology explained in the previous section (2) led to the identification of 119 relevant studies. Among\\nthe papers considered, 102 were peer-reviewed, while 17 had not yet undergone peer review at the time of\\nwriting. Our categorization method resulted in the distribution of 70 papers under ’Classification’, accounting\\nfor 58.8% of the total; 16 papers were categorized as ’Modeling’, making up 13.4%; and 33 papers fell into\\nthe ’Other’ category, representing 27.8% of the study corpus included in this review (See figure 2c). The\\ncategory containing most of the papers is the ’Classification’ category, followed by ’Other’; the category with\\nthe least amount of studies is ’Modeling’. Generally, the trend of using ANNs with MEG data has increased\\nover the years across all categories, as depicted in figure 2a. A similar growth trajectory is anticipated for\\nstudies on this topic, analogous to the rapid expansion of research utilizing ANNs for EEG data analysis ([5]).\\n3.1\\nGeneral overview\\nThe in-depth survey revealed several key observations about this emerging field (figure 2). Interestingly, out\\nof the 119 selected studies for this review, 65 use less than 20 subjects, 35 use between 20 and 100 subjects,\\nand 16 use more than 100 subjects. The remaining studies use synthetic MEG data or do not mention the\\nnumber of subjects, or patients included in the study. The number of samples (number of trials or epochs)\\nin datasets varies across a wide range, from 17 to 2304000 (see figure 2b), with an average of 102247, a\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 12}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nstandard deviation of 383501, and a median of 4000. However, a few outliers significantly impact the average\\nand standard deviation of the number of samples used in the included studies (Correcting for outliers gives\\nan average sample number of 13246 and a standard deviation of 21147). Furthermore, the length of the\\ndata segments fed to the neural network varies between 300ms to 10s. The most commonly used sampling\\nfrequencies are 1000 Hz (26 papers) and 250 Hz (17 papers) (as illustrated by figure 3), and the median across\\nall studies was 250 Hz.\\nFigure 3: Distribution of sampling frequencies (in Hz) reported across the reviewed studies (N=110 reporting).\\nThe height of each bar indicates the total number of studies using a specific frequency. Stacked colors\\nrepresent the main study categories (yellow: ’Other’, red: ’Modeling’, blue: ’Classification’). The most\\nfrequently reported rates were 1000 Hz (N=26) and 250 Hz (N=17).\\n3.2\\nClassification studies\\n3.2.1\\nStudy aims and subcategories\\nAs explained in section 2, studies in the ’Classification’ category were divided into four main subcategories\\nbased on their primary goal: BCI, decoding problems, clinical applications, and event detection.\\nBCI studies: BCI studies assess the feasibility of building brain-controlled devices that rely on predicting the\\nsubject’s intentions from their neural data. BCI studies cover a wide range of approaches. Still, most consist\\neither of offline assessments of previously recorded data or online examination of intention decoding in a\\nclosed-loop setting. BCI is one of the neuroscience domains in which ML made its earliest incursions (e.g.\\ndecoding arm movement direction and kinematics from neuronal activity in non-human primates). Of the\\n13 studies subcategorized as BCI, five use ANNs for motor tasks ([94, 93, 99, 102, 103]), seven focus on\\nspeech-related BCI ([91, 92, 96, 97, 98, 100, 101]), and finally, in [95], the authors investigate how ANNs\\ncan improve the state-of-the-art of BCI.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 13}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nDecoding studies: Generally speaking, decoding studies aim to better understand the neural correlates of\\nbehavior and cognitive processes. This is typically accomplished by using ML or statistical inference. This\\napproach determines the neural responses that underlie specific sensory, motor, or higher-order cognitive\\nfunctions by revealing the features that exhibit the strongest separation between classes. In this category,\\neight studies use ANNs for speech decoding ([50, 53, 57, 61, 60, 56, 101, 72]). Three studies specifically\\nfocus on using ANNs for auditory stimuli decoding ([65, 63, 54]). Additionally, 10 studies use them for\\nvisual stimuli decoding ([52, 51, 55, 62, 58, 70, 54, 69, 67, 68]). The remaining decoding studies use ANNs\\nfor affect states decoding ([49]), age decoding ([66]), hand gestures ([71, 74]), rhythm decoding ([64]) or text\\ndecoding ([73, 75, 77]).\\nClinical studies: we describe clinical studies, which aim to predict or detect diseases or anomalies in patients.\\nThirteen studies use ANNs for clinical purposes or CAD. Two focus on epilepsy-related signal properties\\ndetection ([87, 80]). Two studies focus on Alzheimer’s disease early detection ([83]) or evolution ([82]). Three\\nfocus on neurological disease diagnostic ([79, 90, 89]), and the remaining studies focus on schizophrenia\\ndetection ([84]), autism detection in children population ([88]), PTSD severity evaluation ([81]), migraine\\ndiagnostic ([78]), mild traumatic brain injury detection ([85]) or depression and bipolar disorder detection\\n([86]).\\nEvent detection studies: Lastly, this subcategory encompasses research using ANNs to identify specific,\\noften transient, events within continuous or segmented MEG recordings. A dominant application within\\nthe reviewed literature involves the automatic detection and sometimes localization of pathological neural\\nevents, particularly epileptic spikes or high-frequency oscillations, which are crucial biomarkers for epilepsy\\ndiagnosis and treatment planning ([105, 113, 115, 117, 104, 111, 106, 114, 110, 108, 112, 109, 118]).\\nOther studies target different types of events, such as specific visual targets within rapid presentations\\n([107]). Collectively, event detection studies contributed significantly to the ’Classification’ category, with\\nepilepsy-related applications being particularly numerous (15 studies across ’Clinical’ and ’Event detection’\\nsubcategories combined).\\n3.2.2\\nThe pipeline\\nAcross all classification studies, the typical pipeline involves using MEG recordings as input to ANNs with\\nthe goal of predicting class labels associated with sensory, motor, cognitive, or clinical states. While the\\nprecise implementation varies, most studies follow a common structure that includes preprocessing the MEG\\nsignals, optionally extracting features, and feeding the resulting data to a classifier—most often a CNN. The\\ninput of the model may consist of hand-crafted features, raw MEG signals, or a combination of both. These\\ninputs are typically segmented into trials or epochs of fixed duration, which are then used to train and evaluate\\nthe model. The network outputs a predicted class label or probability distribution over target categories.\\nDespite this shared pipeline, classification goals, model inputs, and preprocessing strategies differ substantially\\ndepending on the application, and no standard protocol has emerged across the 70 studies. Figure 4 (left\\npanel) illustrates a schematic representation of this typical ’Classification’ pipeline, alongside comparable\\ndiagrams for the ’Modeling’ and ’Other’ application categories.\\n3.2.3\\nThe data and preprocessing\\nParticipant numbers and dataset sizes varied considerably across the 70 reviewed classification studies. Due\\nto differences in the goal and rationale, the number of subjects varied substantially (ranging from 2 [50]\\nto 646 [66]). Sample sizes (trials or epochs) also varied widely; while a majority of studies reporting this\\ninformation (21 out of 56) used more than 1000 samples, thirteen studies did not specify this number (see\\nfigure 2b for the distribution).\\nAmong all classification papers, 27 studies make use of hand-crafted features. While most of these studies\\ntrain the network exclusively with hand-crafted features ([93, 78, 91, 97, 80, 92, 83, 85, 82, 109, 112, 112,\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 14}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nFigure 4: Representative workflows for applying ANNs to MEG data across the main application categories\\nidentified in this review. (left) ’Classification’ pipeline: typically involves preprocessing MEG data, feeding\\nit to an ANN classifier, and evaluating prediction accuracy. (center) ’Modeling’ pipeline: often compares\\nrepresentations (activations) extracted from ANNs processing stimuli with corresponding preprocessed MEG\\ndata, using similarity analysis (e.g., RSA, neural predictivity) and evaluating against baselines. (right) ’Other’\\npipeline: encompasses methodological applications like preprocessing or source localization using ANNs,\\noften evaluated by comparison with classical approaches or metrics like AUC or spatial error.\\n15'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 15}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n87, 53, 49, 61, 60, 50, 70, 86, 90, 116, 117]), a few use a combination of hand-crafted features and raw MEG\\ndata ([81, 63, 79, 89]). Among the reviewed studies, spectral power and functional connectivity matrices\\nwere the two most frequently used types of hand-crafted features.\\nIn the studies that used the original MEG data as input rather than hand-crafted features, the reported length\\nof the segments used as samples for training and testing the ANNs varied between 125 and 2,400. This\\ncorresponds to trial durations ranging from approximately 41.67 ms to 10,000 ms, with a median of 1,063 ms\\nacross the studies that reported this information. Unfortunately, a few papers do not mention the length of the\\nsegments used as input for their neural network ([51, 58, 64, 89]).\\nData augmentation techniques are generally recommended in deep learning to improve model generalization\\nand mitigate overfitting ([1]), but was however employed relatively infrequently in the surveyed classification\\nstudies. Specifically, only 12 out of 70 studies reported using such methods ([108, 91, 53, 56, 86, 59, 69,\\n68, 90, 72, 75, 118]). These studies use some form of linear shifting and/or sensor shuffling or mix-up\\nregularization ([168]). An example of linear shifting consists of selecting a different cropping of a trial or\\nsegment around the cue in the temporal dimension. The shifted samples end up having the same length but\\ndifferent onsets.\\nTypically, across all studies in this category, the MEG data were preprocessed before being fed as input to a\\nneural network. This preprocessing phase generally includes down-sampling, band-pass filtering, and either\\nde-noising and/or artifact removal. Of the 70 studies in this category, 12 studies (’decoding’: [57, 61, 53, 50]\\n’BCI’: [91, 97, 100, 98, 92, 93], ’clinical’: [86], ’event detection’: [117]) use discrete wavelet transform to\\nincrease signal-to-noise ratio (SNR) in the data as a preprocessing step ([169]). Notably, these 12 studies are\\nspread across only four distinct research groups representing a small portion of the reviewed studies. Five\\nstudies employ alternative de-noising techniques ([62, 69, 89, 90, 113]). Additionally, 16 studies use ICA to\\nremove heart and/or eye movement artefacts ([54, 71, 67, 95, 88, 84, 105, 107, 89, 90, 74, 68, 82, 78, 83, 56]).\\nThe remaining 38 studies do not explicitly mention de-noising or artifact removal techniques in their\\npreprocessing pipeline.\\nThe studies in this subcategory used MEG data sampled at frequencies between 50 Hz and 2400 Hz. Higher\\nsampling frequencies (e.g., 1000–2400 Hz) were typically used in studies focused on decoding fine-grained\\ntemporal dynamics, such as those targeting auditory and motor-related tasks, where capturing rapid neural\\noscillations (e.g., in the gamma range) is critical. In contrast, lower sampling rates (e.g., 50–250 Hz)\\nwere generally the result of preprocessing choices aimed at reducing computational load or were used in\\nstudies not reliant on high-frequency information, such as certain clinical applications or feature-based\\npipelines. Although many studies did not report their original sampling frequency, most MEG systems\\n(e.g., Elekta, CTF, BTi) record at native rates of 1000 Hz or above, suggesting that downsampling is\\ncommonly applied during preprocessing. Figure 3 displays the distribution of sampling frequencies across\\nthe reviewed studies, with 1000 Hz (26 studies of which 23 being classification studies) and 250 Hz (17\\nstudies of wich 10 are classification studies) being the most frequently used rates, and a median of 600 Hz\\nacross all reported values. Still, eight studies omitted details about the sampling frequency, in most cases\\nbecause the authors extracted features from the MEG data, making the original sampling rates less pertinent\\n([49, 78, 93, 83, 64, 87, 88, 117]).\\n3.2.4\\nNetwork architectures\\nANNs, like any other parametric algorithm, have architectural parameters that play a key role in determining\\ntheir capacity to learn from data. These parameters —including depth, number of units, and connectivity\\npatterns— affect how well the model can extract and represent relevant features. Optimal configurations vary\\ndepending on the type of task and the nature of the input data. In this section, we provide an overview of the\\nmost commonly used architectures and their configurations across classification studies.\\n16'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 16}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nThe most commonly used ANNs for classification-type studies are CNNs. The second most used architecture\\nwas multi-layer perceptrons (MLP), followed by recurrent neural network (RNN) architectures. For the\\nCNNs, the depth of the networks varied widely, between one and over 100 hidden layers (up to 164 in one\\ncase [97]). This wide range reflects the heterogeneity in input data characteristics, task complexity, and\\nspecific CNN architectures employed across the reviewed studies, ranging from relatively shallow custom\\nCNNs to very deep pre-existing architectures like GoogLeNet ([170]) and ResNet ([171]) variants (e.g.,\\n[63, 97]). In contrast, the papers that used MLPs essentially limit themselves to a maximum of six hidden\\nlayers. This typical depth difference often relates to parameter efficiency: the weight-sharing mechanism in\\nCNNs generally allows for deeper architectures with high-dimensional data such as MEG recordings, whereas\\nthe large number of parameters in fully-connected MLP layers can make similar depths computationally\\ncostly and difficult to train effectively ([1]).\\nMost of the CNN architectures used in the reported MEG classification papers comprise the input layer\\nfollowed by multiple blocks, typically containing convolution layers followed by activation functions (like\\nrectified linear unit, ReLU) and often pooling layers (like max pooling) for downsampling. At the end, one\\nor two fully connected layers are followed by an activation function. For the convolution layers, the critical\\nparameters are filter size, stride, padding, and number of channels. For fully connected layers, the crucial\\nparameter is the number of units, also called the width of the layer.\\nAlthough an in-depth discussion of parameter initialization techniques is beyond the intended scope of this\\nreview, it is important to keep in mind that initialization methods (e.g., random, Xavier, or He initialization)\\nare important and can profoundly influence model performance and stability (see [172]). Similarly, the\\nchoice of activation function (e.g., Sigmoid, Tanh, ReLU and its variants) applied after convolutional or fully\\nconnected layers is another critical design decision impacting network behavior and learning dynamics.\\n3.2.5\\nTraining and validation techniques\\nBeyond the architecture used and data (pre-)processing, details about training parameters, loss function,\\noptimizer, and regularization techniques are essential to replicate the results of such studies. While training\\nparameters may not have as significant an impact as architecture parameters, they remain essential and can\\nheavily impact training time and performance. Out of the 70 classification studies, 17 do not mention training\\nparameters. The remaining studies generally mention learning rate, batch size, loss function, optimization\\nmethod, and regularization techniques. Batch sizes, regularization techniques (dropout, L1 and L2 norm\\nweight usage), number of epochs, early stop criteria, learning rates, and momentum parameters are crucial to\\nwatch out for. While the specific training parameters used in each study are crucial for reproducibility, the\\nhigh degree of variability across different experimental settings and goals makes a detailed summary within\\nthis review text impractical. Instead, to provide a resource for readers interested in specific implementations,\\nfurther details on the reported parameters for individual studies can be found in the comprehensive online\\ntable (https://tinyurl.com/ub3s5mr).\\nOf the studies that specify the optimization algorithm utilized for training their network, 29 employed adaptive\\nmoment estimation (Adam [173]) or one of its variants, while four used SGD. Adam, uses both the first-order\\nand second-order moments of the gradient to adapt the learning rate, whereas SGD only uses the first-order\\nmoment. Adam is known for its faster convergence in some architectures ([174]), but it can quickly plateau\\nafter convergence ([175]) and may perform worse than SGD in specific use cases ([176]).\\nAs far as the loss function is concerned, most of the surveyed studies used cross-entropy to measure the\\ndiscrepancy between the predicted probabilities and the actual distribution. More specifically, it computes the\\nloss by taking the negative log of the probability assigned to the true class, aiming to minimize this loss to\\nimprove model accuracy.\\nValidation techniques are essential to training any model in a classification setting, enabling the proper\\nevaluation of the model’s performance. In a setting where generalizing to new subjects is essential, such\\n17'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 17}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nas BCI or CAD, it is crucial to exclude multiple subjects’ data from the training process. This ensures\\nthe test of the trained algorithm’s generalization capabilities on entirely new and unseen data. It is also\\nessential to evaluate and fine-tune the algorithm’s performances on training and validation datasets that do\\nnot share information with each other or the test set. Among the 71 studies in this category, 26 used the\\nK-fold validation technique (K varying from two to ten), 29 used a simple train/valid/test set split, 11 used\\nleave one out or leave P out at the subject level. Five did not mention any validation technique in their\\nanalysis pipeline. The choice between a simple train/valid/test split and cross-validation methods (like K-fold\\nor leave-subject-out) often depends on dataset size and computational resources. While cross-validation\\nprovides more robust performance estimates and is recommended when data is scarce, as it leads to a less\\nnoisy estimate of performance ([177]), simple splits are frequently used in deep learning when dealing with\\nvery large datasets where the computational cost of K-fold is high and a single split is deemed sufficient\\nfor stable evaluation. Reflecting this trade-off, the studies in our review using simple splits often involved\\nsubstantially larger datasets compared to those employing cross-validation (see tables 2 and 3). The leave one\\nsubject out (LOSO) cross-validation strategy is particularly relevant when subject generalization is essential\\nand the amount of available data is small. Furthermore, when using K-fold or random splits, it is crucial\\nto consider potential subject data leakage between folds or sets, ensuring that the evaluation reflects true\\ngeneralization rather than subject identification.\\nANNs are powerful and can tackle a large set of problems; however, they present challenges in terms of\\ninterpretation, implementation, and tuning. In some instances, it is unnecessary to use ANNs for problems\\nthat simpler algorithms can solve. This is why it is essential when using ANNs to check whether we are\\nusing the right tool for the right problem by computing a baseline of performances that can be reached with\\nmore classical machine-learning approaches. Unlike ANNs, which can implicitly learn features, classical\\nalgorithms require careful feature selection to effectively manage the high dimensionality of MEG data.\\nAmong the papers in the ’Classification’ category, 27 out of the 70 studies did not compare their ANN’s\\nperformance to simpler algorithms’ performances.\\nA critical part of decoding studies is understanding what aspect of the data allowed decoding. It is essential\\nto find where and how information is encoded in the data to understand more about the brain. When using\\nML algorithms, especially ANNs, it can be hard to make sense of what is happening in the latent space.\\nThis is why using visualization tools will help with the interpretability of the network. Of the 70 papers\\nin the ’Classification’ category, only 16 included interpretation or visualization techniques applied to the\\nnetwork. These techniques varied widely, including methods such as visualizing activation patterns or feature\\nmaps ([178]), generating feature importance or contribution maps (e.g., using permutation feature importance\\n[179], saliency methods [180], or additive feature attribution approaches [181]), analyzing performance\\nacross time or frequency, examining network-derived connectivity, and applying specific tools like gradient-\\nweighted class activation mapping (Grad-CAM [182]). This list provides illustrative examples but is not\\nexhaustive. Further details on methods used in specific studies can be found in the supplementary online table\\n(https://tinyurl.com/ub3s5mr). Only one of these studies was a BCI sub-categorized study ([95]), two\\nwere event detection studies ([105, 115]), four were clinical sub-categorized studies ([82, 78, 86, 88]), and\\nthe remaining 9 were decoding studies ([51, 55, 54, 69, 56, 67, 68, 73, 74]).\\n3.2.6\\nImbalanced datasets\\nDespite the growing interest in using ANNs for MEG-based classification, relatively few studies explicitly\\naddress the challenge posed by imbalanced datasets. This issue is especially prevalent in clinical and\\nevent detection studies, where one class—such as a rare neurological condition or the presence of epileptic\\nspikes—may be underrepresented in the training data. Class imbalance can significantly bias the model’s\\nperformance, as neural networks tend to favor the majority class unless corrective measures are implemented.\\nAmong the 70 classification studies reviewed, only a handful explicitly mention strategies to deal with class\\nimbalance. When addressed, this was most commonly achieved through the use of weighted loss functions,\\n18'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 18}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nsuch as weighted cross-entropy, which assign greater penalty to errors on the minority class. Some studies\\nadopted data-level strategies such as upsampling the minority class or synthetic data generation through\\ntechniques like synthetic minority oversampling technique (SMOTE [183]), although these methods were\\nrarely discussed in detail. Others relied on evaluation metrics that are more robust to imbalance, such as the\\nF1 score, precision-recall curves, or area under the ROC curve (AUC), rather than classification accuracy. In\\ngeneral, however, most studies did not report class distributions or provide a rationale for their evaluation\\nchoices, making it difficult to assess the impact of imbalance on their findings.\\nThe limited attention given to this issue is concerning, given the known sensitivity of neural networks to\\nimbalanced data, particularly when applied to high-dimensional signals like MEG. Future work in this area\\nwould benefit from more systematic reporting of class distributions, explicit justification of performance\\nmetrics, and comparative evaluations of different strategies for mitigating imbalance ([184]). Doing so would\\nimprove the interpretability and reproducibility of results and allow for more meaningful comparisons across\\nstudies.\\n3.3\\nModeling studies\\n3.3.1\\nStudy aims and subcategories\\nThe idea of using AI techniques, particularly deep learning, to reverse engineer brain function has turned into\\na thriving topic in Neuro-AI. In the context of this review, we refer to ’Modeling’ studies that use ANNs to\\nbuild models of some function of the brain. In contrast to the papers that use ANNs to classify MEG data, the\\nstudies in this category do not use MEG data as input for ANNs. Instead, they primarily involve comparisons\\nbetween the ANN activations and MEG recordings obtained in response to presenting the same visual or\\nauditory stimuli to both systems (i.e. the ANN and the human brain). Therefore, the difference between\\nthe MEG-ANN modeling work and the ANN-powered MEG classification work is basic. The underlying\\nrationale for conducting such comparisons is the hypothesis that higher similarities between the artificial and\\nbiological neural responses indicate greater functional similarities between the ANN model and the neural\\nnetwork mechanisms.\\nAlthough the ANN training parameters, regularization, and validation techniques remain essential in this\\ncontext for model training, they do not have the same type of impact on the results as for the classification\\nstudies discussed in the previous section. In the following, we will overview the main trends observed across\\nthe modeling studies, starting with the principle sensory processes that have been explored.\\nFour out of the 16 studies in this category investigated the auditory cortex ([129, 130, 132, 131]), while nine\\nfocused on the visual cortex ([122, 123, 119, 120, 125, 121, 126, 124, 128]). Additionally, one study aimed\\nto model the visual word recognition in the human brain ([127]) and two set out to model speech recognition\\n([133, 134]).\\nOut of the eleven studies investigating the visual cortex (including visual word recognition), eight use\\nrepresentational similarity analysis (RSA [185]) or a variant RSA-based approach, one used a method based\\non ’neural predictivity’ ([121]), and the last one used ’profile responses’ ([127]).\\nAmong the studies focusing on modeling the function of the auditory cortex (including speech recognition),\\ntwo comparing representations of ANNs to those of the brain when presented with auditory stimuli also used\\na variant of RSA ([131, 134]). Three employed methods based on neural predictivity ([129, 130, 133]). The\\nlast study employed ’dimensionality analysis’ ([132]).\\n19'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 19}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n3.3.2\\nThe pipeline\\nA schematic overview of a typical pipeline for studies in the ’Modeling’ category is provided in figure 4\\n(center panel). Most of the articles included in this category essentially compare neuromagnetic activity and\\nthe ANN activations generated by the same task (e.g., visual stimulus categorization).\\nTo achieve this, MEG brain activity is typically recorded from healthy subjects as they engage in simple\\ncategorization or identification tasks involving either visual or auditory stimuli. Subsequently, these same\\nstimuli are fed to an ANN model previously trained on a similar task and data distribution. In practice,\\ndifferent ANNs, such as CNNs for image stimuli and RNNs for audio stimuli, perform the same activities\\nas the human subjects in the MEG study. This is then followed by a comparison of stimulus representation\\nbetween ANNs and human brains. While some studies opt to train their ANN models from scratch, others\\nuse pre-trained models to bypass the resource-intensive training phase.\\nMore concretely, once the models are trained, the same stimuli used for collecting brain data are fed to the\\nANNs and the layers’ responses’ are extracted. These responses are then considered as the ANNs’ activity,\\nwhich will then be compared to the MEG activity induced by the same set of stimuli. That said, in one study,\\nthe authors concatenated the responses from all the layers responses to create a single model-level response\\n([119]). Once the brain data are collected and the ANNs’ activations have been extracted, the next step is to\\ncompare them. In the following, we will review the main two methods used across the studies we surveyed:\\nRSA and neural predictivity. The remaining method used in this context are ’profile responses’ ([127]) and\\n’dimensionality analysis’ ([132]).\\nRepresentational similarity snalysis: RSA is an established and widely used computational method to\\ncompare patterns of neural activity across different conditions or stimuli. This is achieved by constructing\\nand analyzing similarity matrices representing the correlation or distance between the neural responses\\nto each pair of conditions. These similarity matrices are known as representational dissimilarity matrices\\n(RDMs), which serve as a crucial tool in RSA by providing a quantitative measure of how neural responses\\ndiffer across various experimental conditions, thus facilitating a deeper understanding of the underlying\\nrepresentational structures in the brain. In some studies, representational similarity matrices (RSMs) are\\nused instead of RDMs, employing direct correlation as the similarity metric rather than (1 −correlation) for\\nRDMs. Occasionally, decoding accuracies, an indirect measure of dissimilarity, are used instead of correlation\\nmeasures ([119, 120, 132]).\\nThe methodology for computing RDMs from fMRI data is generally uniform across many studies, typically\\nfocusing on regions of interest (ROI) RDMs. In this approach, the neural activity within a specified ROI is\\nrepresented by a 1D vector describing the activity of each voxel. However, there are alternative methods\\nthat do not rely on predefined ROIs. One such alternative is the searchlight RSA (sRSA) technique, which\\ninvolves specifying the shape and size of searchlight regions to scan the brain. Although computationally\\nintensive, sRSA offers the advantage of making fewer assumptions about specific brain regions.\\nBy contrast to RSA applications to fMRI data, the high temporal resolution of MEG provides the opportunity\\nto conduct time-resolved RSA either in the sensor or in source space. To handle this additional time dimension,\\nsix of the 11 studies using RSA consider a 1D response vector for each time point by aggregating the activity\\nof all brain sensors or specific ROIs’ voxels’ responses (in the case of source space). Ultimately, they generate\\ntime-resolved RDMs, sometimes referred to as ’RDM movies’ (see [125]). This particular RSA variant\\nwas termed RDA by [125]. Among the remaining studies employing RSA, two used a temporal variant\\nfrom searchlight RSA, termed spatiotemporal sRSA (ssRSA [131]). Instead of computing RDMs for every\\ntime point, the authors of these studies use time segments of a fixed length (25 ms for [131] and 16 ms\\nfor [122]). These studies combined this approach with searchlight RSA to construct multiple RDMs for\\ndifferent regions using a specific time interval. The final studies represented a combination of both approaches\\n([126, 128, 134]).\\n20'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 20}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nThe eight studies computed the RDMs for the ANNs, one RDM per layer, with an additional RDM per model\\nin one study.\\nFinally, a similarity score is computed by correlating the RDMs of MEG data with those of ANNs. The\\nobjective is to evaluate the similarity between the geometrical representations , defined as neural activity\\npatterns organized into a geometric space according to the distances among these patterns, of MEG and ANNs.\\nThis comparison is performed across both time (how MEG neural patterns evolve and align temporally with\\nANN activations) and space (how spatial patterns from MEG sensors or sources relate to ANN activations).\\nSuch comparison reveals whether and when ANNs produce neural representations resembling those of the\\nhuman brain. For more information on RSA, see [186, 187, 185].\\nNeural predictivity: As an alternative approach to RSA, neural predictivity essentially involves using an\\nANNs’ layer activity to predict brain activity. This prediction is typically achieved using an ML algorithm,\\nsuch as linear regression or a support vector machine. Some subjects are used to train the ML model, while\\nthe remaining subjects are kept aside as a test set. The algorithms’ performances are evaluated by comparing\\nthe predicted brain activity with the actual recorded brain activity, using correlation coefficients or mean\\nsquared error. We refer the readers to [188] for further details.\\nMultivariate pattern analysis (MVPA): Nine out of the 16 studies surveyed in this category begin with an\\nMVPA analysis ([122, 123, 119, 120, 126, 127, 124, 132]). MVPA is a statistical approach in neuroimaging\\nthat uses standard machine learning algorithms, such as LDA and SVM, to analyze and interpret patterns\\nof brain activity across multiple voxels or sensors, thereby facilitating the identification of cognitive states\\nfrom thorough and varied neural data. But why do many studies in this category begin by using MVPA? The\\nprimary objective of conducting MVPA analysis before initiating the similarity analysis between MEG and\\nANN is to help identify patterns and features in the data most relevant for distinguishing between different\\nmental states or stimuli. Applying MVPA first effectively reduces the dimensionality of the MEG data. It\\nbrings focus to the most informative features, which can then be used to compare stimulus representations\\nacross artificial and biological networks.\\nIt is helpful to note here that out of the studies which used MVPA, three actually used MVPA results (i.e.\\ndecoding accuracy) to build similarity measures ([119, 120, 132]). Furthermore, some studies described the\\nentire process of classification followed by similarity analysis as MVPA.\\n3.3.3\\nThe data and preprocessing\\nAcross the 16 modeling studies reviewed, the number of participants ranged from 11 to 92 (median = 15), and\\nthe number of trials or samples ranged from 77 to 66,300 (median = 3,585). However, five studies did not\\nreport the number of samples, and none reported the number of trials used for model training explicitly, as\\nthese studies did not train ANNs on MEG data.\\nIt is important to note that, unlike classification studies where MEG data serve as input to ANNs, modeling\\nstudies in this review did not train ANNs using MEG signals. Instead, it primarily compares ANN activations\\nwith MEG recordings, observing responses to the same visual or auditory stimuli presented separately to\\nboth systems. For instance, in the study by Kietzmann et al. ([125]), the input data consists of images from\\ndifferent categories (animate or inanimate objects, faces, etc). Although MEG data are not used as inputs to\\nthe ANNs in these studies, they are still used to compare the representations in brain signals and the latent\\nspace variables of ANNs.\\nAll MEG data collected for the studies in this category were preprocessed using a similar procedure, which\\nin principle consists of a band-pass filter, an artifact removal and/or correction technique, and data down-\\nsampling. The band-pass filtering’s lower and upper cutoff frequencies are 0.03 or 0.1 Hz to 300 or 330\\nHz. However, there seems to be no discernible pattern explaining the sampling frequencies (figure 3), the\\nde-noising, or artifact removal techniques used (see table 4). Among the 10 studies using RSA, five have used\\nsource reconstruction before computing RDMs ([122, 125, 131, 127, 128]). It is worth noting that only two\\n21'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 21}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nstudies explored the frequency domain of MEG data by investigating the similarities in one or more frequency\\nbands ([132, 130]).\\n3.3.4\\nNetwork architectures\\nThe choice of ANN architecture is a particularly relevant aspect of the ANN-MEG modeling studies because\\nthe ultimate goal here is to assess and interpret similarities and discrepancies between the way information is\\nprocessed in biological and artificial networks. As a general rule, the articles that focus on the visual cortex use\\na CNN architecture trained on image classification. Out of these studies, one incorporates lateral connections\\nbetween layers, in addition to the typical top-down connections, introducing temporal relationships in their\\nANN ([125]). In this study, the ANN is trained to learn how to recreate MEG data RDMs from the stimuli\\nimage displayed to the subject. This is achieved through changing the ANN’s objective function. About half\\nof the studies interested in the auditory cortex use LSTM ([132, 129]), the remaining studies use transformer\\nnetwork and a CNN ([130]), an MLP architecture ([131]) or the BERT ([189]) model ([134]). Finally, in\\n[127], the authors investigated the neural responses for a visual word recognition task by comparing the visual\\ncortex’s response patterns to those of CNN architectures.\\nWe found that while a few studies in this category train the networks from scratch, the majority (nine out\\nof the 16) use pre-trained architectures from well-established types of networks, including VGG variants\\nsuch as VGG-S (streamlined), VGG-F (fast), and VGG-11 (11 layers), which excel in image recognition.\\nOther popular architectures include AlexNet, known for its effectiveness in image classification; CORnet-S, a\\nbrain-inspired model for predicting neural responses ([190]); and BERT, a transformer-based model widely\\nused in natural language processing.\\n3.3.5\\nTraining and validation techniques\\nIn modeling studies, the emphasis is placed less on predictive performance and more on the alignment\\nbetween ANN representations and brain responses. As a result, training and validation procedures are not\\nalways detailed with the same rigor as in classification-focused work. Still, when custom architectures are\\ntrained, typical practices include specifying the optimization algorithm, loss function, and stopping criteria,\\nthough many studies using pre-trained networks omit these details entirely.\\nAmong the studies that reported their training procedures, most adopted standard deep learning practices\\nsuch as stochastic optimization and loss minimization over a supervised objective. The Adam optimizer was\\nfrequently employed due to its computational efficiency and robustness to noisy gradients, although few\\npapers provided full training specifications. In modeling studies, ANNs are typically pretrained or fine-tuned\\non tasks unrelated to MEG data though these tasks are often similar to the experimental conditions used to\\ncollect the MEG data (e.g., object recognition, language processing). Training and validation procedures\\nfocus on the model’s primary objective (e.g., classification or language modeling), while representational\\nalignment with MEG signals is assessed in a separate analysis using similarity metrics such as RSA. These\\napproaches differ fundamentally from classification pipelines, as MEG data is not used to train or validate the\\nmodel itself, but rather to evaluate how well its internal representations reflect brain activity.\\n3.3.6\\nEstimating performance baselines\\nAssessing baseline performance is crucial for benchmarking and evaluating the significance of the observed\\nsimilarities. Essentially, by establishing what can be expected by chance, or alternatively, what is maximally\\nachievable given the noise in the data, we can confidently assert whether the similarities measured by RSA\\nreflect genuine and meaningful correspondences between the representations in artificial neural networks\\nand those observed in biological neural activity. In the following, we review the main methods of baseline\\nperformance used in the reviewed papers, focusing on noise ceilings (NC) and assessment of the untrained\\nmodel’s performance.\\n22'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 22}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nNoise ceiling, or the shared-response model (SRM) comparison, refers to the theoretical upper limit or\\nmaximum level of similarity that can be achieved in the absence of measurement noise or variability in brain\\ndata. In other words, a noise ceiling provides an estimate of the best performance any model can achieve\\ngiven the noise in the data. As such, it serves as a benchmark for the observed similarities between ANNs\\nand the brain, indicating the extent to which individual subjects’ brain responses can be explained with a\\nmodel-free approach. This concept often acts as a proxy for signal-to-noise ratio analysis, aiding researchers\\nin interpreting the significance and reliability of their observed similarities by considering the inherent noise\\nor variability in the data. As is common in RSA, the upper noise ceiling is estimated as the mean correlation\\nbetween the group-average RSM and each participant-specific RSM. The lower noise ceiling is estimated as\\nthe mean correlation between the group-average RSM and each participant-specific RSM while iteratively\\nexcluding a given participant from the group-average. Some studies report both upper and lower noise ceilings.\\nAlthough it is an essential measure, only five of the surveyed studies reported it. Among them, four used RSA\\nas a similarity method ([122, 123, 125, 128]), one used neural predictivity ([130]) and the last used profile\\nresponses ([127]). For detailed instructions on how to compute the noise ceiling, refer to [123].\\nUntrained models performance as baseline: In RSA studies, a common approach to assess performance is\\nto compare the results obtained with the trained ANN model to those obtained with an untrained ANN model.\\nSome RSA-based studies (e.g. [191]) have found that even randomly initialized models can exhibit some\\nsimilarity to the neural representation. Such an intrinsic similarity could result from built-in properties of\\nspecific models’ architecture (e.g., convolutional layers resembling the visual cortex). However, most RSA\\nstudies make implicit assumptions (or at least have some expectations) that training ANNs would lead to\\nenhanced ANN-brain similarities. In this context, comparisons of RSA results obtained with trained and\\nuntrained models can be very informative. Interestingly, out of the sixteen studies we found in this category,\\nonly three contrast their modeling results with those of a random model ([119, 127, 130]).\\nWhile most studies focus on a single architecture, some include multiple ANN models. By conducting\\nRSA analyses between the brain data and each one of the models, such studies can pick up the specific\\nnetwork architecture and training properties that increase the similarities between the ANN and biological\\nresponses. In [122], the authors employed a feature-based model where RDMs are computed based on the\\nfeatures extracted from the stimuli. In [119], the authors added a model trained on noise and an unecological\\nmodel where images have been assigned random labels. Furthermore, in [123], the authors compared their\\nsimilarities with those obtained by a semantic model. In [128], the authors use semantic models that measure\\nsemantic similarity between objects, i.e., the degree of resemblance in meaning between two pieces of text,\\nsuch as words or sentences. In addition, a new trend consists of training the same ANN on several related tasks\\n(or use different objective functions) to investigate how each training goal shapes the learned representations\\n([192, 193, 194]).\\n3.4\\nOther studies\\n3.4.1\\nStudy aims and subcategories\\nThe final group of studies reviewed includes all MEG-related research involving artificial neural networks that\\ndo not fall neatly into either the ’Classification’ or ’Modeling’ categories. These works span a broad range of\\nobjectives and methodologies, including preprocessing pipelines, source localization, and the development\\nof novel methods or architectures. Despite their diversity, these studies share a common goal: improving\\nthe utility, interpretability, and methodological foundations of MEG analysis using artificial neural networks.\\nThis category includes 33 studies and can be divided into three broad subcategories.\\nThe first subcategory focuses on preprocessing techniques [145, 144, 147, 146, 143, 142, 148, 149], proposing\\nANN-based tools to enhance signal quality by improving artifact detection, SNR, or visualization of neural\\ndynamics. These studies often frame their contributions as supplements to or replacements for traditional\\ntechniques like ICA or wavelet filtering.\\n23'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 23}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nThe second subcategory encompasses studies addressing the source localization problem, also known as\\nthe MEG inverse problem [135, 166, 137, 139, 136, 167, 140, 138, 141]. These studies typically train\\nANNs, often using simulated MEG data, to predict source locations and commonly evaluate performance\\nby comparison to established inverse modeling techniques such as minimum norm estimate (MNE) [195],\\nBeamforming [196], or sLORETA [197].\\nThe final group includes methods-oriented studies [157, 152, 150, 151, 153, 156, 154, 161, 155, 163, 159,\\n160, 162, 165, 164, 158] that introduce new architectures, training strategies, or analysis frameworks designed\\nto advance ANN-based MEG research. These studies often overlap with classification tasks but are primarily\\nmethodological in focus, aiming to improve neural representation learning, domain adaptation, or multimodal\\nalignment rather than solving specific neuroscientific questions.\\nAlthough the aims of the studies in this category are heterogeneous, they collectively demonstrate the potential\\nof ANNs to enhance the entire MEG analysis pipeline—from raw data handling to high-level inference. In\\nthe following sections, we outline the typical processing workflows used across these studies and examine\\ntheir data, architectures, training strategies, and evaluation frameworks.\\n3.4.2\\nThe pipeline\\nGiven the diverse aims of the studies in this category, the pipelines vary considerably depending on whether\\nthe study focuses on preprocessing, source localization, or methodological innovation. Nonetheless, each\\nsubcategory follows a relatively coherent structure that reflects its specific objectives. Figure 4 (right panel)\\nillustrates a generalized pipeline applicable to many ’Other’-categorized studies.\\nIn preprocessing-focused studies, the pipeline generally begins with raw MEG data acquisition followed by\\nsegmentation and optional band-pass filtering. These signals are then passed to an ANN architecture trained\\nto detect and correct artifacts, enhance signal quality, or extract meaningful representations. For instance,\\nsome studies used deep convolutional networks to classify time-series segments as clean or contaminated by\\nartifacts such as blinks, saccades, or heartbeats [148], while others proposed autoencoder-based approaches\\nto denoise the data and simultaneously improve interpretability [149].\\nIn source localization studies, the pipeline often begins with simulated dipolar sources projected to sensor\\nspace using forward models. These simulated signals, sometimes mixed with noise at different levels, serve\\nas input for training ANNs tasked with predicting the original cortical source locations. Once trained, these\\nmodels are validated either on additional simulated datasets or real MEG recordings. The ANN output typically\\nincludes spatial maps or coordinate predictions that are compared against known ground truths or the output\\nof classical methods like MNE, Beamformers, or sLORETA [135, 166, 137, 139, 136, 167, 140, 138, 141].\\nMethods-focused studies typically design and validate new architectural or learning frameworks, often\\nrepurposing existing MEG datasets for benchmarking. The pipeline usually involves adapting a neural network\\nfor a specific task—such as decoding, temporal forecasting, or multimodal alignment—and then comparing\\nits performance to standard models or techniques. In these studies, the output may be a classification, a\\nreconstructed signal, a latent representation, or a learned alignment between modalities [157, 152, 150, 151,\\n153, 156, 154, 161, 155, 163, 159, 160, 162, 165, 164, 158]. In some cases, these architectures are trained to\\ngeneralize across datasets or tasks, with evaluation metrics designed to test their robustness, transferability, or\\nexplanatory power.\\nDespite the variability in input-output goals across these pipelines, what unites them is the emphasis on\\nenhancing MEG data processing through neural network-driven components. Whether applied to raw data,\\nsource estimation, or methodological refinement, ANNs are increasingly used as flexible tools capable of\\nimproving the accuracy, interpretability, or automation of MEG workflows.\\n24'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 24}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n3.4.3\\nThe data and preprocessing\\nThe types of data used in this category are as varied as the study objectives, encompassing raw MEG\\nrecordings, independent components (ICs), connectivity graphs, and even anatomical MRI when required for\\nsource modeling. Most preprocessing and method studies worked directly with raw or minimally processed\\nMEG signals, while source localization studies typically relied on simulated data for training and anatomical\\npriors for real-world testing.\\nAcross the 33 studies in this group, the number of trials used ranged widely, from as few as 294 to over\\n620,000, and the number of subjects varied between three and 676. However, nine studies did not report the\\nsize of their dataset, and two omitted the number of participants. When MEG data were used, downsampling\\nwas common. In most cases, the data were resampled to around 250 Hz (see figure 3, although some studies\\nretained higher sampling rates—such as 2000 Hz [141] or 2034 Hz [152]—to preserve fine-grained temporal\\ninformation. Preprocessing routines generally included band-pass filtering and artifact removal, although\\ndetailed procedures were not always specified.\\nIn preprocessing-oriented studies, the raw signals were typically cleaned using either manual ICA, automatic\\nartifact detection algorithms, or ANN-based classifiers. For example, one study introduced time contrastive\\nlearning (TCL) as an unsupervised alternative to traditional ICA for separating neural from non-neural sources\\n[142], while another employed a hybrid deep learning architecture combining 1D and 2D CNNs to detect and\\nremove a wide range of artifacts, including eye and cardiac activity [148]. Others used denoising autoencoders\\nto enhance SNR and support visualization of the signal structure [149].\\nIn the source localization subcategory, seven studies explicitly used simulated datasets for training, leveraging\\nground truth source positions to supervise the learning process [139, 137, 166, 167, 149, 138, 73]. These\\ndatasets were often augmented with varying levels of noise to ensure robustness. After training, the models\\nwere validated on real MEG recordings, with or without coregistered MRI data. Anatomical information\\nwas used to refine spatial accuracy, either by constraining predictions to the cortical surface or by integrating\\nMRI-based head models into the forward projection process.\\nWhile some preprocessing and methods papers reused data from previous decoding studies, others evaluated\\ntheir approach across multiple datasets to test generalizability. Despite this opportunity, only nine of the 16\\nmethods-focused studies applied their model to more than one dataset or task type [156, 152, 151, 161, 163,\\n162, 160, 164, 165]. This highlights the ongoing challenge of establishing robust benchmarks in ANN-based\\nMEG methodology research.\\n3.4.4\\nNetwork architectures\\nThe diversity of goals in this category is reflected in the wide variety of network architectures employed.\\nWhile CNNs dominated preprocessing and methodological studies, source localization studies tended to rely\\non architectures with temporal modeling capabilities such as recurrent networks.\\nIn preprocessing and methods papers, CNN-based architectures were the most commonly used. These\\nincluded both standard 1D or 2D CNNs [144, 148, 146, 145, 143, 147, 153, 156, 150, 166, 155, 160, 162,\\n141, 165, 164] and more specialized variants such as attention-augmented CNNs [152, 140] or recurrent\\nCNNs [151]. The choice of CNNs is often motivated by their ability to capture spatiotemporal patterns\\nin MEG signals, and in some cases, to operate directly on time-frequency representations or connectivity\\nmatrices. Autoencoders were also employed, especially in unsupervised preprocessing pipelines aiming to\\ndenoise or reconstruct input signals [149].\\nIn source localization studies, architectures that explicitly modeled temporal dynamics were more common.\\nThese included long short-term memory (LSTM) networks and other types of RNNs, which are well-suited\\nto tracking changes in MEG activity over time. Some studies also explored ResNet-based models [138] or\\nmore novel frameworks such as the multiple penalized state space (MPSS) architecture, designed to integrate\\n25'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 25}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nspatial and temporal priors [167]. A few studies combined recurrent and convolutional elements to capture\\nboth local and distributed features in the MEG signals.\\nMethod-focused studies, in contrast, emphasized innovation in architecture or training paradigms. For\\ninstance, one study introduced a contrastive learning framework to align neural and visual representations\\nacross modalities [165], while others trained foundational models for MEG forecasting and decoding,\\nachieving state-of-the-art performance across several benchmarks [163]. Another novel direction was the use\\nof generative adversarial networks, such as CycleGAN, to enhance signal translation between domains or\\nconditions [161].\\nAcross all subcategories, the architectures were tailored to specific goals—whether improving signal quality,\\nsolving inverse problems, or developing transferable decoding systems. However, these studies also high-\\nlighted that ANN architecture alone is rarely sufficient: success often depends on the thoughtful combination\\nof network design, data preparation, and evaluation strategies.\\n3.4.5\\nTraining and validation techniques\\nTraining and validation procedures varied across studies in this category, depending largely on the objective\\nand data type used. Nonetheless, most studies acknowledged the importance of robust evaluation to assess\\ngeneralization performance, especially in the context of methodological innovation or when working with\\nsmall datasets.\\nIn preprocessing and method-oriented studies, training typically followed standard supervised or unsupervised\\nlearning procedures. When labeled data were available, supervised learning was used with optimization\\nalgorithms such as Adam or SGD, often in combination with dropout, batch normalization, and early stopping\\nto prevent overfitting. Several studies also employed data augmentation strategies to improve generalizability,\\nincluding sensor shuffling, temporal jittering, or synthetic data generation [152, 148, 146, 161]. In studies\\nleveraging unsupervised approaches like autoencoders or contrastive learning, training aimed to minimize\\nreconstruction loss or maximize representation alignment rather than task-specific accuracy.\\nValidation strategies in these studies included simple train/validation/test splits, K-fold cross-validation, and\\nLOSO schemes. LOSO was most commonly used when generalizing across participants was crucial or\\nwhen datasets were small. K-fold and bootstrap resampling methods were employed to ensure robustness,\\nparticularly when evaluating architectural innovations across multiple configurations. In method papers\\nproposing general-purpose architectures, performance was often tested across different tasks or datasets to\\ndemonstrate transferability.\\nIn source localization studies, validation was often carried out in two stages. First, models were trained\\nand evaluated on simulated data with known source locations and varying levels of noise, allowing precise\\nquantification of localization error using spatial accuracy metrics or distance from the ground truth. Second,\\nthe models were applied to real MEG data, and their outputs were compared to those produced by traditional\\nmethods such as MNE, Beamforming, or sLORETA [139, 137, 166, 167, 149, 138, 73].\\nOverall, validation was a key element in determining the reliability and potential applicability of the proposed\\nnetworks. However, the breadth of tasks and data types makes direct comparisons across studies challenging.\\nGreater consistency in reporting training durations, regularization strategies, and evaluation metrics would be\\nbeneficial for future work seeking to benchmark ANN-based pipelines in MEG analysis.\\n3.4.6\\nComparison to classical approaches\\nA distinguishing feature of many studies in this category is their explicit comparison between ANN-based\\nmethods and more traditional MEG analysis techniques. These comparisons were critical for justifying the\\nadoption of neural networks, particularly in areas where classical approaches remain well-established, such\\nas source localization or artifact correction.\\n26'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 26}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nIn the source localization subcategory, ANN models were frequently benchmarked against established inverse\\nsolutions, including minimum norm estimate (MNE) [195], Beamformer [196], and variations of LORETA\\n(e.g., eLORETA, sLORETA) [197]. Metrics used for comparison included localization error, area under the\\ncurve (AUC), and spatial dispersion. Several studies showed that ANN-based models —particularly those\\ntrained on simulated data— were capable of achieving localization accuracy that matched or exceeded classical\\nmethods, especially in low signal-to-noise regimes or when dealing with complex source configurations [139,\\n137, 166, 167, 149, 138, 73]. Recent contributions, such as ConvDip [198], demonstrated that convolutional\\narchitectures can provide competitive performance while being more robust to noise and variability in source\\nconfigurations.\\nIn preprocessing-focused studies, the goal was often to improve artifact detection and signal enhancement\\nbeyond what traditional ICA or heuristic thresholding could achieve. For instance, one study replaced\\nICA with time contrastive learning (TCL), an unsupervised deep learning framework that automatically\\nseparated neural from non-neural sources [142]. Another employed a deep CNN-based artifact classifier that\\noutperformed manual rejection and conventional statistical approaches in identifying blinks, heartbeats, and\\nother common contaminants [148].\\nIn the methods subcategory, comparisons to classical baselines were essential for demonstrating the added\\nvalue of architectural or algorithmic innovations. Thirteen studies explicitly compared their ANN models to\\nsimpler or more common alternatives [145, 144, 136, 166, 158, 139, 137, 159, 163, 160, 162, 140, 141]. These\\ncomparisons often focused on decoding accuracy, robustness to inter-subject variability, and computational\\nefficiency. Some studies emphasized that classical models require extensive feature engineering or hand-tuned\\npipelines, while neural networks can learn directly from the data and generalize more flexibly across tasks\\nand domains.\\nDespite the promising results reported in many of these comparisons, a number of studies also highlighted the\\ninterpretability and reproducibility challenges inherent to ANN-based methods. As such, the integration of\\nneural networks into the broader MEG analysis landscape is often framed not as a wholesale replacement of\\nclassical techniques, but as a complementary approach that may offer advantages under specific conditions.\\n4\\nDiscussion\\n4.1\\nAdded value of combining ANN with MEG\\nANNs are increasingly used alongside neuroimaging modalities such as fMRI, EEG, and MEG to enhance\\nour understanding of brain function. fMRI is highly valued for its spatial resolution and ability to map\\ncognitive states through blood flow changes ([9]). However, its slower temporal resolution limits its capacity\\nto capture rapid neural dynamics. In contrast, both MEG and EEG offer the temporal precision necessary\\nfor analyzing fast brain activities. However, MEG distinguishes itself with superior spatial resolution and\\nreduced susceptibility to scalp and skull distortions ([8, 12, 6]).\\nPreprocessing steps, such as artifact removal and feature extraction, are crucial for ensuring that the high-\\ndimensional MEG signals are effectively utilized by ANNs. These steps often form the foundation for\\ndownstream tasks by improving signal clarity and optimizing data input for architectures like CNNs, RNNs,\\nand transformers ([143, 144, 142]). In addition to preprocessing, ANNs are widely applied in MEG for\\nclassification tasks, including decoding ([119, 59]), BCIs ([93, 103]), clinical diagnostics ([104, 111]), and\\nevent detection ([110, 51]). By leveraging MEG’s temporal precision, ANNs have been used to analyze neural\\nsynchronization patterns, facilitating insights into cognitive processes such as perception, attention, language\\nprocessing, and decision-making. For example, in neural classification, MEG’s precise temporal information\\nsupports real-time detection of complex states ([97, 55]); in BCI applications, it enhances user control through\\nresponsive spatiotemporal decoding ([103, 93]); in clinical diagnostics, it enables more accurate seizure\\n27'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 27}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\ndetection by addressing signal distortions; and in event detection ([83, 104]), it allows subtle oscillatory\\nchanges to be captured with higher fidelity ([110, 152]).\\nBeyond these applications, MEG–ANN integration plays a critical role in modeling studies, where the\\ngoal is to explore how MEG signals align with representations learned by neural networks. These studies\\ncompare MEG data with internal ANN activations, providing insights into how cortical activity corresponds\\nto hierarchical network processing ([120, 123, 121]). MEG’s high temporal and spatial resolution provides\\na unique advantage in tracking dynamic neural activity, making it particularly well-suited for investigating\\nhow the temporal dynamics of ANN activations align with neural processes in the human brain, especially in\\ndomains like vision and audition, where precise timing is critical ([119, 130]).\\n4.2\\nCurrent trends and dominant applications of ANNs in MEG\\nTaken together, the reviewed body of literature reveals a notable surge in publications and highlights the\\ndiverse applications of ANNs in MEG data analysis. This reflects not only a growing appreciation for\\ndata-driven approaches in neuroscience but also an increased interest in the unique contributions of ANNs\\nto the field. Based on the papers we reviewed, it seems that the excitement about the potential of ANNs for\\nadvancing MEG research is distinct from the recent surge in using standard ML techniques ([5]).\\nOur review shows that the power and versatility of deep learning is opening up frameworks for exploring\\nMEG data that go beyond advanced statistical analyses. While many of the papers reviewed do indeed\\nleverage the power of ANNs for classification tasks ([119, 97, 93]), many studies benefit from other strengths\\nof ANNs. In particular, the reviewed literature shows that modeling studies that compare representations\\nacross ANNs and human MEG data seem to be gaining momentum ([121, 129, 131]).\\nMethodological developments for MEG data analytics are increasingly leveraging the strength and versatility\\nof ANNs, including the introduction of novel architectures, enhanced data preprocessing techniques such\\nas artifact detection and correction ([142, 145]), innovative source reconstruction approaches ([166, 138]),\\nand the development of foundation models ([163, 115, 199]). These methodological innovations not only\\nbroaden the scope of MEG applications but also accelerate the pace of MEG research. That said, this review\\nalso highlights that there are still several critical limitations, including issues with interpretability and data\\nscarcity, which will need to be addressed for the field to harness the potential of ANNs in MEG analysis\\nfully. Further advancements —possibly including more robust foundation models for MEG that better handle\\nlimited datasets and enhance generalizability— are necessary to improve model transparency and effectively\\nmanage limited datasets to achieve more reliable and generalizable results. In the next section, we will explore\\nin greater detail the bottlenecks and challenges that emerge from the corpus of studies included in this review.\\n4.3\\nCurrent limitations and challenges\\nThe diversity and wide range of ways ANNs have been applied to MEG research reflect a correspondingly\\nbroad spectrum of challenges and limitations. To provide clarity, we categorize the predominant issues\\nobserved across the reviewed literature into a concise set of topics, offering strategic guidance where possible\\nto enhance the robustness, efficacy, and reproducibility of future work in this rapidly evolving field. First, we\\naddress the general data scarcity problem prevalent in MEG-based neuroimaging studies. Next, we underscore\\nthe importance of model validation techniques and the need for accurate performance evaluation. We then\\ndiscuss the critical role of hyperparameters in ANN-based MEG studies. Lastly, we examine the issue of\\nreproducibility in this domain.\\n4.3.1\\nDealing with data scarcity\\nIt is recommended that data augmentation techniques be used when feasible to cope with scarce data. For\\ninstance, common approaches for time-series data like M/EEG include adding noise, applying transformations\\n28'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 28}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nsuch as amplitude scaling or time warping, or generating synthetic trials using generative models. For\\nevent-based epochs, introducing different event timings for each trial window will increase the sample size.\\nIn the case of continuous data, it is possible to use overlapping segments to artificially increase dataset size.\\nDepending on the type of study and data, various data augmentation techniques can be added to a study’s\\npreprocessing pipeline. For a systematic comparison and more details on available methods specifically\\nevaluated on EEG, many of which are applicable to event-related MEG data, we refer the reader to [200].\\n4.3.2\\nModel validation techniques\\nIn many neural network applications, the vast amount of data combined with the typical training duration\\nfor ANNs makes cross-validation an impractical method for model validation. Instead, most of the time, the\\ndataset is randomly split into three subsets to validate the model: training, validation, and test (also called\\nevaluation set). The training set generally represents 50 to 80 percent of the initial dataset, and the remaining\\ndata is split in half to create the validation and test sets. The training set is employed to train the network\\nwith a set of hyperparameters and tested on the validation set. The test set is only used at the end when\\nthe architecture and hyper-parameters are fixed and once the desired performance has been reached on the\\nvalidation set. It is solely used to evaluate the final model performance. However, when data is scarce, as it is\\noften the case for neuroscience studies, it is recommended to use re-sampling methods such as bootstrapping\\nor cross-validation to make use of the data as much as possible ([177]). It will allow the train and validation\\nsets to be changed several times to acquire a less noisy measure of the model’s performance. In the studies\\nincluded in this survey, when cross-validation is used, K-fold was the preferred technique, with the value of K\\nranging from 4 to 50. When the goal is to test generalization to new subjects, the leave P subjects out (LOPO)\\ncross-validation or a stratified version of K-Fold is recommended.\\n4.3.3\\nModel performance evaluation and baseline comparisons\\nSome classification studies did not include baseline results against which one could have compared the\\nreported ANN performances. This can be a relevant concern given that classical shallow ML approaches\\n(e.g. logistic regression [201], random forest [179], SVM [202], etc.) often involve simpler classifier\\nimplementations and potentially faster training times —once appropriate features have been engineered—,\\ncompared to designing and training complex ANNs. However, the overall practical advantage depends\\nheavily on the computational complexity of the necessary feature extraction pipeline, which can itself be\\nintensive and time-consuming, potentially offsetting the gains in classifier simplicity. Additionally, small\\nsample size can lead to inflated accuracies, that can substantially exceed the theoretical chance level of\\n50% for two classes (eg. reaching 70% purely by chance). Therefore, rigorous statistical evaluations, such\\nas permutation tests are required to properly establish meaningful baseline performances, thus ensuring\\nrobust assessments of classifier significance [203]. Generally, one should always evaluate simple solutions\\nbefore spending time and effort on more complex ones, which might also be more challenging to interpret.\\nHowever, only a few articles among the modeling studies mentioned some kind of baseline performance\\ncomparison ([122, 123, 119, 120, 127, 130]). Ideally, modeling studies should report NC and/or evaluate the\\nperformance of their similarity analysis with an untrained model. Together, these measures would reinforce\\nrobust similarity scores’ reliability and help identify and discard weaker results.\\n4.3.4\\nHyperparameter settings\\nIn recent years, we have witnessed the development of increasingly deep networks aimed, for a large part,\\nat enhancing performance in image classification tasks. However, the studies reviewed here reveal a trend\\ntoward relatively shallower network architectures. Intriguingly, when depth is evaluated as a hyperparameter\\nin these studies, it appears to have a lesser impact on performance compared to other parameters, such as\\nlayer width or filter size. When using ANNs, the training phase is a pivotal component, and an incorrect\\nselection of the optimizer and parameters could result in a network that fails to learn. Unfortunately, 16 of the\\n29'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 29}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nclassification studies did not provide enough information about the hyperparameters used. However, most\\nstudies specify the loss function, optimizer, learning rates, batch size, and the number of epochs, including\\nthe early stop condition when applicable. It is essential to systematically detail a model’s hyperparameters\\nand training regimen (in any type of ANN study) to ensure the ability to reproduce results or to use the\\nsame model similarly with different data. In this context, a recommended practice is to also provide the\\nsource code alongside all the parameters and hyperparameters used to generate any given result of a study.\\nFurthermore, given the dependence of optimal hyperparameters, including network depth, on the specific\\napplication, researchers designing networks for their own MEG studies may find the detailed information\\ncollated in tables 2, 3, 4, and 5 useful as a starting point or source of inspiration. Lastly, it’s worth mentioning\\nthat many studies in our review employ batch sizes that are not powers of two. However, using batch sizes\\nthat are powers of two is recommended to optimize computation and reduce training times ([204]).\\n4.3.5\\nVariability in MEG data acquisition, preprocessing, and experimental protocols\\nOne of the main challenges in applying ANNs to MEG research is the variability in acquisition hardware,\\npreprocessing steps, and experimental protocols across studies. These differences can affect data quality and\\nconsistency, which in turn influence model performance and the ability to compare results across studies.\\nAt the acquisition level, studies used different sensor types (e.g., magnetometers, planar gradiometers, axial\\ngradiometers) and MEG systems (e.g., Elekta Neuromag, CTF, BTi), with sampling frequencies ranging\\nfrom 50 Hz to 2400 Hz. Preprocessing methods also varied: some studies used ICA (often via EEGLAB or\\nFieldTrip) for artifact rejection, others applied wavelet denoising or relied on proprietary tools. In 39 studies,\\npreprocessing steps were not clearly reported. Segment lengths and filtering parameters also differed, even in\\nstudies using similar tasks.\\nExperimental protocols varied in terms of task design (e.g., resting-state, oddball, language processing, or\\nBCI tasks), number and timing of trials, baseline usage, and whether the data were epoched or treated as\\ncontinuous. This variability makes it difficult to directly compare ANN architectures or performance metrics.\\nTo address these challenges, future studies would benefit from more consistent preprocessing pipelines,\\nstandardized task protocols where feasible, and clearer reporting of methods. Efforts such as the brain imaging\\ndata structure extension for MEG (MEG-BIDS [205]) and open datasets with thorough documentation can\\nsupport better reproducibility and enable more reliable benchmarking across studies.\\n4.3.6\\nThe issue of reproducibility\\nIn the field of neuroscience, concerns about the reproducibility of results are prevalent. The substantial costs\\nand privacy issues related to acquiring neuroimaging or behavioral data restrict the availability of open-access\\ndatasets, thereby challenging the reproducibility of research conducted with such data ([206, 207]). In\\naddition, this challenge is compounded by numerous studies that did not include enough information about\\nthe network architecture or the training parameters or did not provide truly open access to their code, from\\nwhich this information could be extracted. While some studies state code is ’available upon request’, this\\noften proves ineffective in practice, with requests frequently going unanswered or being declined ([208]),\\nmeaning truly accessible code is likely rarer than implied. This overall lack of transparency and accessibility\\nfurther impedes the reproducibility of the results, making it difficult for researchers to validate and build on\\nprevious findings. The application of ML to neuroscience data faces challenges, mainly due to the relatively\\nsmall/moderate sample sizes in neuroscience datasets. Besides, most algorithms require large amounts of data\\nto achieve robust performance. Moreover, small datasets will make generalization harder to reach since having\\nfewer data will further increase the impact of inter-subject variability. Solutions exist to tackle problems\\ncaused by small datasets, such as cross-validation, bootstrapping, and data augmentation. Still, more data will\\nalways be a preferable approach to achieve better results ([1]). However, the trend of small private datasets\\n30'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 30}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nin the world of neuroscience is slowly changing with increasing awareness around the importance of open\\nscience practices, and public datasets are becoming more common.\\nThe utility of these public datasets for reproducible research and cross-study comparisons is greatly en-\\nhanced by standardized data formats and organizational structures, such as MEG-BIDS ([205]).\\nRe-\\nsources providing standardized, open-access MEG recordings suitable for ANN analysis include repos-\\nitories and datasets such as OpenNeuro (https://openneuro.org/), the human connectome project\\n(https://www.humanconnectome.org/), the Cambridge centre for ageing and neuroscience (Cam-\\nCAN) dataset (https://camcan-archive.mrc-cbu.cam.ac.uk/dataaccess/), the open MEG archive\\n(OMEGA) database (https://www.mcgill.ca/bic/neuroinformatics/omega), and the MNE sam-\\nple datasets (https://mne.tools/stable/overview/datasets_index.html), offering valuable bench-\\nmarks for training, testing, and comparing models.\\n4.3.7\\nInterpretation and visualizations\\nIn BCI studies, where the primary concern is the algorithm’s performance, the black-box nature of ANNs is\\noften considered less problematic. However, in many other applications, including brain decoding studies,\\ngaining insight into what is happening in the model’s latent space is key to understanding how the information\\nwas extracted from the signal, as well as why and how decoding is possible. In domains like image\\nclassification or object detection, there are various tools (Grad-CAM [209], tSNE [210], UMAP [211]) that\\nallow the user to have a representation of what the network ’sees’ and what properties of the data are important\\nin its decision-making process.\\nA large proportion (52 out of 70) of the classification studies, where interpretation would be the most\\ninteresting, did not use these tools to gain insights into the representations learned and used by the model.\\nA similar pattern of limited application was observed in the ’Other’ category studies reviewed. While\\ninterpretability might be considered less critical for some preprocessing or source localization tasks compared\\nto decoding, it remains vital for evaluating novel methodological approaches. Indeed, roughly half of the\\n’Methods’ studies (9 out of 16) included visualization techniques such as filter or activation map analysis.\\nInterpretability tools were also used in a majority of the ’Preprocessing’ studies (5 out of 8), often employing\\nmethods like Grad-CAM [182]. However, interpretability techniques were notably absent in all reviewed\\nANN-based source localization studies. This suggests that developing and applying robust interpretability\\nmethods beyond classification and basic modeling tasks remains an important area for future work in MEG\\nresearch using ANNs.\\n4.3.8\\nNote of caution on inconsistent nomenclature\\nConsistent terminology is important for effective communication across all scientific disciplines. However,\\nit becomes particularly crucial in emerging fields where the community still defines methodologies and\\ncore concepts. Adopting a uniform nomenclature is essential in a rapidly evolving research domain, such\\nas the intersection of ANN and neuroimaging. Our extensive literature review reveals that terminology\\nwithin classification studies is fairly consistent. However, we noted that the nomenclature in other categories,\\nparticularly within modeling studies, tends to show less consistency. To give a few examples, some research\\npapers include RSA as a component of the MVPA analysis, while others do not. Additionally, some studies\\nuse neural predictability but do not explicitly mention it in their papers. In the modeling studies reviewed, the\\nanalysis pipelines exhibit considerable variety and flexibility, leading to notable diversity across the studies.\\nFor researchers looking to adopt these methodologies, we recommend a thorough examination of multiple\\nstudies. This approach will provide a comprehensive understanding of the various pipelines and help identify\\nthe methods most aligned with specific research objectives. Additionally, it is crucial to strive for consistency\\nin terminology with the existing literature to ensure clarity and help the community move towards established\\nprocedures and nomenclature. For those interested in RSA in particular, insightful comments and critiques of\\nthe method can be found in [212].\\n31'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 31}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n4.3.9\\nHardware and practical application constraints\\nAnother consideration involves the practical constraints of MEG for certain applications, notably BCIs. While\\nEEG is often considered more practical for widespread BCI use due to lower cost and portability, MEG’s\\narguably superior signal quality and spatial resolution offer potential advantages for specific BCI paradigms\\nrequiring high fidelity decoding, as explored in several studies reviewed here ([97, 99, 95]). However,\\ntraditional SQUID-based MEG systems face significant hurdles for practical BCI deployment, including high\\ncost, the need for magnetically shielded rooms, and strict subject immobility requirements. Although ANNs\\ncan contribute by improving the robustness of decoding complex MEG signals and potentially adapting to\\nnoise or variability (3.2), they do not resolve these fundamental infrastructural and hardware limitations.\\nPromisingly, emerging sensor technologies like optically pumped magnetometers (OPMs) may mitigate\\nthese physical constraints. OPMs operate without cryogenics, are wearable, and allow for subject movement,\\npotentially enabling more flexible, cost-effective, and practical MEG-based BCI systems in the future ([213]).\\n4.3.10\\nChallenges in ANN-based source reconstruction\\nApplying ANNs to the MEG inverse problem, i.e., source reconstruction, presents both opportunities and\\nsignificant challenges, as highlighted by the studies reviewed in section 3.4. Source reconstruction is\\ninherently ill-posed and highly sensitive to factors like sensor noise, assumptions about source activity, and,\\ncrucially, the accuracy of the head model used to compute the forward solution ([8, 29]). While ANNs offer\\npotential advantages, such as learning complex non-linear source-sensor mappings directly from data, their\\neffectiveness, particularly when trained on simulated data as is common practice ([166, 138]), is fundamentally\\nconstrained by the fidelity of these simulations.\\nIndeed, if an ANN is trained using data generated with an inaccurate forward model (due to simplified head\\ngeometry, incorrect conductivity values, or sensor misalignment), its ability to generalize and accurately\\nlocalize sources in real-world MEG data will be severely compromised. The network may perform well\\non simulated test data derived from the same flawed model but fail when applied to actual measurements.\\nTherefore, the development of effective ANN-based source reconstruction methods heavily relies on the\\ngeneration of high-quality training data using realistic simulations and, most importantly, accurate subject-\\nspecific forward models. Furthermore, even with accurate models, challenges remain in ensuring that\\nANNs do not simply learn biases present in the simulation protocols and in interpreting the learned source\\nrepresentations. It is therefore important to keep in mind that although ANNs may provide a powerful\\ndata-driven alternative to traditional inverse methods, they do not circumvent the fundamental physical\\nconstraints and modeling requirements inherent to MEG source localization.\\n5\\nConclusion\\nMachine learning, notably the use of ANNs, has become a cornerstone in contemporary research, reshaping\\nboth data analysis and modeling across diverse scientific fields. Although ANNs are relatively novel in the\\nrealm of neuroimaging, the collection of studies in this review demonstrates a rapidly growing interest in\\nemploying ANNs for both the analysis and modeling of MEG data. This trend is part of a broader movement\\ntowards integrating advanced data-driven and computational methods to deepen our understanding of neural\\nmechanisms. The large body of work we reviewed indicates that ANNs are increasingly recognized as\\npowerful tools in neuromagnetic imaging due to their robust performance, versatility, and ability to handle\\ncomplex datasets. A comparison between our figure 2a and figure 10b in [5] suggests a trajectory similar to\\nthat observed in the field of EEG, where ANNs have already shown extensive utility. This parallel points\\nto a likely expansion in the number of studies employing ANNs for MEG data, further diversifying the\\napplications and methodologies within the field.\\n32'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 32}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nOne of the most remarkable observations is the diverse range of applications for which ANNs are now\\nbeing employed with MEG data. While traditional ML tools like SVM or random forest classifiers are well-\\nestablished, particularly for brain decoding and classification tasks based on extracted features ([48]), ANNs\\nfacilitate a distinct spectrum of applications, as demonstrated throughout this review. Notably, their capacity\\nfor representation learning and building end-to-end systems has led to their use not only in classification but\\nalso in addressing the source estimation problem ([166, 138]), providing novel tools for data cleaning and\\nartifact rejection ([148, 149]), and, significantly, building and evaluating complex information processing\\nmodels of the human brain ([120, 131, 214]).\\nDespite the remarkable advances in AI tools and the new opportunities that ANNs present for MEG data\\nanalysis and modeling, it is critical to avoid the allure of AI hype and the rush to employ complex AI\\nalgorithms solely for their novelty. Many research questions may still be best addressed using standard\\napproaches that are well-established within the field and which often come with greater result interpretability.\\nMoreover, the extensive range of parameters and hyperparameters involved in ANNs, coupled with the\\nexpertise required for their proper implementation, can sometimes introduce errors or yield misleading\\noutcomes.\\nLooking ahead, the future of this emerging research appears ripe with promising developments. Two\\nparticularly exciting prospects stand out. First, advances in AI interpretability and visualization tools are\\npoised to significantly enhance the application of ANNs in MEG research, making these complex models\\nmore accessible and their findings more actionable. Second, the burgeoning adoption of AI foundation models\\nacross various disciplines, including neuroscience, is set to play a transformative role in both basic and clinical\\nMEG applications. The scientific community’s growing enthusiasm for developing foundation models for\\nbrain imaging modalities ([199, 215]) and time series data ([216]) suggests that neuroscience will not be\\nfar behind ([163, 115]). Several studies have successfully applied ANNs to multimodal neuroimaging data,\\nincluding MEG-EEG integration. For example, frameworks have been developed to process and fuse MEG\\nand EEG signals for enhanced decoding performance and artifact removal using deep neural architectures\\n([217, 218, 219]). These approaches highlight the potential of ANNs to handle the complementary strengths\\nof different modalities and suggest promising avenues for future multimodal brain decoding pipelines.\\nIntegrating such modalities could revolutionize our understanding of the brain and its disorders.\\n33'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 33}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nAcknowledgements\\nConflict of interest\\nThe authors declare that they have no conflicts of interest relevant to this work.\\nFunding\\nI.R. received support from the Canada CIFAR AI chair program and the Canada excellence research chairs\\nprogram. K.J. was supported by Canada research chairs program funding (950-232368). V.H. was supported\\nthrough the ianadian institutes of health tesearch project grant (166197). H.A. was funded through the faculty\\nof medicine at the university of Montreal’s merit scholarship. A.D. was funded through computational\\nneuroscience and cognitive neuroimaging lab (CoCoLab) and Montreal institute of learning algorithms\\n(MILA) scholarships.\\n34'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 34}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\nReferences\\n[1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\\n[2] Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz, Amelia\\nChristensen, Claudia Clopath, Rui Ponte Costa, Archy de Berker, Surya Ganguli, et al. A deep learning\\nframework for neuroscience. Nature neuroscience, 22(11):1761–1770, 2019.\\n[3] Anthony Zador, Sean Escola, Blake Richards, Bence Ölveczky, Yoshua Bengio, Kwabena Boahen,\\nMatthew Botvinick, Dmitri Chklovskii, Anne Churchland, Claudia Clopath, et al. Toward next-\\ngeneration artificial intelligence: Catalyzing the neuroai revolution. arXiv preprint arXiv:2210.08340,\\n2022.\\n[4] Dominik Walther, Johannes Viehweg, Jens Haueisen, and Patrick Mäder. A systematic comparison of\\ndeep learning methods for eeg time series analysis. Frontiers in Neuroinformatics, 17:1067095, 2023.\\n[5] Yannick Roy, Hubert Banville, Isabela Albuquerque, Alexandre Gramfort, Tiago H Falk, and Jocelyn\\nFaubert. Deep learning-based electroencephalography analysis: a systematic review. Journal of neural\\nengineering, 16(5):051001, 2019.\\n[6] Sanjay P Singh. Magnetoencephalography: basic principles. Annals of Indian Academy of Neurology,\\n17(Suppl 1):S107, 2014.\\n[7] Martine Gavaret, Jean-Michel Badier, Fabrice Bartolomei, Christian-Georges Bénar, and Patrick\\nChauvel. Meg and eeg sensitivity in a case of medial occipital epilepsy. Brain topography, 27:192–196,\\n2014.\\n[8] Sylvain Baillet, John C Mosher, and Richard M Leahy. Electromagnetic brain mapping. IEEE Signal\\nprocessing magazine, 18(6):14–30, 2001.\\n[9] Emma L Hall, Siân E Robson, Peter G Morris, and Matthew J Brookes. The relationship between meg\\nand fmri. Neuroimage, 102:80–91, 2014.\\n[10] Chamandeep Kaur, Preeti Singh, Amandeep Bisht, Garima Joshi, and Sunil Agrawal. Recent develop-\\nments in spatio-temporal eeg source reconstruction techniques. Wireless Personal Communications,\\n122(2):1531–1558, 2022.\\n[11] Fernando Lopes da Silva. Eeg and meg: relevance to neuroscience. Neuron, 80(5):1112–1128, 2013.\\n[12] Sylvain Baillet. Magnetoencephalography for brain electrophysiology and imaging. Nature neuro-\\nscience, 20(3):327–339, 2017.\\n[13] Aapo Hyvärinen and Erkki Oja. Independent component analysis: Algorithms and applications. Neural\\nNetworks, 13(4-5):411–430, 2000.\\n[14] Mike X. Cohen. Analyzing Neural Time Series Data: Theory and Practice. MIT Press, 2014.\\n[15] György Buzsáki. Rhythms of the brain. In Rhythms of the Brain. Oxford University Press, 2006.\\n[16] Hans Berger. Über das elektrenkephalogramm des menschen. Archiv für Psychiatrie und Ner-\\nvenkrankheiten, 87(1):527–570, 1929.\\n[17] C.-K. Peng, Shlomo Havlin, H. Eugene Stanley, and Ary L. Goldberger. Quantification of scaling\\nexponents and crossover phenomena in nonstationary heartbeat time series. Chaos: An Interdisciplinary\\nJournal of Nonlinear Science, 5(1):82–87, 1995.\\n[18] Bo Hjorth. Eeg analysis based on time domain properties. Electroencephalography and Clinical\\nNeurophysiology, 29(3):306–310, 1970.\\n[19] Harold Edwin Hurst. Long-term storage capacity of reservoirs. Transactions of the American society\\nof civil engineers, 116(1):770–799, 1951.\\n35'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 35}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[20] Steven M. Pincus. Approximate entropy as a measure of system complexity. Proceedings of the\\nNational Academy of Sciences, 88(6):2297–2301, 1991.\\n[21] Joshua S. Richman and J. Randall Moorman. Physiological time-series analysis using approximate\\nentropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology,\\n278(6):H2039–H2049, 2000.\\n[22] Christoph Bandt and Bernd Pompe. Permutation entropy: A natural complexity measure for time\\nseries. Physical Review Letters, 88(17):174102, 2002.\\n[23] Stéphane G. Mallat. A theory for multiresolution signal decomposition: The wavelet representation.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 11(7):674–693, 1989.\\n[24] Peter D. Welch. The use of fast fourier transform for the estimation of power spectra: A method\\nbased on time averaging over short, modified periodograms.\\nIEEE Transactions on Audio and\\nElectroacoustics, 15(2):70–73, 1967.\\n[25] Pascal Fries. A mechanism for cognitive dynamics: Neuronal communication through neuronal\\ncoherence. Trends in Cognitive Sciences, 9(10):474–480, 2005.\\n[26] Jean-Philippe Lachaux, Eugenio Rodriguez, Jacques Martinerie, and Francisco J. Varela. Measuring\\nphase synchrony in brain signals. Human Brain Mapping, 8(4):194–208, 1999.\\n[27] Cornelis J. Stam, Guido Nolte, and Andreas Daffertshofer. Phase lag index: A new measure of\\nfunctional connectivity from multi-channel eeg. Clinical Neurophysiology, 118(6):1302–1310, 2007.\\n[28] Martin Vinck, Robert Oostenveld, Marijn van Wingerden, Francesco Battaglia, and Cyriel M. A.\\nPennartz. An improved index of phase-synchronization for electrophysiological data in the presence of\\nvolume conduction, noise and sample-size bias. NeuroImage, 55(4):1548–1565, 2011.\\n[29] Matti Hämäläinen, Riitta Hari, Risto J. Ilmoniemi, Jukka Knuutila, and Olli V. Lounasmaa. Magne-\\ntoencephalography—theory, instrumentation, and applications to noninvasive studies of the working\\nbrain. Reviews of Modern Physics, 65(2):413–497, 1993.\\n[30] Danilo Bzdok. Classical statistics and statistical learning in imaging neuroscience. Frontiers in\\nneuroscience, 11:543, 2017.\\n[31] David Wipf and Srikantan Nagarajan. A unified bayesian framework for meg/eeg source imaging.\\nNeuroImage, 44(3):947–966, 2009.\\n[32] Denes Szucs and John Ioannidis. When null hypothesis significance testing is unsuitable for research:\\na reassessment. Frontiers in human neuroscience, 11:390, 2017.\\n[33] George A. F. Seber and Alan J. Lee. Linear Regression Analysis. Wiley, 2nd edition, 2003.\\n[34] Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in\\nthe brain. Psychological Review, 65(6):386–408, 1958.\\n[35] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data\\nMining, Inference, and Prediction. Springer, 2nd edition, 2009.\\n[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep con-\\nvolutional neural networks. Advances in Neural Information Processing Systems, 25:1097–1105,\\n2012.\\n[37] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\\nDropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning\\nResearch, 15:1929–1958, 2014.\\n[38] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. Proceedings of the 32nd International Conference on Machine\\nLearning, 37:448–456, 2015.\\n36'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 36}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[39] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning.\\nJournal of Big Data, 6(1):60, 2019.\\n[40] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\\nStatistics, 22(3):400–407, 1951.\\n[41] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by\\nback-propagating errors. Nature, 323(6088):533–536, 1986.\\n[42] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\\n[43] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–\\n1780, 1997.\\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing\\nSystems, 30:5998–6008, 2017.\\n[45] Yarden Cohen, Tatiana A Engel, Christopher Langdon, Grace W Lindsay, Torben Ott, Megan AK\\nPeters, James M Shine, Vincent Breton-Provencher, and Srikanth Ramaswamy. Recent advances at the\\ninterface of neuroscience and artificial neural networks. Journal of Neuroscience, 42(45):8514–8523,\\n2022.\\n[46] Joshua I Glaser, Ari S Benjamin, Roozbeh Farhoodi, and Konrad P Kording. The roles of supervised\\nmachine learning in systems neuroscience. Progress in neurobiology, 2019.\\n[47] Joshua I Glaser, Raeed H Chowdhury, Matthew G Perich, Lee E Miller, and Konrad P Kording.\\nMachine learning for neural decoding. arXiv preprint arXiv:1708.00909, 2017.\\n[48] Steven Lemm, Benjamin Blankertz, Thorsten Dickhaus, and Klaus-Robert Müller. Introduction to\\nmachine learning for brain imaging. Neuroimage, 56(2):387–399, 2011.\\n[49] Guangliang Yu, Xiang Li, Dawei Song, Xiaozhao Zhao, Peng Zhang, Yuexian Hou, and Bin Hu.\\nEncoding physiological signals as images for affective state recognition using convolutional neural\\nnetworks. In 2016 38th Annual International Conference of the IEEE Engineering in Medicine and\\nBiology Society (EMBC), pages 812–815. IEEE, 2016.\\n[50] Jun Wang, Myungjong Kim, Angel W Hernandez-Mulero, Daragh Heitzman, and Paul Ferrari. Towards\\ndecoding speech production from single-trial magnetoencephalography (meg) signals. In 2017 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3036–3040.\\nIEEE, 2017.\\n[51] Alexander E Hramov, Nikita S Frolov, Vladimir A Maksimenko, Vladimir V Makarov, Alexey A\\nKoronovskii, Juan Garcia-Prieto, Luis Fernando Antón-Toro, Fernando Maestú, and Alexander N\\nPisarchik. Artificial neural network detects human uncertainty. Chaos: An Interdisciplinary Journal of\\nNonlinear Science, 28(3):033607, 2018.\\n[52] NS Frolov and AN Pisarchik. Diagnostics of the brain neural-ensemble states using meg records and\\nartificial neural-network concepts. Technical Physics Letters, 44(5):441–444, 2018.\\n[53] Debadatta Dash, Paul Ferrari, Daragh Heitzman, and Jun Wang. Decoding speech from single trial meg\\nsignals using convolutional neural networks and transfer learning. In 2019 41st Annual International\\nConference of the IEEE Engineering in Medicine and Biology Society (EMBC), pages 5531–5535.\\nIEEE, 2019.\\n[54] Jon A Garry, Thomas Trappenberg, Steven Beyea, and Timothy Bardouille. Classification and analysis\\nof minimally-processed data from a large magnetoencephalography dataset using convolutional neural\\nnetworks. bioRxiv, page 846964, 2019.\\n37'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 37}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[55] Yangwoo Kim, Sehyeon Jang, Kyungho Won, and Sung Chan Jun. Canet: A channel attention network\\nto determine informative multi-channel for image classification from brain signals. In 2019 41st Annual\\nInternational Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pages\\n680–683. IEEE, 2019.\\n[56] Demetres Kostas, Elizabeth W Pang, and Frank Rudzicz. Machine learning for meg during speech\\ntasks. Scientific reports, 9(1):1–13, 2019.\\n[57] Debadatta Dash, Alan Wisler, Paul Ferrari, and Jun Wang. Towards a speaker independent speech-bci\\nusing speaker adaptation. In INTERSPEECH, pages 864–868, 2019.\\n[58] Zebin Huang and Tianyou Yu. Cross-subject meg decoding using 3d convolutional neural networks. In\\n2019 WRC Symposium on Advanced Robotics and Automation (WRC SARA), pages 354–359. IEEE,\\n2019.\\n[59] Ismail Alaoui Abdellaoui, Jesus Garcia Fernandez, Caner Sahinli, and Siamak Mehrkanoon. Deep\\nbrain state classification of meg data. arXiv preprint arXiv:2007.00897, 2020.\\n[60] Debadatta Dash, Paul Ferrari, and Jun Wang. Role of brainwaves in neural speech decoding. In 2020\\n28th European Signal Processing Conference (EUSIPCO), pages 1357–1361. IEEE, 2021.\\n[61] Debadatta Dash, Paul Ferrari, Angel W Hernandez-Mulero, Daragh Heitzman, Sara G Austin, and Jun\\nWang. Neural speech decoding for amyotrophic lateral sclerosis. In INTERSPEECH, pages 2782–2786,\\n2020.\\n[62] Jingcong Li, Jiahui Pan, Fei Wang, and Zhuliang Yu. Inter-subject meg decoding for visual information\\nwith hybrid gated recurrent network. Applied Sciences, 11(3):1215, 2021.\\n[63] Yulong Feng, Wei Xiao, Teng Wu, Jianwei Zhang, Jing Xiang, and Hong Guo. A new recognition\\nmethod for the auditory evoked magnetic fields. Computational Intelligence and Neuroscience, 2021,\\n2021.\\n[64] Pei-Chun Chang, Jia-Ren Chang, Po-Yu Chen, Li-Kai Cheng, Jen-Chuen Hsieh, Hsin-Yen Yu, Li-Fen\\nChen, and Yong-Sheng Chen. Decoding neural representations of rhythmic sounds from magnetoen-\\ncephalography. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP), pages 1280–1284. IEEE, 2021.\\n[65] Nina Pilyugina, Akihiko Tsukahara, and Keita Tanaka. Comparing methods of feature extraction of\\nbrain activities for octave illusion classification using machine learning. Sensors, 21(19):6407, 2021.\\n[66] Denis A Engemann, Apolline Mellot, Richard Höchenberger, Hubert Banville, David Sabbagh, Lukas\\nGemein, Tonio Ball, and Alexandre Gramfort. A reusable benchmark of brain-age prediction from\\nm/eeg resting-state signals. Neuroimage, 262:119521, 2022.\\n[67] Ran Shi, Yanyu Zhao, Zhiyuan Cao, Chunyu Liu, Yi Kang, and Jiacai Zhang. Categorizing objects\\nfrom meg signals using eegnet. Cognitive Neurodynamics, pages 1–13, 2021.\\n[68] Zhihao Zhang, Tong Chen, Ye Liu, Chongyang Wang, Ke Zhao, Chang Hong Liu, and Xiaolan Fu.\\nDecoding the temporal representation of facial expression in face-selective regions. NeuroImage,\\n283:120442, 2023.\\n[69] Richard Csaky, Mats WJ Van Es, Oiwi Parker Jones, and Mark Woolrich. Group-level brain decoding\\nwith deep learning. Human Brain Mapping, 44(17):6105–6119, 2023.\\n[70] ÖZER Zeynep, Onursal Cetin, Kutlucan GÖRÜR, and Feyzullah TEMURTA¸S. Brain decoding over\\nthe meg signals using riemannian approach and machine learning. Balkan Journal of Electrical and\\nComputer Engineering, 11(3):207–218, 2023.\\n[71] Yifeng Bu, Deborah L Harrington, Roland R Lee, Qian Shen, Annemarie Angeles-Quinto,\\nZhengwei Ji, Hayden Hansen, Jaqueline Hernandez-Lucas, Jared Baumgartner, Tao Song, et al.\\nMagnetoencephalogram-based brain–computer interface for hand-gesture decoding using deep learn-\\ning. Cerebral Cortex, page bhad173, 2023.\\n38'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 38}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[72] Maria Boyko, Polina Druzhinina, Georgii Kormakov, Aleksandra Beliaeva, and Maxim Sharaev.\\nMegformer: enhancing speech decoding from brain activity through extended semantic representations.\\nIn International Conference on Medical Image Computing and Computer-Assisted Intervention, pages\\n281–290. Springer, 2024.\\n[73] Yiqian Yang, Hyejeong Jo, Yiqun Duan, Qiang Zhang, Jinni Zhou, Won Hee Lee, Renjing Xu, and\\nHui Xiong. Mad: Multi-alignment meg-to-text decoding. arXiv preprint arXiv:2406.01512, 2024.\\n[74] Ivan Zubarev, Mila Nurminen, and Lauri Parkkonen. Robust discrimination of multiple naturalistic\\nsame-hand movements from meg signals with convolutional neural networks. Imaging Neuroscience,\\n2:1–15, 2024.\\n[75] Yiqian Yang, Yiqun Duan, Hyejeong Jo, Qiang Zhang, Renjing Xu, Oiwi Parker Jones, Xuming\\nHu, Chin-teng Lin, and Hui Xiong.\\nNeugpt: Unified multi-modal neural gpt.\\narXiv preprint\\narXiv:2410.20916, 2024.\\n[76] Dulhan Jayalath, Gilad Landau, Brendan Shillingford, Mark Woolrich, and Oiwi Parker Jones.\\nThe brain’s bitter lesson: Scaling speech decoding with self-supervised learning. arXiv preprint\\narXiv:2406.04328, 2024.\\n[77] Michelle Shu and Alona Fyshe. Sparse autoencoders for word decoding from magnetoencephalography.\\nIn Proceedings of the third NIPS Workshop on Machine Learning and Interpretation in NeuroImaging\\n(MLINI). Citeseer, 2013.\\n[78] Lu Meng and Jing Xiang. Brain network analysis and classification based on convolutional neural\\nnetwork. Frontiers in computational neuroscience, 12:95, 2018.\\n[79] Jo Aoe, Ryohei Fukuma, Takufumi Yanagisawa, Tatsuya Harada, Masataka Tanaka, Maki Kobayashi,\\nYou Inoue, Shota Yamamoto, Yuichiro Ohnishi, and Haruhiko Kishima. Automatic diagnosis of\\nneurological diseases using meg signals with a deep neural network. Scientific reports, 9(1):1–9, 2019.\\n[80] Peipei Gu, Ting Wu, Mingyang Zou, Yijie Pan, Jiayang Guo, Jianbing Xiahou, Xueping Peng, Hailong\\nLi, Junxia Ma, and Ling Zhang. Multi-head self-attention model for classification of temporal lobe\\nepilepsy subtypes. Frontiers in Physiology, 11:604764, 2020.\\n[81] Jing Zhang, Simeon M Wong, J Don Richardson, Rakesh Jetly, and Benjamin T Dunkley. Predicting\\nptsd severity using longitudinal magnetoencephalography with a multi-step learning framework.\\nJournal of Neural Engineering, 17(6):066013, 2020.\\n[82] Mengjia Xu, David Lopez Sanz, Pilar Garces, Fernando Maestu, Quanzheng Li, and Dimitrios Pantazis.\\nA graph gaussian embedding method for predicting alzheimer’s disease progression with meg brain\\nnetworks. IEEE Transactions on Biomedical Engineering, 68(5):1579–1588, 2021.\\n[83] Antonio Giovannetti, Gianluca Susi, Paola Casti, Arianna Mencattini, Sandra Pusil, María Eugenia\\nLópez, Corrado Di Natale, and Eugenio Martinelli. Deep-meg: spatiotemporal cnn features and\\nmultiband ensemble classification for predicting the early signs of alzheimer’s disease with magnetoen-\\ncephalography. Neural Computing and Applications, 33(21):14651–14667, 2021.\\n[84] Jie Wu and Xiaoxia Huang. Classification of meg signals in schizophrenia based on eegnet. Interna-\\ntional Core Journal of Engineering, 7(4):450–454, 2021.\\n[85] Ming-Xiong Huang, Charles W Huang, Deborah L Harrington, Ashley Robb-Swan, Annemarie\\nAngeles-Quinto, Sharon Nichols, Jeffrey W Huang, Lu Le, Carl Rimmele, Scott Matthews, et al.\\nResting-state magnetoencephalography source magnitude imaging with deep-learning neural network\\nfor classification of symptomatic combat-related mild traumatic brain injury. Human brain mapping,\\n42(7):1987–2004, 2021.\\n[86] Chun-Chih Huang, Intan Low, Chia-Hsiang Kao, Chuan-Yu Yu, Tung-Ping Su, Jen-Chuen Hsieh,\\nYong-Sheng Chen, and Li-Fen Chen. Meg-based classification and grad-cam visualization for major\\n39'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 39}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\ndepressive and bipolar disorders with semi-cnn. In 2022 44th Annual International Conference of the\\nIEEE Engineering in Medicine & Biology Society (EMBC), pages 1823–1826. IEEE, 2022.\\n[87] Yuya Fujita, Takufumi Yanagisawa, Ryohei Fukuma, Natsuko Ura, Satoru Oshino, and Haruhiko\\nKishima. Abnormal phase–amplitude coupling characterizes the interictal state in epilepsy. Journal of\\nNeural Engineering, 19(2):026056, 2022.\\n[88] Kasturi Barik, Katsumi Watanabe, Joydeep Bhattacharya, and Goutam Saha. Functional connectivity\\nbased machine learning approach for autism detection in young children using meg signals. Journal of\\nNeural Engineering, 20(2):026012, 2023.\\n[89] R Vijay Anand, T Shanmuga Priyan, Madala Guru Brahmam, Balamurugan Balusamy, and Francesco\\nBenedetto. Imnmagn: Integrative multimodal approach for enhanced detection of neurodegenerative\\ndiseases using fusion of multidomain analysis with graph networks. IEEE Access, 2024.\\n[90] J Achterberg, D Akarca, DE Astle, K Baker, et al. Synaptic function and sensory processing in\\nzdhhc9-associated neurodevelopmental disorder: a mechanistic account. 2024.\\n[91] Debadatta Dash, Paul Ferrari, Saleem Malik, Albert Montillo, Joseph A Maldjian, and Jun Wang.\\nDetermining the optimal number of meg trials: A machine learning and speech decoding perspective.\\nIn International Conference on Brain Informatics, pages 163–172. Springer, 2018.\\n[92] Debadatta Dash, Paul Ferrari, Saleem Malik, and Jun Wang. Overt speech retrieval from neuromagnetic\\nsignals using wavelets and artificial neural networks. In 2018 IEEE Global Conference on Signal and\\nInformation Processing (GlobalSIP), pages 489–493. IEEE, 2018.\\n[93] Alexander E Hramov and Alexander N Pisarchik. Kinesthetic and visual modes of imaginary movement:\\nMeg studies for bci development. In 2019 3rd School on Dynamics of Complex Networks and their\\nApplication in Intellectual Robotics (DCNAIR), pages 66–68. IEEE, 2019.\\n[94] Alexander E Hramov, Elena N Pitsik, Parth Chholak, Vladimir A Maksimenko, Nikita S Frolov,\\nSemen A Kurkin, and Alexander N Pisarchik. A meg study of different motor imagery modes in\\nuntrained subjects for bci applications. In ICINCO (1), pages 188–195, 2019.\\n[95] Ivan Zubarev, Rasmus Zetter, Hanna-Leena Halme, and Lauri Parkkonen. Adaptive neural network\\nclassifier for decoding meg signals. NeuroImage, 197:425–434, 2019.\\n[96] Debadatta Dash, Paul Ferrari, Saleem Malik, and Jun Wang. Automatic speech activity recognition\\nfrom meg signals using seq2seq learning. In 2019 9th International IEEE/EMBS Conference on Neural\\nEngineering (NER), pages 340–343. IEEE, 2019.\\n[97] Debadatta Dash, Paul Ferrari, and Jun Wang. Decoding imagined and spoken phrases from non-invasive\\nneural (meg) signals. Frontiers in Neuroscience, 2020.\\n[98] Debadatta Dash, Paul Ferrari, Satwik Dutta, and Jun Wang. Neurovad: Real-time voice activity\\ndetection from non-invasive neuromagnetic signals. Sensors, 20(8):2248, 2020.\\n[99] Hong Gi Yeom, June Sic Kim, and Chun Kee Chung. Lstm improves accuracy of reaching trajectory\\nprediction from magnetoencephalography signals. IEEE Access, 8:20146–20150, 2020.\\n[100] Debadatta Dash, Paul Ferrari, and Jun Wang. Decoding speech evoked jaw motion from non-invasive\\nneuromagnetic oscillations. In 2020 International Joint Conference on Neural Networks (IJCNN),\\npages 1–8. IEEE, 2020.\\n[101] Alessandro Lopopolo and Antal van den Bosch. Part-of-speech classification from magnetoencephalog-\\nraphy data using 1-dimensional convolutional neural network. PsyArXiv, 2020.\\n[102] Anastasia O Ovchinnikova, Anatoly N Vasilyev, Ivan P Zubarev, Bogdan L Kozyrskiy, and Sergei L\\nShishkin. Meg-based detection of voluntary eye fixations used to control a computer. Frontiers in\\nneuroscience, 15:619591, 2021.\\n40'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 40}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[103] Yongdong Fan, Tong Zhou, and Qiong Li. A novel motor task classification scheme in magnetoen-\\ncephalography based on cnn. In Advances in Intelligent Information Hiding and Multimedia Signal\\nProcessing: Proceeding of the IIH-MSP 2021 & FITAT 2021, Kaohsiung, Taiwan, Volume 2, pages\\n21–31. Springer, 2022.\\n[104] Jiayang Guo, Kun Yang, Hongyi Liu, Chunli Yin, Jing Xiang, Hailong Li, Rongrong Ji, and Yue\\nGao. A stacked sparse autoencoder-based detector for automatic identification of neuromagnetic high\\nfrequency oscillations in epilepsy. IEEE transactions on medical imaging, 37(11):2474–2482, 2018.\\n[105] Li Zheng, Pan Liao, Shen Luo, Jingwei Sheng, Pengfei Teng, Guoming Luan, and Jia-Hong Gao.\\nEms-net: A deep learning method for autodetecting epileptic magnetoencephalography spikes. IEEE\\nTransactions on Medical Imaging, 39(6):1833–1844, 2019.\\n[106] Jun Liu, Siqi Sun, Yang Liu, Jiayang Guo, Hailong Li, Yuan Gao, Jintao Sun, and Jing Xiang. A\\nnovel megnet for classification of high-frequency oscillations in magnetoencephalography of epileptic\\npatients. Complexity, 2020, 2020.\\n[107] Chuncheng Zhang, Shuang Qiu, Shengpei Wang, and Huiguang He. Target detection using ternary clas-\\nsification during a rapid serial visual presentation task using magnetoencephalography data. Frontiers\\nin Computational Neuroscience, 15:619508, 2021.\\n[108] Ryoji Hirano, Takuto Emura, Otoichi Nakata, Toshiharu Nakashima, Miyako Asai, Kuriko Kagitani-\\nShimono, Haruhiko Kishima, and Masayuki Hirata. Fully-automated spike detection and dipole\\nanalysis of epileptic meg using deep learning. IEEE Transactions on Medical Imaging, 41(10):2879–\\n2890, 2022.\\n[109] Nipun Bhanot, N Mariyappa, H Anitha, GK Bhargava, J Velmurugan, and Sanjib Sinha. Seizure\\ndetection and epileptogenic zone localisation on heavily skewed meg data using rusboost machine\\nlearning technique. International Journal of Neuroscience, 132(10):963–974, 2022.\\n[110] Xiangyu Zhao, Xueping Peng, Ke Niu, Hailong Li, Lili He, Feng Yang, Ting Wu, Duo Chen, Qiusi\\nZhang, Menglin Ouyang, et al. A multi-head self-attention deep learning approach for detection and rec-\\nommendation of neuromagnetic high frequency oscillations in epilepsy. Frontiers in Neuroinformatics,\\n16:771965, 2022.\\n[111] Jiayang Guo, Naian Xiao, Hailong Li, Lili He, Qiyuan Li, Ting Wu, Xiaonan He, Peizhi Chen,\\nDuo Chen, Jing Xiang, et al. Transformer-based high-frequency oscillation signal detection on\\nmagnetoencephalography from epileptic patients. Frontiers in Molecular Biosciences, 9, 2022.\\n[112] Meijuan Zhang, Jun Liu, Chuang Liu, Ting Wu, and Xueping Peng. An efficient cadnet for classification\\nof high-frequency oscillations in magnetoencephalography. In 2022 4th International Conference on\\nRobotics and Computer Vision (ICRCV), pages 25–30. IEEE, 2022.\\n[113] Li Zheng, Pan Liao, Xiuwen Wu, Miao Cao, Wei Cui, Lingxi Lu, Hui Xu, Linlin Zhu, Bingjiang Lyu,\\nXiongfei Wang, et al. An artificial intelligence–based pipeline for automated detection and localisation\\nof epileptic sources from magnetoencephalography. Journal of Neural Engineering, 20(4):046036,\\n2023.\\n[114] Pauline Mouches, Thibaut Dejean, Julien Jung, Romain Bouet, Carole Lartizien, and Romain Quentin.\\nTime cnn and graph convolution network for epileptic spike detection in meg data. In 2024 IEEE\\nInternational Symposium on Biomedical Imaging (ISBI), pages 1–5. IEEE, 2024.\\n[115] Fangyi Wei, Jiajie Mo, Kai Zhang, Haipeng Shen, Srikantan Nagarajan, and Fei Jiang. Nested deep\\nlearning model towards a foundation model for brain signal data. arXiv preprint arXiv:2410.03191,\\n2024.\\n[116] Zeming He and Gaoyan Zhang. Cednet: A continuous emotion detection network for naturalistic\\nstimuli using meg signals. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP), pages 2001–2005. IEEE, 2024.\\n41'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 41}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[117] Antora Dev, Mostafa M Fouda, and Zubair Md Fadlullah. Data-driven model for improving meg\\nepileptic spike detection. In 2024 IEEE 3rd International Conference on Computing and Machine\\nIntelligence (ICMI), pages 1–5. IEEE, 2024.\\n[118] Ryoji Hirano, Miyako Asai, Nobukazu Nakasato, Akitake Kanno, Takehiro Uda, Naohiro Tsuyuguchi,\\nMasaki Yoshimura, Yoshihito Shigihara, Toyoji Okada, and Masayuki Hirata. Deep learning based\\nautomatic detection and dipole estimation of epileptic discharges in meg: a multi-center study. Scientific\\nReports, 14(1):24574, 2024.\\n[119] Radoslaw Martin Cichy, Aditya Khosla, Dimitrios Pantazis, Antonio Torralba, and Aude Oliva.\\nComparison of deep neural networks to spatio-temporal cortical dynamics of human visual object\\nrecognition reveals hierarchical correspondence. Scientific reports, 6:27755, 2016.\\n[120] Radoslaw Martin Cichy, Aditya Khosla, Dimitrios Pantazis, and Aude Oliva. Dynamics of scene\\nrepresentations in the human brain revealed by magnetoencephalography and deep neural networks.\\nNeuroImage, 153:346–358, 2017.\\n[121] Katja Seeliger, Matthias Fritsche, Umut Güçlü, Sanne Schoenmakers, J-M Schoffelen, SE Bosch,\\nand MAJ Van Gerven. Convolutional neural network-based encoding and decoding of visual object\\nrecognition in space and time. NeuroImage, 180:253–266, 2018.\\n[122] Diana C Dima, Gavin Perry, and Krish D Singh. Spatial frequency supports the emergence of\\ncategorical representations in visual cortex during natural scene perception. NeuroImage, 179:102–116,\\n2018.\\n[123] Brett B Bankson, Martin N Hebart, Iris IA Groen, and Chris I Baker. The temporal evolution of\\nconceptual object representations revealed through models of behavior, semantics and deep neural\\nnetworks. NeuroImage, 178:172–182, 2018.\\n[124] Karim Rajaei, Yalda Mohsenzadeh, Reza Ebrahimpour, and Seyed-Mahdi Khaligh-Razavi. Beyond\\ncore object recognition: Recurrent processes account for object recognition under occlusion. PLoS\\ncomputational biology, 15(5):e1007001, 2019.\\n[125] Tim C Kietzmann, Courtney J Spoerer, Lynn KA Sörensen, Radoslaw M Cichy, Olaf Hauk, and\\nNikolaus Kriegeskorte. Recurrence is required to capture the representational dynamics of the human\\nvisual system. Proceedings of the National Academy of Sciences, 116(43):21854–21863, 2019.\\n[126] Giuliano Giari, Elisa Leonardelli, Yuan Tao, Mayara Machado, and Scott L Fairhall. Spatiotemporal\\nproperties of the neural representation of conceptual content for words and pictures–an meg study.\\nNeuroimage, 219:116913, 2020.\\n[127] Marijn van Vliet, Oona Rinkinen, Takao Shimizu, Anni-Mari Niskanen, Barry Devereux, and Riitta\\nSalmelin. Convolutional networks can model the functional modulation of meg responses during\\nreading. bioRxiv, pages 2022–02, 2022.\\n[128] Jacqueline Von Seth, Victoria I Nicholls, Lorraine K Tyler, and Alex Clarke. Recurrent connectivity\\nsupports higher-level visual and semantic object representations in the brain. Communications Biology,\\n6(1):1207, 2023.\\n[129] Peter W Donhauser and Sylvain Baillet. Two distinct neural timescales for predictive speech processing.\\nNeuron, 105(2):385–393, 2020.\\n[130] Charlotte Caucheteux and Jean-Rémi King. Brains and algorithms partially converge in natural\\nlanguage processing. Communications biology, 5(1):1–10, 2022.\\n[131] Cai Wingfield, Chao Zhang, Barry Devereux, Elisabeth Fonteneau, Andrew Thwaites, Xunying\\nLiu, Phil Woodland, William Marslen-Wilson, and Li Su. On the similarities of representations in\\nartificial and brain neural networks for speech recognition. Frontiers in Computational Neuroscience,\\n16:1057439, 2022.\\n42'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 42}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[132] Théo Desbordes, Yair Lakretz, Valérie Chanoine, Maxime Oquab, Jean-Michel Badier, Agnès Trébu-\\nchon, Romain Carron, Christian-G Bénar, Stanislas Dehaene, and Jean-Rémi King. Dimensionality\\nand ramping: Signatures of sentence integration in the dynamics of brains and deep language models.\\nJournal of Neuroscience, 2023.\\n[133] Christian Brodbeck, Thomas Hannagan, and James S Magnuson. Recurrent neural networks as\\nneuro-computational models of human speech recognition. bioRxiv, pages 2024–02, 2024.\\n[134] Bingjiang Lyu, William D Marslen-Wilson, Yuxing Fang, and Lorraine K Tyler. Finding structure\\nduring incremental speech comprehension. ELife, 12:RP89311, 2024.\\n[135] Christoph Dinh, John GW Samuelsson, Alexander Hunold, Matti S Hämäläinen, and Sheraz Khan.\\nContextual minimum-norm estimates (cmne): A deep learning method for source estimation in neuronal\\nnetworks. arXiv preprint arXiv:1909.02636, 2019.\\n[136] Christoph Dinh, John G Samuelsson, Alexander Hunold, Matti S Hämäläinen, and Sheraz Khan.\\nContextual meg and eeg source estimates using spatiotemporal lstm networks. Frontiers in neuroscience,\\n15:552666, 2021.\\n[137] Rui Sun, Wenbo Zhang, Anto Bagic, and Bin He. Personalized deep learning based source imaging\\nframework improves the imaging of epileptic sources from meg interictal spikes. bioRxiv, pages\\n2022–11, 2022.\\n[138] Rui Sun, Wenbo Zhang, Anto Bagi´c, and Bin He. Deep learning based source imaging provides strong\\nsublobar localization of epileptogenic zone from meg interictal spikes. NeuroImage, 281:120366,\\n2023.\\n[139] Jamie A O’Reilly, Judy D Zhu, and Paul Sowman. Localized estimation of electromagnetic sources\\nunderlying event-related fields using recurrent neural networks. Journal of Neural Engineering, 2023.\\n[140] Meng Jiao, Shihao Yang, Xiaochen Xian, Neel Fotedar, and Feng Liu. Multi-modal electrophysiological\\nsource imaging with attention neural networks based on deep fusion of eeg and meg. IEEE Transactions\\non Neural Systems and Rehabilitation Engineering, 2024.\\n[141] Hikaru Yokoyama, Naotsugu Kaneko, Noboru Usuda, Tatsuya Kato, Hui Ming Khoo, Ryohei Fukuma,\\nSatoru Oshino, Naoki Tani, Haruhiko Kishima, Takufumi Yanagisawa, et al. M/eeg source localization\\nfor both subcortical and cortical sources using a convolutional neural network with a realistic head\\nconductivity model. APL bioengineering, 8(4), 2024.\\n[142] Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning\\nand nonlinear ica. Advances in neural information processing systems, 29, 2016.\\n[143] Prabhat Garg, Elizabeth Davenport, Gowtham Murugesan, Ben Wagner, Christopher Whitlow, Joseph\\nMaldjian, and Albert Montillo. Automatic 1d convolutional neural network-based detection of artifacts\\nin meg acquired without electrooculography or electrocardiography. In 2017 International Workshop\\non Pattern Recognition in Neuroimaging (PRNI), pages 1–4. IEEE, 2017.\\n[144] Prabhat Garg, Elizabeth Davenport, Gowtham Murugesan, Ben Wagner, Christopher Whitlow, Joseph\\nMaldjian, and Albert Montillo. Using convolutional neural networks to automatically detect eye-blink\\nartifacts in magnetoencephalography without resorting to electrooculography. In International Con-\\nference on Medical Image Computing and Computer-Assisted Intervention, pages 374–381. Springer,\\n2017.\\n[145] Pierpaolo Croce, Filippo Zappasodi, Laura Marzetti, Arcangelo Merla, Vittorio Pizzella, and Anto-\\nnio Maria Chiarelli. Deep convolutional neural networks for feature-less automatic classification of\\nindependent components in multi-channel electrophysiological brain recordings. IEEE Transactions\\non Biomedical Engineering, 66(8):2372–2380, 2018.\\n43'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 43}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[146] Ahmad Hasasneh, Nikolas Kampel, Praveen Sripad, N Jon Shah, and Jürgen Dammers. Deep learning\\napproach for automatic classification of ocular and cardiac artifacts in meg data. Journal of Engineering,\\n2018, 2018.\\n[147] Yulong Feng, Wei Xiao, Teng Wu, Jianwei Zhang, Jing Xiang, and Hong Guo.\\nAn automatic\\nidentification method for the blink artifacts in the magnetoencephalography with machine learning.\\nApplied Sciences, 11(5):2415, 2021.\\n[148] Alex H Treacher, Prabhat Garg, Elizabeth Davenport, Ryan Godwin, Amy Proskovec, Leonardo\\nGuimaraes Bezerra, Gowtham Murugesan, Ben Wagner, Christopher T Whitlow, Joel D Stitzel,\\nJoseph A Maldjian, and Albert A Montillo. Megnet: Automatic ica-based artifact removal for meg\\nusing spatiotemporal convolutional neural networks. NeuroImage, 241:118402, 2021.\\n[149] Sammi Hamdan, Kyle DuBray, Jordan Treutel, Rajendra Paudyal, and Khem Poudel. Reducing meg\\ninterference using machine learning. Machine Learning with Applications, 12:100462, 2023.\\n[150] Yiluan Guo, Hossein Nejati, and Ngai-Man Cheung. Deep neural networks on graph signals for\\nbrain imaging analysis. In 2017 IEEE International Conference on Image Processing (ICIP), pages\\n3295–3299. IEEE, 2017.\\n[151] Zachary J Harper and Charles M Welzig. Exploring spatiotemporal functional connectivity dynamics\\nof the human brain using convolutional and recursive neural networks. In 2019 International Joint\\nConference on Neural Networks (IJCNN), pages 1–6. IEEE, 2019.\\n[152] Ismail Alaoui Abdellaoui, Jesús García Fernández, Caner Sahinli, and Siamak Mehrkanoon. Enhancing\\nbrain decoding using attention augmented deep neural networks. In ESANN, 2021.\\n[153] B Lakshmi Priya and S Jayalakshmy. Cnn-time frequency representation based brain wave decoding\\nfrom magnetoencephalography signals. In 2022 IEEE 1st International Conference on Data, Decision\\nand Systems (ICDDS), pages 1–6. IEEE, 2022.\\n[154] Giorgio Gosti, Edoardo Milanetti, Viola Folli, Francesco de Pasquale, Marco Leonetti, Maurizio\\nCorbetta, Giancarlo Ruocco, and Stefania Della Penna. A recurrent hopfield network for estimating\\nmeso-scale effective connectivity in meg. Neural Networks, 170:72–93, 2024.\\n[155] Mohamed Elshafei, Zubair Md Fadlullah, and Mostafa M Fouda. Optimizing meg-eeg mapping\\nin resource-constrained non-intrusive bio-magnetic sensing systems: A data-driven approach. In\\n2023 11th International Conference on Information and Communication Technology (ICoICT), pages\\n423–428. IEEE, 2023.\\n[156] Yongdong Fan, Haokun Mao, and Qiong Li.\\nA model-agnostic feature attribution approach to\\nmagnetoencephalography predictions based on shapley value. IEEE Journal of Biomedical and Health\\nInformatics, 2023.\\n[157] Richard Csaky, Mats WJ van Es, Oiwi Parker Jones, and Mark Woolrich. Interpretable many-class\\ndecoding for meg. NeuroImage, 282:120396, 2023.\\n[158] Yongjie Zhu, Tiina Parviainen, Erkka Heinilä, Lauri Parkkonen, and Aapo Hyvärinen. Unsupervised\\nrepresentation learning of spontaneous meg data with nonlinear ica. NeuroImage, 274:120142, 2023.\\n[159] Adrià Solana, Erik Fransén, and Gonzalo Uribarri. Classification of raw meg/eeg data with detach-\\nrocket ensemble: an improved rocket algorithm for multivariate time series analysis. arXiv preprint\\narXiv:2408.02760, 2024.\\n[160] Yongdong Fan, Qiong Li, Haokun Mao, and Feng Jiang. Magnetoencephalography decoding transfer\\napproach: From deep learning models to intrinsically interpretable models. IEEE Journal of Biomedical\\nand Health Informatics, 2024.\\n[161] Alban Gallard, Benoit Brebion, Katrin Sippel, Amer Zaylaa, Hubert Preissl, Sahar Moghimi, Yael\\nFregier, and Fabrice Wallois. Transforming spontaneous premature neonatal eeg to unpaired sponta-\\nneous fetal meg using a cyclegan learning approach. medRxiv, pages 2024–03, 2024.\\n44'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 44}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[162] Jing-Lun Chou, Yih-Ning Huang, and Chun-Shu Wei. A unified brain signal decoder based on multi-\\nbranch architecture. In 2024 International Joint Conference on Neural Networks (IJCNN), pages 1–8.\\nIEEE, 2024.\\n[163] Richard Csaky, Mats WJ van Es, Oiwi Parker Jones, and Mark Woolrich. Foundational gpt model for\\nmeg. arXiv preprint arXiv:2404.09256, 2024.\\n[164] Yonatan Gideoni, Ryan Charles Timms, and Oiwi Parker Jones. Non-invasive neural decoding in\\nsource reconstructed brain space. arXiv preprint arXiv:2410.19838, 2024.\\n[165] Matteo Ferrante, Tommaso Boccato, Grigorii Rashkov, and Nicola Toschi. Towards neural foundation\\nmodels for vision: Aligning eeg, meg, and fmri representations for decoding, encoding, and modality\\nconversion. arXiv preprint arXiv:2411.09723, 2024.\\n[166] Dimitrios Pantazis and Amir Adler. Meg source localization via deep learning. Sensors, 21(13):4278,\\n2021.\\n[167] Jose Sanchez-Bornot, Roberto C Sotero, JA Scott Kelso, Özgür ¸Sim¸sek, and Damien Coyle. Solving\\nlarge-scale meg/eeg source localisation and functional connectivity problems simultaneously using\\nstate-space models. NeuroImage, 285:120458, 2024.\\n[168] Luigi Carratino, Moustapha Cissé, Rodolphe Jenatton, and Jean-Philippe Vert. On mixup regularization.\\nThe Journal of Machine Learning Research, 23(1):14632–14662, 2022.\\n[169] Abdeldjalil Ouahabi. A review of wavelet denoising in medical imaging. In 2013 8th international\\nworkshop on systems, signal processing and their applications (WoSSPA), pages 19–26. IEEE, 2013.\\n[170] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions (2014). arXiv\\npreprint arXiv:1409.4842, 10, 2014.\\n[171] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recogni-\\ntion. corr abs/1512.03385 (2015), 2015.\\n[172] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural\\nnetworks. In Proceedings of the thirteenth international conference on artificial intelligence and\\nstatistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010.\\n[173] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980, 2014.\\n[174] Yan Pan and Yuanzhi Li. Toward understanding why adam converges faster than sgd for transformers.\\narXiv preprint arXiv:2306.00204, 2023.\\n[175] Aman Gupta, Rohan Ramanath, Jun Shi, and S Sathiya Keerthi. Adam vs. sgd: Closing the generaliza-\\ntion gap on image classification. In OPT2021: 13th Annual Workshop on Optimization for Machine\\nLearning, pages 1–7, 2021.\\n[176] Esraa Hassan, Mahmoud Y Shams, Noha A Hikal, and Samir Elmougy. The effect of choosing\\noptimizer algorithms to improve computer vision tasks: a comparative study. Multimedia Tools and\\nApplications, 82(11):16591–16633, 2023.\\n[177] Richard D Riley, Joie Ensor, Kym IE Snell, Frank E Harrell, Glen P Martin, Johannes B Reitsma,\\nKarel GM Moons, Gary Collins, and Maarten van Smeden. Calculating the sample size required for\\ndeveloping a clinical prediction model. Bmj, 368, 2020.\\n[178] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. arxiv. arXiv\\npreprint arXiv:1311.2901, 2013.\\n[179] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.\\n[180] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:\\nVisualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.\\n45'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 45}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[181] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in\\nneural information processing systems, 30, 2017.\\n[182] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,\\nand Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization.\\nIn Proceedings of the IEEE international conference on computer vision, pages 618–626, 2017.\\n[183] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic\\nminority over-sampling technique. Journal of artificial intelligence research, 16:321–357, 2002.\\n[184] Philipp Thölke, Yorguin-Jose Mantilla-Ramos, Hamza Abdelhedi, Charlotte Maschke, Arthur Dehgan,\\nYann Harel, Anirudha Kemtur, Loubna Mekki Berrada, Myriam Sahraoui, Tammy Young, et al. Class\\nimbalance should not throw you off balance: Choosing the right classifiers and performance metrics\\nfor brain decoding with imbalanced data. NeuroImage, 277:120253, 2023.\\n[185] Nikolaus Kriegeskorte, Marieke Mur, and Peter A Bandettini. Representational similarity analysis-\\nconnecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2:4, 2008.\\n[186] Jörn Diedrichsen and Nikolaus Kriegeskorte. Representational models: A common framework for\\nunderstanding encoding, pattern-component, and representational-similarity analysis. PLoS computa-\\ntional biology, 13(4):e1005508, 2017.\\n[187] Hamed Nili, Cai Wingfield, Alexander Walther, Li Su, William Marslen-Wilson, and Nikolaus\\nKriegeskorte.\\nA toolbox for representational similarity analysis.\\nPLoS computational biology,\\n10(4):e1003553, 2014.\\n[188] Marcel AJ van Gerven. A primer on encoding models in sensory neuroscience. Journal of Mathematical\\nPsychology, 76:172–183, 2017.\\n[189] Mikhail V Koroteev. Bert: a review of applications in natural language processing and understanding.\\narXiv preprint arXiv:2103.11943, 2021.\\n[190] Jonas Kubilius, Martin Schrimpf, Kohitij Kar, Rishi Rajalingham, Ha Hong, Najib Majaj, Elias Issa,\\nPouya Bashivan, Jonathan Prescott-Roy, Kailyn Schmidt, et al. Brain-like object recognition with\\nhigh-performing shallow recurrent anns. Advances in neural information processing systems, 32, 2019.\\n[191] Juliette Millet and Jean-Remi King. Inductive biases, pretraining and fine-tuning jointly account for\\nbrain responses to speech. arXiv preprint arXiv:2103.01032, 2021.\\n[192] Colin Conwell, Jacob S Prince, Kendrick N Kay, George A Alvarez, and Talia Konkle. A large-scale\\nexamination of inductive biases shaping high-level visual representation in brains and machines. Nature\\ncommunications, 15(1):9383, 2024.\\n[193] Nancy Kanwisher, Meenakshi Khosla, and Katharina Dobs. Using artificial neural networks to ask\\n‘why’questions of minds and brains. Trends in Neurosciences, 46(3):240–254, 2023.\\n[194] Katharina Dobs, Julio Martinez, Alexander JE Kell, and Nancy Kanwisher. Brain-like functional\\nspecialization emerges spontaneously in deep neural networks. Science advances, 8(11):eabl8913,\\n2022.\\n[195] Matti S Hämäläinen and Risto J Ilmoniemi. Interpreting magnetic fields of the brain: minimum norm\\nestimates. Medical & biological engineering & computing, 32(1):35–42, 1994.\\n[196] Barry D Van Veen and Kevin M Buckley. Beamforming: A versatile approach to spatial filtering. IEEE\\nassp magazine, 5(2):4–24, 1988.\\n[197] Roberto Domingo Pascual-Marqui et al. Standardized low-resolution brain electromagnetic tomography\\n(sloreta): technical details. Methods Find Exp Clin Pharmacol, 24(Suppl D):5–12, 2002.\\n[198] Lukas Hecker, Rebekka Rupprecht, Ludger Tebartz Van Elst, and Jürgen Kornmeier. Convdip: A\\nconvolutional neural network for better eeg source imaging. Frontiers in Neuroscience, 15:569918,\\n2021.\\n46'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 46}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[199] Josue Ortega Caro, Antonio Henrique Oliveira Fonseca, Christopher Averill, Syed A Rizvi, Matteo\\nRosati, James L Cross, Prateek Mittal, Emanuele Zappala, Daniel Levine, Rahul M Dhodapkar, et al.\\nBrainlm: A foundation model for brain activity recordings. bioRxiv, pages 2023–09, 2023.\\n[200] Cédric Rommel, Joseph Paillard, Thomas Moreau, and Alexandre Gramfort. Data augmentation\\nfor learning predictive models on eeg: a systematic comparison. Journal of Neural Engineering,\\n19(6):066020, 2022.\\n[201] Chao-Ying Joanne Peng, Kuk Lida Lee, and Gary M Ingersoll. An introduction to logistic regression\\nanalysis and reporting. The journal of educational research, 96(1):3–14, 2002.\\n[202] Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal margin\\nclassifiers. In Proceedings of the fifth annual workshop on Computational learning theory, pages\\n144–152, 1992.\\n[203] Etienne Combrisson and Karim Jerbi. Exceeding chance level by chance: The caveat of theoretical\\nchance levels in brain signal classification and statistical assessment of decoding accuracy. Journal of\\nneuroscience methods, 250:126–136, 2015.\\n[204] Ibrahem Kandel and Mauro Castelli. The effect of batch size on the generalizability of the convolutional\\nneural networks on a histopathology dataset. ICT Express, 2020.\\n[205] Guiomar Niso, Krzysztof J Gorgolewski, Elizabeth Bock, Teon L Brooks, Guillaume Flandin, Alexan-\\ndre Gramfort, Richard N Henson, Mainak Jas, Vladimir Litvak, Jeremy T Moreau, et al. Meg-bids, the\\nbrain imaging data structure extended to magnetoencephalography. Scientific data, 5(1):1–5, 2018.\\n[206] Russell A Poldrack, Chris I Baker, Joke Durnez, Krzysztof J Gorgolewski, Paul M Matthews, Marcus R\\nMunafò, Thomas E Nichols, Jean-Baptiste Poline, Edward Vul, and Tal Yarkoni. Scanning the horizon:\\ntowards transparent and reproducible neuroimaging research. Nature Reviews Neuroscience, 18(2):115–\\n126, 2017.\\n[207] Katherine S Button, John PA Ioannidis, Claire Mokrysz, Brian A Nosek, Jonathan Flint, Emma SJ\\nRobinson, and Marcus R Munafò. Power failure: why small sample size undermines the reliability of\\nneuroscience. Nature Reviews Neuroscience, 14(5):365, 2013.\\n[208] Mirko Gabelica, Ružica Bojˇci´c, and Livia Puljak. Many researchers were not compliant with their\\npublished data sharing statement: a mixed-methods study. Journal of Clinical Epidemiology, 150:33–\\n41, 2022.\\n[209] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,\\nand Dhruv Batra. Grad-cam: visual explanations from deep networks via gradient-based localization.\\nInternational journal of computer vision, 128:336–359, 2020.\\n[210] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\\nlearning research, 9(Nov):2579–2605, 2008.\\n[211] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and\\nprojection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.\\n[212] Marin Dujmovi´c, Jeffrey S Bowers, Federico Adolfi, and Gaurav Malhotra. Some pitfalls of measuring\\nrepresentational similarity using representational similarity analysis. bioRxiv, pages 2022–04, 2022.\\n[213] Nikita Fedosov, Daria Medvedeva, Oleg Shevtsov, and Alexei Ossadtchi. Low count of optically\\npumped magnetometers furnishes a reliable real-time access to sensorimotor rhythm. arXiv preprint\\narXiv:2412.18353, 2024.\\n[214] Guangyu Robert Yang and Xiao-Jing Wang. Artificial neural networks for neuroscientists: A primer.\\narXiv preprint arXiv:2006.01001, 2020.\\n[215] Ran Wang and Zhe Sage Chen. Large-scale foundation models and generative ai for bigdata neuro-\\nscience. arXiv preprint arXiv:2310.18377, 2023.\\n47'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-20T01:19:47+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Artificial Neural Networks for Magnetoencephalography_ A review of an emerging field.pdf', 'total_pages': 48, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-20T01:19:47+00:00', 'trapped': '', 'modDate': 'D:20250520011947Z', 'creationDate': 'D:20250520011947Z', 'page': 47}, page_content='Artificial neural networks for magnetoencephalography: a review of an emerging fieldA PREPRINT\\n[216] Azul Garza and Max Mergenthaler-Canseco. Timegpt-1. arXiv preprint arXiv:2310.03589, 2023.\\n[217] Jamie A O’Reilly, Judy D Zhu, and Paul F Sowman. Localized estimation of event-related neural source\\nactivity from simultaneous meg-eeg with a recurrent neural network. Neural Networks, 180:106731,\\n2024.\\n[218] Ivan Zubarev, Gavriela Vranou, and Lauri Parkkonen. Mneflow: Neural networks for eeg/meg decoding\\nand interpretation. SoftwareX, 17:1–5, 2022.\\n[219] Arthur Dehgan, Annalisa Pascarella, Yann Harel, Irina Rish, and Karim Jerbi. Meegnet: An open\\nsource python library for the application of convolutional neural networks to meg. bioRxiv, pages\\n2025–03, 2025.\\n48'), Document(metadata={'producer': 'Acrobat PDFMaker 15 for Word', 'creator': 'Word', 'creationdate': '2023-09-14T11:50:32+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Semantic reconstruction of continuous language from MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Semantic reconstruction of continuous language from MEG signals.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': 'Microsoft Word - ICASSP-BoWang-V2-submit.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-09-14T19:50:41+08:00', 'trapped': '', 'modDate': \"D:20230914195041+08'00'\", 'creationDate': 'D:20230914115032Z', 'page': 0}, page_content='SEMANTIC RECONSTRUCTION OF CONTINUOUS LANGUAGE FROM MEG SIGNALS \\n \\nBo Wang1, Xiran Xu1, Longxiang Zhang1, Boda Xiao2, Xihong Wu1,3, Jing Chen1,3 \\n \\n1Key Laboratory of Machine Perception (Ministry of Education), Speech and Hearing Research Center, \\nSchool of Intelligence Science and Technology, Peking University \\n2Academy for Advanced Interdisciplinary Studies, Peking University \\n3National Biomedical Imaging Center, College of Future Technology, Peking University \\njanechenjing@pku.edu.cn \\n \\nABSTRACT \\n \\nDecoding language from neural signals holds considerable theo-\\nretical and practical importance. Previous research has indicated the \\nfeasibility of decoding text or speech from invasive neural signals. \\nHowever, when using non-invasive neural signals, significant chal-\\nlenges are encountered due to their low quality. In this study, we \\nproposed a data-driven approach for decoding semantic of language \\nfrom Magnetoencephalography (MEG) signals recorded while sub-\\njects were listening to continuous speech. First, a multi-subject de-\\ncoding model was trained using contrastive learning to reconstruct \\ncontinuous word embeddings from MEG data. Subsequently, a \\nbeam search algorithm was adopted to generate text sequences based \\non the reconstructed word embeddings. Given a candidate sentence \\nin the beam, a language model was used to predict the subsequent \\nwords. The word embeddings of the subsequent words were corre-\\nlated with the reconstructed word embedding. These correlations \\nwere then used as a measure of the probability for the next word. \\nThe results showed that the proposed continuous word embedding \\nmodel can effectively leverage both subject-specific and subject-\\nshared information. Additionally, the decoded text exhibited signif-\\nicant similarity to the target text, with an average BERTScore of \\n0.816, a score comparable to that in the previous fMRI study.  \\nIndex Terms— Semantic decoding, MEG, brain-computer in-\\nterface, text generation \\n  \\n1. INTRODUCTION \\n \\nEvery year, a considerable number of people lose their ability to \\nspeak due to cerebral stroke or ALS (Amyotrophic Lateral Sclerosis). \\nOver the past few decades, brain-computer interfaces (BCIs) have \\nmade great progress in language decoding. Previous studies have \\ndemonstrated that acoustic information [1], [2], articulatory move-\\nments [3], [4], and semantic information [5] could be effectively de-\\ncoded from intracranial recordings, offering hope for restoring \\ncommunication to the patients. However, since invasive, these BCIs \\nwere not suitable for the majority of the patients.  \\nDecoding semantic of language from non-invasive recordings, on \\nthe other hand, remains a major challenge. Research utilizing func-\\ntional magnetic resonance imaging (fMRI) has demonstrated the ro-\\nbust decoding of semantic information from the blood-oxygen-level \\ndependent (BOLD) response [6], [7]. Nonetheless, fMRI lacks port-\\nability and is unable to capture rapid changes, making it impractical \\nfor real-time applications in daily life [8]. Instead, Electro-/Magne-\\ntoencephalography (EEG/MEG) measures neural activity at milli-\\nsecond resolution with a safe and potentially wearable setup [9], \\nmaking them more suitable for BCI application.  \\nHowever, decoding semantic from EEG/MEG is extremely chal-\\nlenging due to its low signal-to-noise ratio (SNR) and low spatial \\nresolution. Previous studies typically used a subject-specific or com-\\nmon linear decoder to regress semantic features from EEG/MEG \\ndata evoked by a single word [10]–[14]. Subsequently, the most \\nprobable word from a small closed set was selected as the decoding \\noutput based on the distance between the reconstructed features and \\nthe true features. In work of Hultén et al. [11], MEG data were rec-\\norded when subjects were reading the written text of 118 nouns. \\nSubject-specific ridge linear models were adopted to predict \\nword2vec features from MEG data using leave-two-out pairwise \\ncomparisons (the model was trained on 116 of the words and tested \\non the remaining 2). A maximum mean pair-wise decoding accuracy \\nof 66% (chance accuracy: 50%) was reported with a decoding win-\\ndow of 100 ms. In a more recent work of Ghazaryan et al. [12], the \\nsimilar paradigm was used but with a smaller word set (60 words), \\nand the decoding accuracy significantly higher than chance level \\nwas only shown on 6 of 19 subjects. When averaging MEG data \\nacross subjects to improve signal SNR, the decoding accuracy was \\nup to 78%.  \\nThe limited amount of data and simple linear models impose con-\\nstraints on the effective modeling of the intricate relationship be-\\ntween neural responses and features. Moreover, since previous stud-\\nies showed that neural representation of semantic shared certain \\n \\nFig. 1 Architecture of generating word sequences from MEG with a CWER model and beam search decoding.'), Document(metadata={'producer': 'Acrobat PDFMaker 15 for Word', 'creator': 'Word', 'creationdate': '2023-09-14T11:50:32+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Semantic reconstruction of continuous language from MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Semantic reconstruction of continuous language from MEG signals.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': 'Microsoft Word - ICASSP-BoWang-V2-submit.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-09-14T19:50:41+08:00', 'trapped': '', 'modDate': \"D:20230914195041+08'00'\", 'creationDate': 'D:20230914115032Z', 'page': 1}, page_content='commonalities but also displayed a diversity across individuals [15], \\n[16], the common or subject-specific decoder was unable to capture \\nboth shared and subject-specific neural response patterns to the same \\nstimulus. As a result, the decoding performance was limited, and it \\nwas only possible to select words from a closed set, rather than open \\nvocabulary decoding. \\nTo address these issues, we proposed a novel framework to de-\\ncode semantic from MEG signals which were recorded when the \\nsubjects were listening to speech, i.e., spoken stories. Our frame-\\nwork consists of two parts, as shown in Fig. 1. Firstly, a continuous \\nword embedding reconstruction (CWER) model is trained to recon-\\nstruct continuous word embeddings from MEG. The continuous \\nword embeddings were derived from temporal sequence of spoken \\nwords and transformer-based pre-trained language model (PLM) \\nwhich encodes important linguistic information, including semantic \\nfeatures, syntactic features, and long-distant dependencies [17], [18]. \\nNeuroimaging studies have indicated that these word embeddings \\ncould be effectively mapped to the neural activity measured during \\nnatural speech processing [5], [19], [20]. Subsequently, the similar-\\nity was measured between the reconstructed word embeddings and \\nthe word embeddings of the subsequent words which are predicted \\nby a language model (LM) for a candidate word sequence. The sim-\\nilarity was then used as a measure of the probability of the next word. \\nBy incorporating this probability into a beam search algorithm, we \\ncan generate the most probable sequence of words.  \\nCompared with the previous studies [10]–[14], our framework \\nmodels the relationship between neural responses and semantic fea-\\ntures in a data-driven manner through neural networks. By adding a \\nsubject embedding layer, the reconstruction model captures both the \\nsubject-specific and the common patterns of the neural responses to \\nthe same piece of speech. In terms of the decoding output, instead \\nof selecting a word from a small closed set, our framework could \\ngenerate open vocabulary continuous word sequence. \\n \\n2. METHODS \\n \\n2.1. Continuous word embedding reconstruction model \\n \\nThe CWER model aimed to reconstruct word embeddings from a \\nsequence of high-dimensional MEG signals. These MEG signals \\nwere recorded when healthy subjects were listening passively to sto-\\nries in their native language. The story text was fed into PLM, and \\nthe hidden layers output was utilized as the target of word embed-\\ndings. With the temporal boundaries of words, continuous word em-\\nbeddings can be created by filling word embeddings in their corre-\\nsponding period. \\nConsider 𝑋∈𝑅!×# as a segment of MEG data from subject 𝑠∈\\n[𝑆], where 𝐶 represents the number of MEG sensors, 𝑇 denotes the \\nnumber of time samples, and 𝑆 is the subject number. Let 𝑍∈𝑅$×# \\nbe a segment of continuous word embeddings of the story text that \\nwas corresponding to the MEG segment. Here, 𝐷 represents the di-\\nmensionality of the word embeddings, and both 𝑋 and 𝑍 share the \\nsame time samples. The CWER model takes 𝑋 and 𝑠 as input and \\noutputs the estimated 𝑍-. Previous study revealed that MEG response \\nto spoken words showed dynamic spatial patterns [21]. As 1D con-\\nvolution can effectively capture temporal dependencies while ex-\\ntracting spatial patterns among channels, it was used in the present \\nstudy. An overview diagram of the CWER model is shown in Fig. \\n2. The MEG data 𝑋 is first fed in a linear layer and followed by a \\n1 × 1 convolution layer to transform MEG signals into a higher hid-\\nden space with the dimension of 𝐷%. Then, a subject embedding \\nlayer is added, conditioning on the subject’s one-hot label, to learn \\na linear transformation 𝑀& ∈𝑅$!×$! along the channel dimension \\nfor each subject. This allows us to account for inter-subject variation. \\nSubsequently, a stack of five blocks of three convolutional layers \\nwas applied to extract MEG spatial-temporal features. The convolu-\\ntional layer was the same as that in previous work [22] but with a \\nkernel size of 9 over the time axis to further increase the receptive \\nfield. Finally, the output of convolutional blocks was sequentially \\nfed into a 1 × 1 convolution layer with 2𝐷\\' channels, followed by a \\nGELU activation, and then another 1 × 1 convolution layer with 𝐷 \\nchannels to output the estimated 𝑍-. \\nThe reconstructed embeddings were expected to be similar to the \\ntarget embeddings while being dissimilar to the non-target embed-\\ndings, enabling effective selection of target words from the set dur-\\ning the subsequent beam search decoding. Therefore, a contrastive \\nloss was adopted. More specifically, for a segment of MEG data 𝑋, \\nthe corresponding word embedding 𝑍 is considered as a positive \\nsample (denoted as 𝑍%), and 𝑁−1 non-target word embeddings \\nsegments {𝑍\\', … , 𝑍(} sampled from the training set are the negative \\nsamples. Here, 𝑍) ( 𝑖= 1, 2, … , 𝑁) comprises a series of 𝑇 𝐷-di-\\nmensional vectors, i.e., 𝑍) = <𝑧%\\n), 𝑧\\'\\n), … , 𝑧#\\n) >. The InfoNCE loss is \\nconsidered in this paper: \\n𝐿= −@log\\nexp(sim(𝑧*J, 𝑧*\\n%)/𝜏)\\n∑\\nexp<sim<𝑧*J, 𝑧*\\n)>/𝜏>\\n(\\n)+%\\n#\\n*+%\\n(1) \\nwhere sim(∙,∙) is Pearon correlation between two vectors, and 𝜏 is \\nthe temperature parameter in InfoNCE [23]. Intuitively, this loss is \\nthe log loss of an N-way softmax-based classifier that tries to clas-\\nsify 𝑍- as 𝑍%. \\n \\n2.2. Word sequence generation \\n \\nThe most likely word sequence could be estimated using a beam \\nsearch algorithm [6]. Specifically, the beam contains 𝑘 candidates. \\nFor each candidate, the language model uses the last 8-s generated \\nwords (𝑠,-), . . , 𝑠,-%)  to predict the next word distribution \\n𝑃(𝑠,|𝑠,-), . . , 𝑠,-%) over the decoder vocabulary. Nucleus sampling \\nis used to identify subsequent words that belong to the top 𝑝 percent \\nof the probability mass and have a probability within a factor 𝑟 of \\nthe most likely word. For each continuation 𝑠, , a sequence \\n(𝑠,-.-%, . . , 𝑠,) consisting of the word itself and its preceding 𝐿-1 \\nwords is fed into the PLM model, to get the word embedding for 𝑠,. \\nGiven the neural response 𝑋, the CWER model outputs an estimated \\nembedding 𝑍-. With the known onset time 𝑡/, and offset time 𝑡/00 \\nof the next word, the reconstructed word embedding for the contin-\\nuations is given as:  \\n𝑧̂ =\\n1\\n𝑡/00 −𝑡/,\\n@ 𝑧*J\\n*\"##\\n*+*\"$\\n(2) \\n \\nFig. 2 Architecture of the CWER model.'), Document(metadata={'producer': 'Acrobat PDFMaker 15 for Word', 'creator': 'Word', 'creationdate': '2023-09-14T11:50:32+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Semantic reconstruction of continuous language from MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Semantic reconstruction of continuous language from MEG signals.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': 'Microsoft Word - ICASSP-BoWang-V2-submit.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-09-14T19:50:41+08:00', 'trapped': '', 'modDate': \"D:20230914195041+08'00'\", 'creationDate': 'D:20230914115032Z', 'page': 2}, page_content=\"As a result, the Pearson correlation between 𝑧̂ and embeddings of all \\ncontinuations are measured as the next word probability. The 𝑘 most \\nlikely continuations across all candidates are retained in the beam. \\nAfter iterating through all the words along time, the candidate with \\nthe highest probability was output as the decoded sequence. \\n \\n3. EXPERIMENTS \\n \\n3.1 Dataset \\n \\nThe dataset used is from Donders Institute for Brain [24]. The MEG \\ndata were recorded with a 275-channel axial gradiometer CTF sys-\\ntem at a sampling of 1200 Hz from 3 participants, while they listened \\nto audiobooks of The Adventures of Sherlock Holmes in English. For \\neach participant, the data were recorded in 10 separate sessions, con-\\nsisting of 66 trials in total, with a total duration of approximately 10 \\nhours. \\n \\n3.2. MEG preprocessing and continuous word embedding prep-\\naration \\n \\nThe MEG data underwent average re-referencing and were sub-\\njected to band-pass filtering (1-40 Hz), notch filtering (49-51, 99-\\n101, and 149-151 Hz), and down-sampling to 120 Hz. Subsequently, \\nindependent component analysis (ICA) was applied for each trial to \\nremove eye-blink artifacts. In our pilot study, we observed that MEG \\nexhibited the highest efficiency in predicting word embedding \\nwithin the low-frequency range. The MEG data were further low-\\npass filtered at a cut-off frequency of 4 Hz, downsampled to 40 Hz \\nto reduce computational costs, and standardized along the time axis \\nfor each sensor.  \\nTo get continuous word embedding, each word along with its \\npreceding 𝐿-1 words is fed into GPT [25]. The hidden output of the \\n9th layer was used as word embedding. The GPT was initialized \\nwith the model weight from the model that was used in [6] and fine-\\ntuned on another 4 books (The Case-Book of Sherlock Holmes, The \\nMemoirs of Sherlock Holmes, The Return of Sherlock Holmes and \\nHis Last Bow) from Conan Doyle. The temporal onset and offset of \\neach spoken word were included in the dataset. To create continuous \\nword embedding, a multivariate time series was constructed with \\neach word embedding filling in its corresponding time slot. To \\nmatch the MEG data, the continuous word embedding was at a sam-\\npling rate of 40 Hz and 4 Hz low-pass filtered. \\n \\n3.3 Experiment setup \\n \\nFor each subject, the MEG data from session 4 (8 trials) were used \\nas the testing set, and the data from the remaining 9 sessions (58 \\ntrials) were used as the training set. To compensate the delay be-\\ntween stimulus and its corresponding brain response, the MEG data \\nwere shifted backward by 0.25 s firstly. The MEG data and the \\npaired continuous word embeddings in the training set were further \\nsplit into segments with a duration of 10 seconds and an overlap of \\n80%. The data in the testing set were split into segments with a du-\\nration of 10 seconds without overlapping.  \\nDuring the training of the CWER model, an Adam optimizer with \\na learning rate of 5 × 10-1 was used. The batch size was set to 32, \\nand the negative sample 𝑁 was set to 128. The hidden size for both \\n𝐷% and 𝐷' was set to 256. The dropout rate for residual convolu-\\ntional layers was set to 0.5. The temperature parameter in InfoNCE \\nwas set to 0.025. Training was stopped when no loss reduction was \\nfound for 2 consecutive training epochs in the testing set. For com-\\nparison, a CWER model without subject layer and three subject-spe-\\ncific CWER models for every subject were also trained. \\nBesides, a linear model was used as the baseline. The linear map-\\nping was trained with ridge regression to reconstruct word \\nembedding \\n𝑧*\\n from \\nthe \\ntime \\nseries \\nof \\nMEG \\ndata \\n{𝑥*23!, … , 𝑥* … , 𝑥*23%}. Here, 𝜏% and 𝜏' were set to −40 and 60, \\ncorresponding to 1-s before and 1.5-s after stimuli respectively, as a \\nprevious study suggested that brain signal robustly encoded upcom-\\ning words starting −1-s before and ending 1.5-s after the onset of \\nspoken words [5]. The ridge parameter was determined by cross-\\nvalidation procedure with a grid search (20 grid values were loga-\\nrithmically spaced ranging from 10-4 to 101). More details about \\nthe linear model can be found in [26]. For comparison, a common \\nlinear model and three subject-specific linear models were trained, \\nrespectively. All the models are implemented with Pytorch. \\nDuring the beam search decoding, the fine-tuned GPT in Contin-\\nuous word embedding preparing was served as the language model \\nand the word embedding extraction model. The beam width was set \\nto 200, and the nucleus sampling parameters 𝑝 and 𝑟 were set to 0.9 \\nand 0.1, respectively. The words that occurred at least twice in the \\ntrain set were selected, forming a decoder vocabulary consisting of \\n3,660 unique words. The constraint on the continuation number for \\neach candidate is the same as that in [6]. \\n \\n3.4 Evaluation \\n \\n3.4.1 Segment-level evaluation \\nThe evaluation at the segment level was treated as an assessment of \\na retrieval task. With the trained CWER model, the continuous word \\nembedding was reconstructed for each trial in the testing set. These \\nreconstructed continuous word embeddings were then split into seg-\\nments with a duration of 3, 5, and 10 s without overlapping, resulting \\nin 1,210, 723, and 359 segments for each duration condition, respec-\\ntively. The same segmentation manipulation was also applied to the \\ntrue continuous word embeddings, thereby creating a set of candi-\\ndates. \\nFor each reconstructed segment, the Pearson correlations between \\nthe reconstructed segment and each segment in the candidate set \\nwere calculated, and then the top 10 segments in the candidate set \\nwith the highest correlation were retrieved. Top-10 accuracy was \\ndefined as the percentage of reconstructed segments whose target \\nsegment was in the retrieval set. Meanwhile, rank accuracy was also \\ncalculated as the previous study did [7]. For each reconstructed seg-\\nment, the rank of its target segment among the candidate set was \\nnormalized into rank accuracy using equation (3). \\nrank566 = 1 −\\n< rank > −1\\n< segment number > −1\\n(3) \\n \\n3.4.2 Sequence-level evaluation \\nTo assess the quality of the generated text, BERTScore [27] was \\nused to evaluate semantic similarity between the generated text and \\nthe target text. Window similarity was measured by scoring the pre-\\ndicted and target words with BERTScore within a 20-s window \\naround every second of a trial. Trial similarity was then calculated \\nby averaging BERTScore across all windows in a trial.  \\nTo get the chance level of decoding score, null sequences were \\ngenerated by using only language model. During beam search de-\\ncoding, the next word probability was randomly assigned instead of \\nbeing estimated from MEG data. For each trial in the testing set, 500 \\nnull sequences were generated. The p-value was given by the pro-\\nportion of null distribution scores that were higher than the brain \\ndecoded. The decoding accuracy for trial (window) was defined as \\nthe percentage of trial (window) whose p-value was smaller than \\n0.05. \\n \\n4. RESULT AND DISCUSSION \\n \\n4.1 Continuous word embedding reconstruction\"), Document(metadata={'producer': 'Acrobat PDFMaker 15 for Word', 'creator': 'Word', 'creationdate': '2023-09-14T11:50:32+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Semantic reconstruction of continuous language from MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Semantic reconstruction of continuous language from MEG signals.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': 'Microsoft Word - ICASSP-BoWang-V2-submit.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-09-14T19:50:41+08:00', 'trapped': '', 'modDate': \"D:20230914195041+08'00'\", 'creationDate': 'D:20230914115032Z', 'page': 3}, page_content='The segment-level accuracy for each model as a function of the de-\\ncoding duration is shown in Table 1. The accuracy for the random \\nmodel was calculated by using a random continuous word embed-\\nding and the results were averaged over 500 runs. As expected, the \\naccuracies of the common linear model were higher than that of the \\nrandom model, indicating that the semantic features could be recon-\\nstructed from MEG with a linear decoder. The subject-specific lin-\\near model outperformed the common linear model, suggesting sig-\\nnificant individual differences among the subjects. Our common \\nCWER model (without subject layer) and subject-specific CWER \\nmodel outperformed the common linear model and subject-specific \\nlinear model, respectively. This showed that the utilization of a non-\\nlinear network can enhance decoding accuracy. Moreover, the \\nCWER model outperformed both the common CWER model and \\nthe subject-specific CWER model, which revealed that the CWER \\nmodel can effectively leverage both subject-specific and subject-\\nshared information.  \\nCompared to the previous studies of semantic decoding with \\nMEG or fMRI [7], [10]-[14], [28], the performance of our proposed \\nmodels showed the promising advantage on the segment-level \\nevaluation. The decoding performance improvement can be at-\\ntributed to the fact that large amount of the training data and nonlin-\\near CWER model were used in the current work. \\nTo determine whether words can be distinguished as concrete or \\nabstract from the reconstructed word embeddings, 24 concrete \\nwords and 25 abstract words from the testing set were selected. The \\ntarget and reconstructed word embeddings were decomposed via a \\n2-dimensional tSNE [29]. The result is shown in Fig. 3. As expected, \\nthe concrete words and abstract words can be well distinguished in \\nthe reconstructed word embeddings tSNE space, confirming the \\ngood performance on semantic reconstruction from MEG signals.  \\n \\n4.2 Word sequence generation \\n \\nMetrics scores for sequence-level evaluation of each model are \\nshown in Table 2. Scores for Null were computed by averaging met-\\nrics for 500 null sequences. For the CWER model, the averaged \\nBERTScore was 0.816, that was higher than the score for the null \\nmodel and was also higher than that in the previous fMIR work \\nwhere a BERTScore of 0.810 was reported [6]. The similarity be-\\ntween the generated and target text was significant for all trials, in-\\ndicating effective semantic reconstruction from the brain. Addition-\\nally, the CWER model outperformed the CWER model without a \\nsubject layer. This revealed that improving continuous word embed-\\nding reconstruction performance can help generate text with better \\ncontextual similarity. \\nThe decoding accuracy was 20.8% for windows, which indicated \\nthat 20.8% windows had a significant BERTScore. The value was \\nlower than that in [6], where 72-82% timepoints had a significantly \\nhigher BERTScore than chance level. This is because the semantic \\nof language is spatially distributed in the cerebral cortex [16]. The \\nprevious study utilized fMRI data, which has a significantly higher \\nspatial resolution compared to the MEG data used in this study.  \\nIt should be noted that the actual timing information of words was \\nused during sequence generation. However, in BCI applications, the \\ntiming information of words is typically unknown. In such instances, \\na model can be developed as previous studies to predict the onset of \\nwords from MEG data, as those methods used in the previous stud-\\nies [4], [6]. Simultaneously, given the significant advantage of MEG \\nin temporal resolution compared to fMRI, it is more effective in ex-\\ntracting highly dynamic acoustic features or articulatory movements. \\nThis information would assist in generating text from the brain. It is \\nworthy to be investigated in future work. \\n \\n5. CONCLUSION \\n \\nIn the present work, we proposed a framework to reconstruct seman-\\ntic of language from MEG data. The semantic feature reconstruction \\nperformance was greatly improved by using a neural network that \\nleveraged both subject-specific and subject-shared information. By \\nusing a beam search language model, text sequences were generated \\nfrom the reconstructed features. The generated sequences demon-\\nstrated significant similarity with the target sequences. The result of \\nthis work could support the semantic decoding from non-invasive \\nMEG.  \\n \\n6. ACKNOWLEDGEMENTS \\n \\nThis work was supported by the National Key Research and Devel-\\nopment Program of China (No.2021ZD0201503), a National Natu-\\nral Science Foundation of China (No.12074012), and the High-per-\\nformance Computing Platform of Peking University.  \\n \\n \\nFig. 3 TSNE plot visualizing of abstract and concrete words \\nfor the target and the reconstructed word embedding.  \\nTable 2. Similarity scores between the generated word sequence \\nand the target word sequence, and the decoding accuracy for win-\\ndows and trials. The scores and accuracies were averaged across \\nall windows or trials of all subjects. \\nModel \\nScore \\nAcc (%) \\nWindow \\nTrial \\nNull \\n0.812 \\n– \\nCWER (w.o. s. l.) \\n0.815 \\n18.1 \\n79.8 \\nCWER \\n0.816 \\n20.8 \\n100 \\nTable 1. Decoding accuracy for segment level evaluation of each \\nmodel. (s. s.: subject specific; w.o. s.l.: without subject layer) \\nModel \\nTop-10 acc (%) \\nRank acc (%) \\nDuration (s) \\nDuration (s) \\n3 \\n5 \\n10 \\n3 \\n5 \\n10 \\nRandom \\n0.1 \\n1.3 \\n2.7 \\n50.1 \\n50.1 \\n50.2 \\nLinear (common) 10.7 \\n25.8 \\n64.8 \\n82.5 \\n88.4 \\n95.3 \\nLinear (s. s.) \\n17.2 \\n39.2 \\n83.2 \\n87.0 \\n92.4 \\n97.8 \\nCWER (w.o. s. l.) 24.4 \\n49.7 \\n85.4 \\n85.0 \\n93.0 \\n98.1 \\nCWER (s. s.) \\n26.3 \\n50.2 \\n85.6 \\n90.7 \\n94.5 \\n98.3 \\nCWER  \\n30.0 \\n55.7 \\n89.0 \\n90.3 \\n94.7 \\n98.3'), Document(metadata={'producer': 'Acrobat PDFMaker 15 for Word', 'creator': 'Word', 'creationdate': '2023-09-14T11:50:32+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Semantic reconstruction of continuous language from MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Semantic reconstruction of continuous language from MEG signals.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': 'Microsoft Word - ICASSP-BoWang-V2-submit.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-09-14T19:50:41+08:00', 'trapped': '', 'modDate': \"D:20230914195041+08'00'\", 'creationDate': 'D:20230914115032Z', 'page': 4}, page_content='7. REFERENCES \\n \\n[1] \\nH. Akbari, B. Khalighinejad, J. L. Herrero, A. D. Mehta, and \\nN. Mesgarani, “Towards reconstructing intelligible speech from the \\nhuman auditory cortex,” Sci Rep, vol. 9, no. 1, p. 874, 2019. \\n[2] \\nD. A. Moses, N. Mesgarani, M. K. Leonard, and E. F. Chang, \\n“Neural speech recognition: continuous phoneme decoding using \\nspatiotemporal representations of human cortical activity,” J. Neural \\nEng., vol. 13, no. 5, p. 056004, 2016. \\n[3] \\nF. R. Willett et al., “A high-performance speech neuropros-\\nthesis,” Nature, vol. 620, no. 7976, pp. 1031–1036, 2023. \\n[4] \\nD. A. Moses et al., “Neuroprosthesis for decoding speech in \\na paralyzed person with anarthria,” New England Journal of Medi-\\ncine, vol. 385, no. 3, pp. 217–227, 2021. \\n[5] \\nA. Goldstein et al., “Shared computational principles for lan-\\nguage processing in humans and deep language models,” Nat Neu-\\nrosci, vol. 25, no. 3, pp. 369–380, 2022. \\n[6] \\nJ. Tang, A. LeBel, S. Jain, and A. G. Huth, “Semantic recon-\\nstruction of continuous language from non-invasive brain record-\\nings,” Nat Neurosci, vol. 26, no. 5, pp. 858–866, 2023. \\n[7] \\nF. Pereira et al., “Toward a universal decoder of linguistic \\nmeaning from brain activation,” Nat Commun, vol. 9, no. 1, p. 963, \\n2018. \\n[8] \\nN. K. Logothetis, “What we can do and what we cannot do \\nwith fMRI,” Nature, vol. 453, no. 7197, pp. 869-878, 2008. \\n[9] \\nR. A. Seymour et al., “Using OPMs to measure neural activ-\\nity in standing, mobile participants,” NeuroImage, vol. 244, p. \\n118604, 2021. \\n[10] M. Toneva, O. Stretcu, B. Poczos, L. Wehbe, and T. M. \\nMitchell, “Modeling task effects on meaning representation in the \\nbrain via zero-shot MEG prediction,” in Proceedings of the 34th In-\\nternational Conference on Neural Information Processing Systems \\n(NeurIPS 2020), pp. 5284–5295, 2020. \\n[11] A. Hultén et al., “The neural representation of abstract words \\nmay arise through grounding word meaning in language itself,” Hu-\\nman Brain Mapping, vol. 42, no. 15, pp. 4973-4984, 2021. \\n[12] G. Ghazaryan et al., “Trials and tribulations when attempting \\nto decode semantic representations from MEG responses to written \\ntext,” Language, Cognition and Neuroscience, 2023.  \\n[13] S. Alizadeh, H. Jamalabadi, M. Schönauer, C. Leibold, and S. \\nGais, “Decoding cognitive concepts from neuroimaging data using \\nmultivariate pattern analysis,” NeuroImage, vol. 159, pp. 449–458, \\n2017. \\n[14] A. M. Chan, E. Halgren, K. Marinkovic, and S. S. Cash, “De-\\ncoding word and category-specific spatiotemporal representations \\nfrom MEG and EEG,” NeuroImage, vol. 54, no. 4, pp. 3028-3039, \\n2011. \\n[15] S. L. Frisby, A. D. Halai, C. R. Cox, M. A. L. Ralph, and T. \\nT. Rogers, “Decoding semantic representations in mind and brain,” \\nTrends in Cognitive Sciences, vol. 27, no. 3, pp. 258–281, 2023. \\n[16] A. G. Huth, W. A. de Heer, T. L. Griffiths, F. E. Theunissen, \\nand J. L. Gallant, “Natural speech reveals the semantic maps that tile \\nhuman cerebral cortex,” Nature, vol. 532, no. 7600, pp. 453-458, \\n2016. \\n[17] G. Jawahar, B. Sagot, and D. Seddah, “What does BERT \\nlearn about the structure of language?,” in Proceedings of the 57th \\nAnnual Meeting of the Association for Computational Linguistics \\n(ACL 2019), pp. 3651–3657, 2019. \\n[18] I. Tenney et al., “What do you learn from context? Probing \\nfor sentence structure in contextualized word representations,” In: \\nProceedings of the 7th International Conference on Learning Rep-\\nresentations (ICLR 2019), 2019. \\n[19] C. Caucheteux and J.-R. King, “Brains and algorithms par-\\ntially converge in natural language processing,” Commun Biol, vol. \\n5, no. 1, pp. 1-10, 2022. \\n[20] M. Toneva and L. Wehbe, “Interpreting and improving natu-\\nral-language processing (in machines) with natural language-pro-\\ncessing (in the brain),” in Proceedings of the 33rd International \\nConference on Neural Information Processing Systems (NeurIPS \\n2019), pp. 14954-14964, 2019. \\n[21] F. Pulvermüller, Y. Shtyrov, and R. Ilmoniemi, “Spatiotem-\\nporal dynamics of neural language processing: an MEG study using \\nminimum-norm current estimates,” NeuroImage, vol. 20, no. 2, pp. \\n1020–1025, 2003. \\n[22] A. Défossez, C. Caucheteux, J. Rapin, O. Kabeli, and J.-R. \\nKing, “Decoding speech from non-invasive brain recordings,” arXiv \\npreprint arXiv:2208.12266, 2022. \\n[23] O. Henaff, “Data-efficient image recognition with contrastive \\npredictive coding,” in Proceedings of the 37th International Confer-\\nence on Machine Learning (ICML 2020), pp. 4182–4192, 2020. \\n[24] K. Armeni, U. Güçlü, M. van Gerven, and J.-M. Schoffelen, \\n“A 10-hour within-participant magnetoencephalography narrative \\ndataset to test models of language comprehension,” Sci Data, vol. 9, \\nno. 1, p. 278, 2022. \\n[25] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, \\n“Improving language understanding by generative pre-training,” \\n2018.  \\n[26] M. J. Crosse, G. M. Di Liberto, A. Bednar, and E. C. Lalor, \\n“The multivariate temporal response function (mTRF) toolbox: A \\nMATLAB toolbox for relating neural signals to continuous stimuli,” \\nFront. Hum. Neurosci., vol. 10, art. 604, 2016. \\n[27] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, \\n“BERTScore: Evaluating Text Generation with BERT,” In: Pro-\\nceedings of the 8th International Conference on Learning Represen-\\ntations (ICLR 2020), 2020. \\n[28] J. Wang, V. L. Cherkassky, and M. A. Just, “Predicting the \\nbrain activation pattern associated with the propositional content of \\na sentence: Modeling neural representations of events and states,” \\nHuman Brain Mapping, vol. 38, no. 10, pp. 4865–4881, 2017. \\n[29] L. van der Maaten and G. Hinton, “Visualizing data using t-\\nSNE,” Journal of Machine Learning Research, vol. 9, no. 86, pp. \\n2579–2605, 2008.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 0}, page_content='Brain decoding: toward real-time reconstruction of\\nvisual perception\\nYohann Benchetrit1,∗, Hubert Banville1,∗, Jean-Rémi King1,2\\n1FAIR at Meta, 2Laboratoire des Systèmes Perceptifs, École Normale Supérieure, PSL University\\n∗Equal contribution.\\nIn the past five years, the use of generative and foundational AI systems has greatly improved the\\ndecoding of brain activity. Visual perception, in particular, can now be decoded from functional\\nMagnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however,\\nsuffers from a limited temporal resolution (≈0.5 Hz) and thus fundamentally constrains its real-time\\nusage.\\nHere, we propose an alternative approach based on magnetoencephalography (MEG), a\\nneuroimaging device capable of measuring brain activity with high temporal resolution (≈5,000 Hz).\\nFor this, we develop an MEG decoding model trained with both contrastive and regression objectives\\nand consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG\\nmodule trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly,\\nour MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second,\\nlate brain responses to images are best decoded with DINOv2, a recent foundational image model.\\nThird, image retrievals and generations both suggest that high-level visual features can be decoded\\nfrom MEG signals, although the same approach applied to 7T fMRI also recovers better low-level\\nfeatures. Overall, these results, while preliminary, provide an important step towards the decoding –\\nin real-time – of the visual processes continuously unfolding within the human brain.\\nCorrespondence: {ybenchetrit,hubertjb,jeanremi}@meta.com\\nBlogpost: https://ai.meta.com/blog/brain-ai-image-decoding-meg-magnetoencephalography/\\n1\\nIntroduction\\nAutomating the discovery of brain representations.\\nUnderstanding how the human brain represents the world\\nis arguably one of the most profound scientific challenges. This quest, which originally consisted of searching,\\none by one, for the specific features that trigger each neuron, (e.g. Hubel and Wiesel (1962); O’Keefe and\\nNadel (1979); Kanwisher et al. (1997)), is now being automated by Machine Learning (ML) in two main\\nways. First, as a signal processing tool, ML algorithms are trained to extract informative patterns of brain\\nactivity in a data-driven manner. For example, Kamitani and Tong (2005) trained a support vector machine\\nto classify the orientations of visual gratings from functional Magnetic Resonance Imaging (fMRI). Since\\nthen, deep learning has been increasingly used to discover such brain activity patterns (Roy et al., 2019;\\nThomas et al., 2022; Jayaram and Barachant, 2018; Défossez et al., 2022; Scotti et al., 2023). Second, ML\\nalgorithms are used as functional models of the brain. For example, Yamins et al. (2014) have shown that the\\nembedding of natural images in pretrained deep nets linearly account for the neuronal responses to these\\nimages in the cortex. Since, pretrained deep learning models have been shown to account for a wide variety of\\nstimuli including text, speech, navigation, and motor movement (Banino et al., 2018; Schrimpf et al., 2020;\\nHausmann et al., 2021; Mehrer et al., 2021; Caucheteux et al., 2023).\\nGenerating images from brain activity.\\nThis observed representational alignment between brain activity\\nand deep learning models creates a new opportunity: decoding of visual stimuli need not be restricted to a\\nlimited set of classes, but can now leverage pretrained representations to condition subsequent generative AI\\nmodels. While the resulting image may be partly “hallucinated”, interpreting images can be much simpler\\nthan interpreting latent features. Following a long series of generative approaches (Nishimoto et al., 2011;\\nKamitani and Tong, 2005; VanRullen and Reddy, 2019; Seeliger et al., 2018), diffusion techniques have, in this\\nregard, significantly improved the generation of images from functional Magnetic Resonance Imaging (fMRI).\\n1\\narXiv:2310.19812v3  [eess.IV]  14 Mar 2024'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 1}, page_content='The resulting pipeline typically consists of three main modules: (1) a set of pretrained embeddings obtained\\nfrom the image onto which (2) fMRI activity can be linearly mapped and (3) ultimately used to condition a\\npretrained image-generation model (Ozcelik and VanRullen, 2023; Mai and Zhang, 2023; Zeng et al., 2023;\\nFerrante et al., 2022). These recent fMRI studies primarily differ in the type of pretrained image-generation\\nmodel that they use.\\nThe challenge of real-time decoding.\\nThis generative decoding approach has been mainly applied to fMRI.\\nHowever, the temporal resolution of fMRI is limited by the time scale of blood flow and typically leads to\\none snapshot of brain activity every two seconds – a time scale that challenges its clinical usage, e.g. for\\npatients who require a brain-computer-interface (Willett et al., 2023; Moses et al., 2021; Metzger et al., 2023;\\nDéfossez et al., 2022). On the contrary, magnetoencephalography (MEG) can measure brain activity at a\\nmuch higher temporal resolution (≈5,000 Hz) by recording the fluctuation of magnetic fields elicited by the\\npost-synaptic potentials of pyramidal neurons. This higher temporal resolution comes at a cost, however:\\nthe spatial resolution of MEG is limited to ≈300 sensors, whereas fMRI measures ≈100,000 voxels. In sum,\\nfMRI intrinsically limits our ability to (1) track the dynamics of neuronal activity, (2) decode dynamic stimuli\\n(speech, videos, etc.) and (3) apply these tools to real-time use cases. Conversely, it is unknown whether\\ntemporally-resolved neuroimaging systems like MEG are sufficiently precise to generate natural images in\\nreal-time.\\nOur approach.\\nCombining previous work on speech retrieval from MEG (Défossez et al., 2022) and on\\nimage generation from fMRI (Takagi and Nishimoto, 2023; Ozcelik and VanRullen, 2023), we here develop a\\nthree-module pipeline trained to align MEG activity onto pretrained visual embeddings and generate images\\nfrom a stream of MEG signals (Fig. 1).\\nFigure 1 (A) Approach. Locks indicate pretrained models. (B) Processing schemes. Unlike image generation, retrieval\\nhappens in latent space, but requires the true image in the retrieval set.\\nOur approach provides three main contributions: our MEG decoder (1) yields a 7X increase in performance\\nas compared to linear baselines (Fig. 2), (2) helps reveal when high-level semantic features are processed in\\nthe brain (Fig. 3) and (3) allows the continuous generation of images from temporally-resolved brain signals\\n(Fig. 4). Overall, this approach thus paves the way to better understand the unfolding of the brain responses\\nto visual inputs.\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 2}, page_content='2\\nMethods\\n2.1\\nProblem statement\\nWe aim to decode images from multivariate time series of brain activity recorded with MEG as healthy\\nparticipants watched a sequence of natural images. Let Xi ∈RC×T be the MEG time window collected as an\\nimage Ii was presented to the participant, where C is the number of MEG channels, T is the number of time\\npoints in the MEG window and i ∈[[1, N]], with N the total number of images. Let zi ∈RF be the latent\\nrepresentation of Ii, with F the number of features, obtained by embedding the image using a pretrained\\nimage model (Section 2.4). As described in more detail below, our decoding approach relies on training a\\nbrain module fθ : RC×T →RF to maximally retrieve or predict Ii through zi, given Xi.\\n2.2\\nTraining objectives\\nWe use different training objectives for the different parts of our proposed pipeline. First, in the case of\\nretrieval, we aim to pick the right image Ii (i.e., the one corresponding to Xi) out of a bank of candidate\\nimages. To do so, we train fθ using the CLIP loss (Radford et al., 2021) (i.e., the InfoNCE loss (Oord et al.,\\n2018) applied in both brain-to-image and image-to-brain directions) on batches of size B with exactly one\\npositive example,\\nLCLIP (θ) = −1\\nB\\nB\\nX\\ni=1\\n \\nlog\\nexp(s( ˆzi, zi)/τ)\\nPB\\nj=1 exp(s( ˆzi, zj)/τ)\\n+ log\\nexp(s( ˆzi, zi)/τ)\\nPB\\nk=1 exp(s( ˆzk, zi)/τ)\\n!\\n(1)\\nwhere s is the cosine similarity, zi and ˆzi = fθ(Xi) are the latent representation and the corresponding\\nMEG-based prediction, respectively, and τ is a learned temperature parameter.\\nNext, to go beyond retrieval and instead generate images, we train fθ to directly predict the latent representa-\\ntions z such that we can use them to condition generative image models. This is done using a standard mean\\nsquared error (MSE) loss over the (unnormalized) zi and ˆzi:\\nLMSE(θ) =\\n1\\nNF\\nN\\nX\\ni=1\\n∥zi −ˆzi∥2\\n2\\n(2)\\nFinally, we combine the CLIP and MSE losses using a convex combination with tuned weight to train models\\nthat benefit from both training objectives:\\nLCombined = λLCLIP + (1 −λ)LMSE\\n(3)\\n2.3\\nBrain module\\nWe adapt the dilated residual ConvNet architecture of Défossez et al. (2022), denoted as fθ, to learn the\\nprojection from an MEG window Xi ∈RC×T to a latent image representation zi ∈RF . The original model’s\\noutput ˆYbackbone ∈RF ′×T maintains the temporal dimension of the network through its residual blocks.\\nHowever, here we regress a single latent per input instead of a sequence of T latents like in Défossez et al.\\n(2022). Consequently, we add a temporal aggregation layer to reduce the temporal dimension of ˆYbackbone to\\nobtain ˆyagg ∈RF ′. We experiment with three types of aggregations: global average pooling, a learned affine\\nprojection, and an attention layer. Finally, we add two MLP heads, i.e., one for each term in LCombined, to\\nproject from F ′ to the F dimensions of the target latent. Additional details on the architecture can be found\\nin Appendix A.\\nWe run a hyperparameter search to identify an appropriate configuration of preprocessing, brain module\\narchitecture, optimizer and CLIP loss hyperparameters for the retrieval task (Appendix B). The final\\narchitecture configuration for retrieval is described in Table S1 and contains e.g. 6.4M trainable parameters for\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 3}, page_content='F = 768. The final architecture uses two convolutional blocks and an affine projection to perform temporal\\naggregation (further examined in Appendix K).\\nFor image generation experiments, the output of the MSE head is further postprocessed as in Ozcelik and\\nVanRullen (2023), i.e., we z-score normalize each feature across predictions, and then apply the inverse z-score\\ntransform fitted on the training set (defined by the mean and standard deviation of each feature dimension on\\nthe target embeddings). We select λ in LCombined by sweeping over {0.0, 0.25, 0.5, 0.75} and pick the model\\nwhose top-5 accuracy is the highest on the “large test set” (which is disjoint from the “small test set” used for\\ngeneration experiments; see Section 2.8). When training models to generate CLIP and AutoKL latents, we\\nsimplify the task of the CLIP head by reducing the dimensionality of its target: we use the CLS token for\\nCLIP-Vision (FMSE = 768), the \"mean\" token for CLIP-Text (FMSE = 768), and the channel-average for\\nAutoKL latents (FMSE = 4096), respectively.\\nOf note, when comparing performance on different window configurations e.g. to study the dynamics of visual\\nprocessing in the brain, we train a different model per window configuration. Despite receiving a different\\nwindow of MEG as input, these models use the same latent representations of the corresponding images.\\n2.4\\nImage modules\\nWe study the functional alignment between brain activity and a variety of (output) embeddings obtained from\\ndeep neural networks trained in three different representation learning paradigms, spanning a wide range of\\ndimensionalities: supervised learning (VGG-19), image-text alignment (CLIP), and variational autoencoders.\\nWhen using vision transformers, we further include two additional embeddings of smaller dimensionality: the\\naverage of all output embeddings across tokens (mean), and the output embedding of the class-token (CLS).\\nFor comparison, we also evaluate our approach on human-engineered features obtained without deep learning.\\nThe list of embeddings is provided in Appendix C. For clarity, we focus our experiments on a representative\\nsubset.\\n2.5\\nGeneration module\\nTo fairly compare our work to the results obtained with fMRI results, we follow the approach of Ozcelik and\\nVanRullen (2023) and use a model trained to generate images from pretrained embeddings. Specifically, we\\nuse a latent diffusion model conditioned on three embeddings: CLIP-Vision (257 tokens × 768), CLIP-Text\\n(77 tokens × 768), and a variational autoencoder latent (AutoKL; (4 × 64 × 64). In particular, we use the\\nCLIP-Text embeddings obtained from the THINGS object-category of a stimulus image. Following Ozcelik\\nand VanRullen (2023), we apply diffusion with 50 DDIM steps, a guidance of 7.5, a strength of 0.75 with\\nrespect to the image-to-image pipeline, and a mixing of 0.4.\\n2.6\\nTraining and computational considerations\\nCross-participant models are trained on a set of ≈63,000 examples using the Adam optimizer (Kingma and\\nBa, 2014) with default parameters (β1=0.9, β2=0.999), a learning rate of 3 × 10−4 and a batch size of 128.\\nWe use early stopping on a validation set of ≈15,800 examples randomly sampled from the original training\\nset, with a patience of 10, and evaluate the performance of the model on a held-out test set (see below).\\nModels are trained on a single Volta GPU with 32 GB of memory. We train each model three times using\\nthree different random seeds for the weight initialization of the brain module.\\n2.7\\nEvaluation\\nRetrieval metrics.\\nWe first evaluate decoding performance using retrieval metrics. For a known test set, we\\nare interested in the probability of identifying the correct image given the model predictions. Retrieval metrics\\nhave the advantage of sharing the same scale regardless of the dimensionality of the MEG (like encoding\\nmetrics) or the dimensionality of the image embedding (like regression metrics). We evaluate retrieval using\\neither the relative median rank (which does not depend on the size of the retrieval set), defined as the rank\\nof a prediction divided by the size of the retrieval set, or the top-5 accuracy (which is more common in the\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 4}, page_content='literature). In both cases, we use cosine similarity to evaluate the strength of similarity between feature\\nrepresentations (Radford et al., 2021).\\nGeneration metrics.\\nDecoding performance is often measured qualitatively as well as quantitatively using\\na variety of metrics reflecting the reconstruction fidelity both in terms of perception and semantics. For\\nfair comparison with fMRI generations, we provide the same metrics as Ozcelik and VanRullen (2023),\\ncomputed between seen and generated images: PixCorr (the pixel-wise correlation between the true and\\ngenerated images), SSIM (Structural Similarity Index Metric), and SwAV (the correlation with respect to\\nSwAV-ResNet50 output). On the other hand, AlexNet(2/5), Inception, and CLIP are the respective 2-way\\ncomparison scores of layers 2/5 of AlexNet, the pooled last layer of Inception and the output layer of CLIP.\\nFor the NSD dataset, these metrics are reported for participant 1 only (see Appendix D).\\nTo avoid non-representative cherry-picking, we sort all generations on the test set according to the sum of\\n(minus) SwAV and SSIM. We then split the data into 15 blocks and pick 4 images from the best, middle and\\nworst blocks with respect to the summed metric (Figures S2 and S5).\\nReal-time and average metrics.\\nIt is common in fMRI to decode brain activity from preprocessed values\\nestimated with a General Linear Model. These “beta values” are estimates of brain responses to individual\\nimages, computed across multiple repetitions of such images. To provide a fair assessment of possible MEG\\ndecoding performance, we thus leverage repeated image presentations available in the datasets (see below) by\\naveraging predictions before evaluating metrics and generating images.\\n2.8\\nDataset\\nWe test our approach on the THINGS-MEG dataset (Hebart et al., 2023). Four participants (2 female, 2\\nmale; mean age of 23.25 years), underwent 12 MEG sessions during which they were presented with a set of\\n22,448 unique images selected from the THINGS database (Hebart et al., 2019), covering 1,854 categories.\\nOf those, only a subset of 200 images (each one of a different category) was shown multiple times to the\\nparticipants. The images were displayed for 500 ms each, with a variable fixation period of 1000±200 ms\\nbetween presentations. The THINGS dataset additionally contains 3,659 images that were not shown to the\\nparticipants and that we use to augment the size of our retrieval set and emphasize the robustness of our\\nmethod.\\nMEG preprocessing.\\nWe use a minimal MEG data-preprocessing pipeline as in Défossez et al. (2022). Raw\\ndata from the 272 MEG radial gradiometer channels is downsampled from 1,200 Hz to 120 Hz. The continuous\\nMEG data is then epoched from -500 ms to 1,000 ms relative to stimulus onset and baseline-corrected by\\nsubtracting the mean signal value observed between the start of an epoch and the stimulus onset for each\\nchannel. Finally, we apply a channel-wise robust scaler (Pedregosa et al., 2011) and clip values outside of\\n[−20, 20] to minimize the impact of large outliers.\\nSplits.\\nThe original split of Hebart et al. (2023) consists of 22,248 uniquely presented images, and 200 test\\nimages repeated 12 times each for each participant (i.e., 2,400 trials per participant). The use of this data split\\npresents a challenge, however, as the test set contains only one image per category, and these categories are\\nalso seen in the training set. This means evaluating retrieval performance on this test set does not measure\\nthe capacity of the model to (1) extrapolate to new unseen categories of images and (2) recover a particular\\nimage within a set of multiple images of the same category, but rather only to “categorize” it. Consequently,\\nwe propose two modifications of the original split. First, we remove from the training set any image whose\\ncategory appears in the original test set. This “adapted training set” removes any categorical leakage across\\nthe train/test split and makes it possible to assess the capacity of the model to decode images of unseen\\nimage categories (i.e., a “zero-shot” setting). Second, we propose a new “large test set” that is built using the\\nimages removed from the training set. This new test set effectively allows evaluating retrieval performance of\\nimages within images of the same category1. We report results on both the original (“small”) and the “large”\\n1We leave out images of the original test set from this new large test set, as keeping them would create a discrepancy between\\nthe number of MEG repetitions for training images and test images.\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 5}, page_content='test sets to enable comparisons with the original settings of Hebart et al. (2023). Finally, we also compare our\\nresults to the performance obtained by a similar pipeline but trained on fMRI data using the NSD dataset\\n(Allen et al., 2022) (see Appendix D).\\n3\\nResults\\nML as an effective model of the brain.\\nWhich representations of natural images are likely to maximize\\ndecoding performance? To answer this question, we compare the retrieval performance obtained by linear\\nRidge regression models trained to predict one of 16 different latent visual representations given the flattened\\nMEG response Xi to each image Ii (see Appendix E and black transparent bars in Fig. 2). While all image\\nembeddings lead to above-chance retrieval, supervised and text/image alignment models (e.g. VGG, CLIP)\\nyield the highest retrieval scores.\\nML as an effective tool to learn brain responses.\\nWe then compare these linear baselines to a deep ConvNet\\narchitecture (Défossez et al., 2022) trained on the same dataset to retrieve the matching image given an MEG\\nwindow2. Using a deep model leads to a 7X improvement over the linear baselines (Fig. 2). Multiple types\\nof image embeddings lead to good retrieval performance, with VGG-19 (supervised learning), CLIP-Vision\\n(text/image alignment) and DINOv2 (self-supervised learning) yielding top-5 accuracies of 70.33±2.80%,\\n68.66±2.84%, 68.00±2.86%, respectively (where the standard error of the mean is computed across the\\naveraged image-wise metrics). Similar conclusions, although with lower performance, can be drawn from our\\n“large” test set setting, where decoding cannot rely solely on the image category but also requires discriminating\\nbetween multiple images of the same category. Representative retrieval examples are shown in Appendix G.\\nFigure 2 Image retrieval performance obtained from a trained deep ConvNet. Linear decoder baseline performance\\n(see Table S2) is shown with a black transparent bar for each latent. The original “small” test set (Hebart et al.,\\n2023) comprises 200 distinct images, each belonging to a different category. In contrast, our proposed “large” test set\\ncomprises 12 images from each of those 200 categories, yielding a total of 2,400 images. Chance-level is 2.5% top-5\\naccuracy for the small test set and 0.21% for the large test set. The best latent representations yield accuracies around\\n70% and 13% for the small and large test sets, respectively.\\nTemporally-resolved image retrieval.\\nThe above results are obtained from the full time window (-500 to\\n1,000 ms relative to stimulus onset). To further investigate the feasibility of decoding visual representations as\\nthey unfold in the brain, we repeat this analysis on 100-ms sliding windows with a stride of 25 ms (Fig. 3). For\\nclarity, we focus on a subset of representative image embeddings. As expected, all models yield chance-level\\nperformance before image presentation. For all embeddings, a first clear peak can be observed for windows\\n2We use λ = 1 in LCombined as we are solely concerned with the retrieval part of the pipeline here.\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 6}, page_content='ending around 200-275 ms after image onset. A second peak follows for windows ending around 150-200 ms\\nafter image offset.\\nSupplementary analysis (Fig. S7) further suggests these two peak intervals contain\\ncomplementary information for the retrieval task. Finally, performance quickly goes back to chance-level.\\nInterestingly, the recent self-supervised model DINOv2 yields particularly high retrieval performance after\\nimage offset.\\nFigure 3 Retrieval performance of models trained on 100-ms sliding windows with a stride of 25 ms for different\\nimage representations. The shaded gray area indicates the 500-ms interval during which images were presented to the\\nparticipants and the horizontal dashed line indicates chance-level performance. Accuracy peaks a few hundreds of\\nmilliseconds after both the image onset and offset for all embeddings.\\nRepresentative time-resolved retrieval examples are shown in Appendix G. Overall, the retrieved images tend\\nto come from the correct category, such as “speaker” or “brocoli”, mostly during the first few sub-windows\\n(t ≤1 s). However, these retrieved images do not appear to share obvious low-level features to the images\\nseen by the participants.\\nWhile further analyses of these results remain necessary, it seems that (1) our decoding leverages the brain\\nresponses related to both the onset and the offset of the image and (2) category-level information dominates\\nthese visual representations as early as 250 ms.\\nGenerating images from MEG.\\nWhile framing decoding as a retrieval task yields promising results, it requires\\nthe true image to be in the retrieval set – a well-posed problem which presents limited use-cases in practice.\\nTo address this issue, we trained three distinct brain modules to predict the three embeddings that we use (see\\nSection 2.5) to generate images. Fig. 4 shows example generations from (A) “growing” windows, i.e., where\\nincreasingly larger MEG windows (from [0, 100] to [0, 1,500] ms after onset with 50 ms increments) are used\\nto condition image generation and (B) full-length windows (i.e., -500 to 1,000 ms). Additional full-window\\nrepresentative generation examples are shown in Appendix H. As confirmed by the evaluation metrics of\\nTable 1 (see Table S4 for participant-wise metrics), many generated images preserve the high-level category of\\nthe true image. However, most generations appear to preserve a relatively small amount of low-level features,\\nsuch as the position and color of each object. Lastly, we provide a sliding window analysis of these metrics in\\nAppendix L. These results suggest that early responses to both image onset and offset are primarily associated\\nwith low-level metrics, while high-level features appear more related to brain activity in the 200-500 ms\\ninterval.\\nThe application of a very similar pipeline on an analogous fMRI dataset (Allen et al., 2022; Ozcelik and\\nVanRullen, 2023) – using a simple Ridge regression – shows image reconstructions that share both high-level\\nand low-level features with the true image (Fig. S2). Together, these results suggest that it is not the\\nreconstruction pipeline which fails to reconstruct low-level features, but rather the MEG signals which are\\ncomparatively harder to decode.\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 7}, page_content='Figure 4 Handpicked examples of successful generations. (A) Generations obtained on growing windows starting at\\nimage onset (0 ms) and ending at the specified time. (B) Full-window generations (-500 to 1,000 ms).\\n4\\nDiscussion\\nRelated work.\\nThe present study shares several elements with previous MEG and electroencephalography\\n(EEG) studies designed not to maximize decoding performance but to understand the cascade of visual\\nprocesses in the brain. In particular, previous studies have trained linear models to either (1) classify a small\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 8}, page_content='Table 1 Quantitative evaluation of reconstruction quality from MEG data on THINGS-MEG (compared to fMRI\\ndata on NSD (Allen et al., 2022) using a cross-validated Ridge regression). We report PixCorr, SSIM, AlexNet(2),\\nAlexNet(5), Inception, SwAV and CLIP and their SEM when meaningful. In particular, this shows that fMRI betas as\\nprovided in NSD are significantly easier to decode than MEG signals from THINGS-MEG.\\nLow-level\\nHigh-level\\nDataset\\nPixCorr ↑\\nSSIM ↑\\nAlexNet(2) ↑\\nAlexNet(5) ↑\\nInception ↑\\nCLIP ↑\\nSwAV ↓\\nNSD (fMRI)\\n0.305 ± 0.007\\n0.366 ± 0.005\\n0.962\\n0.977\\n0.910\\n0.917\\n0.410 ± 0.004\\nTHINGS-MEG\\n(averaged across all trials within subject)\\n0.076 ± 0.005\\n0.336 ± 0.007\\n0.736\\n0.826\\n0.671\\n0.767\\n0.584 ± 0.004\\nTHINGS-MEG\\n(averaged across all trials and subjects)\\n0.090 ± 0.009\\n0.341 ± 0.015\\n0.774\\n0.876\\n0.703\\n0.811\\n0.567 ± 0.008\\nTHINGS-MEG\\n(no average)\\n0.058 ± 0.011\\n0.327 ± 0.014\\n0.695\\n0.753\\n0.593\\n0.700\\n0.630 ± 0.007\\nset of images from brain activity (Grootswagers et al., 2019; King and Wyart, 2021), (2) predict brain activity\\nfrom the latent representations of the images (Cichy et al., 2017) or (3) quantify the similarity between\\nthese two modalities with representational similarity analysis (RSA) (Cichy et al., 2017; Bankson et al., 2018;\\nGrootswagers et al., 2019; Gifford et al., 2022). While these studies also make use of image embeddings, their\\nlinear decoders are limited to classifying a small set of object classes, or to distinguishing pairs of images.\\nIn addition, several deep neural networks have been introduced to maximize the classification of speech\\n(Défossez et al., 2022), mental load (Jiao et al., 2018) and images (Palazzo et al., 2020; McCartney et al.,\\n2022; Bagchi and Bathula, 2022) from EEG recordings. In particular, Palazzo et al. (2020) introduced a\\ndeep convolutional neural network to classify natural images from EEG signals. However, the experimental\\nprotocol consisted of presenting all of the images of the same class within a single continuous block, which\\nrisks allowing the decoder to rely on autocorrelated noise, rather than informative brain activity patterns\\n(Li et al., 2020). In any case, these EEG studies focus on the categorization of a relatively small number of\\nimages classes.\\nIn sum, there is, to our knowledge, no MEG decoding study that learns end-to-end to reliably generate an\\nopen set of images.\\nImpact.\\nOur methodological contribution has both fundamental and practical impacts. First, the decoding\\nof perceptual representations could clarify the unfolding of visual processing in the brain. While there is\\nconsiderable work on this issue, neural representations are challenging to interpret because they represent latent,\\nabstract, feature spaces. Generative decoding, on the contrary, can provide concrete and, thus, interpretable\\npredictions. Put simply, generating images at each time step could help neuroscientists understand whether\\nspecific – potentially unanticipated – textures or object parts are represented. For example, Cheng et al.\\n(2023) showed that generative decoding applied to fMRI can be used to decode the subjective perception\\nof visual illusions. Such techniques can thus help to clarify the neural bases of subjective perception and to\\ndissociate them from those responsible for “copying” sensory inputs. Our work shows that this endeavor could\\nnow be applied to clarify when these subjective representations arise. Second, generative brain decoding has\\nconcrete applications. For example, it has been used in conjunction with encoding, to identify stimuli that\\nmaximize brain activity (Bashivan et al., 2019). Furthermore, non-invasive brain-computer interfaces (BCI)\\nhave been long-awaited by patients with communication challenges related to brain lesions. BCI, however,\\nrequires real-time decoding, and thus limits the use of neuroimaging modalities with low temporal resolution\\nsuch as fMRI. This application direction, however, will likely require extending our work to EEG, which\\nprovides similar temporal resolution to MEG, but is typically much more common in clinical settings.\\nLimitations.\\nOur analyses highlight three main limitations to the decoding of images from MEG signals.\\nFirst, generating images from MEG appears worse at preserving low-level features than a similar pipeline on\\n7T fMRI (Fig. S2). This result resonates with the fact that the spatial resolution of MEG (≈cm) is much\\nlower than 7T fMRI’s (≈mm). Moreover, and consistent with previous findings (Cichy et al., 2014; Hebart\\net al., 2023), the low-level features can be predominantly extracted from the brief time windows immediately\\nsurrounding the onset and offset of brain responses. As a result, these transient low-level features might have\\na lesser impact on image generation compared to the more persistent high-level features. Second, the present\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 9}, page_content='approach directly depends on the pretraining of several models, and only learns end-to-end to align the MEG\\nsignals to these pretrained embeddings. Our results show that this approach leads to better performance\\nthan classical computer vision features such as color histograms, Fast Fourier transform and histogram of\\noriented gradients (HOG). This is consistent with a recent MEG study by Défossez et al. (2022) which showed,\\nin the context of speech decoding, that pretrained embeddings outperformed a fully end-to-end approach.\\nNevertheless, it remains to be tested whether (1) fine-tuning the image and generation modules and (2)\\ncombining the different types of visual features could improve decoding performance.\\nEthical implications.\\nWhile the decoding of brain activity promises to help a variety of brain-lesioned patients\\n(Metzger et al., 2023; Moses et al., 2021; Défossez et al., 2022; Liu et al., 2023; Willett et al., 2023), the rapid\\nadvances of this technology raise several ethical considerations, and most notably, the necessity to preserve\\nmental privacy. Several empirical findings are relevant to this issue. Firstly, the decoding performance obtained\\nwith non-invasive recordings is only high for perceptual tasks. By contrast, decoding accuracy considerably\\ndiminishes when individuals are tasked to imagine representations (Horikawa and Kamitani, 2017; Tang et al.,\\n2023). Second, decoding performance seems to be severely compromised when participants are engaged in\\ndisruptive tasks, such as counting backward (Tang et al., 2023). In other words, the subjects’ consent is not\\nonly a legal but also and primarily a technical requirement for brain decoding. To delve into these issues\\neffectively, we endorse the open and peer-reviewed research standards.\\nConclusion.\\nOverall, these results provide an important step towards the decoding of the visual processes\\ncontinuously unfolding in the human brain.\\nAcknowledgments\\nThis work was funded in part by FrontCog grant ANR-17-EURE-0017 to JRK for his work at PSL.\\nReferences\\nEmily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad\\nCaron, Franco Pestilli, Ian Charest, et al. A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial\\nintelligence. Nature neuroscience, 25(1):116–126, 2022.\\nSubhranil Bagchi and Deepti R Bathula. EEG-ConvTransformer for single-trial EEG-based visual stimulus classification.\\nPattern Recognition, 129:108757, 2022.\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and\\ntranslate. arXiv preprint arXiv:1409.0473, 2014.\\nAndrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel,\\nMartin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-based navigation using grid-like representations\\nin artificial agents. Nature, 557(7705):429–433, 2018.\\nB.B. Bankson, M.N. Hebart, I.I.A. Groen, and C.I. Baker. The temporal evolution of conceptual object representations\\nrevealed through models of behavior, semantics and deep neural networks. NeuroImage, 178:172–182, 2018. ISSN\\n1053-8119. doi: https://doi.org/10.1016/j.neuroimage.2018.05.037. https://www.sciencedirect.com/science/article/\\npii/S1053811918304440.\\nPouya Bashivan, Kohitij Kar, and James J DiCarlo. Neural population control via deep image synthesis. Science, 364\\n(6439):eaav9436, 2019.\\nG. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.\\nThomas Carlson, David A Tovar, Arjen Alink, and Nikolaus Kriegeskorte. Representational dynamics of object vision:\\nthe first 1000 ms. Journal of vision, 13(10):1–1, 2013.\\nThomas A Carlson, Hinze Hogendoorn, Ryota Kanai, Juraj Mesik, and Jeremy Turret. High temporal resolution\\ndecoding of object position and category. Journal of vision, 11(10):9–9, 2011.\\nCharlotte Caucheteux, Alexandre Gramfort, and Jean-Rémi King. Evidence of a predictive coding hierarchy in the\\nhuman brain listening to speech. Nature human behaviour, 7(3):430–441, 2023.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 10}, page_content='Fan Cheng, Tomoyasu Horikawa, Kei Majima, Misato Tanaka, Mohamed Abdelhack, Shuntaro C Aoki, Jin Hirano, and\\nYukiyasu Kamitani. Reconstructing visual illusory experiences from human brain activity. bioRxiv, pages 2023–06,\\n2023.\\nRadoslaw Martin Cichy, Dimitrios Pantazis, and Aude Oliva. Resolving human object recognition in space and time.\\nNature neuroscience, 17(3):455–462, 2014.\\nRadoslaw Martin Cichy, Aditya Khosla, Dimitrios Pantazis, and Aude Oliva. Dynamics of scene representations in the\\nhuman brain revealed by magnetoencephalography and deep neural networks. NeuroImage, 153:346–358, 2017.\\nAlexandre Défossez, Charlotte Caucheteux, Jérémy Rapin, Ori Kabeli, and Jean-Rémi King. Decoding speech from\\nnon-invasive brain recordings. arXiv preprint arXiv:2208.12266, 2022.\\nMatteo Ferrante, Tommaso Boccato, and Nicola Toschi. Semantic brain decoding: from fMRI to conceptually similar\\nimage reconstruction of visual stimuli. arXiv preprint arXiv:2212.06726, 2022.\\nAlessandro T Gifford, Kshitij Dwivedi, Gemma Roig, and Radoslaw M Cichy. A large and rich EEG dataset for\\nmodeling human visual object recognition. NeuroImage, 264:119754, 2022.\\nTijl Grootswagers, Amanda K Robinson, and Thomas A Carlson. The representational dynamics of visual objects in\\nrapid serial visual processing streams. NeuroImage, 188:668–679, 2019.\\nSébastien B Hausmann, Alessandro Marin Vargas, Alexander Mathis, and Mackenzie W Mathis. Measuring and\\nmodeling the motor system with machine learning. Current opinion in neurobiology, 70:11–23, 2021.\\nMartin N Hebart, Adam H Dickter, Alexis Kidder, Wan Y Kwok, Anna Corriveau, Caitlin Van Wicklin, and Chris I\\nBaker. THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images. PloS one,\\n14(10):e0223792, 2019.\\nMartin N Hebart, Oliver Contier, Lina Teichmann, Adam H Rockter, Charles Y Zheng, Alexis Kidder, Anna Corriveau,\\nMaryam Vaziri-Pashkam, and Chris I Baker. THINGS-data, a multimodal collection of large-scale datasets for\\ninvestigating object representations in human brain and behavior. eLife, 12:e82580, feb 2023. ISSN 2050-084X. doi:\\n10.7554/eLife.82580. https://doi.org/10.7554/eLife.82580.\\nTomoyasu Horikawa and Yukiyasu Kamitani. Generic decoding of seen and imagined objects using hierarchical visual\\nfeatures. Nature communications, 8(1):15037, 2017.\\nDavid H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat’s\\nvisual cortex. The Journal of physiology, 160(1):106, 1962.\\nVinay Jayaram and Alexandre Barachant. MOABB: trustworthy algorithm benchmarking for bcis. Journal of neural\\nengineering, 15(6):066011, 2018.\\nZhicheng Jiao, Xinbo Gao, Ying Wang, Jie Li, and Haojun Xu. Deep convolutional neural networks for mental load\\nclassification based on EEG data. Pattern Recognition, 76:582–595, 2018.\\nYukiyasu Kamitani and Frank Tong. Decoding the visual and subjective contents of the human brain. Nature\\nneuroscience, 8(5):679–685, 2005.\\nNancy Kanwisher, Josh McDermott, and Marvin M Chun. The fusiform face area: a module in human extrastriate\\ncortex specialized for face perception. Journal of neuroscience, 17(11):4302–4311, 1997.\\nJean-Rémi King and Valentin Wyart. The human brain encodes a chronicle of visual events at each instant of time\\nthrough the multiplexing of traveling waves. Journal of Neuroscience, 41(34):7224–7233, 2021.\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\\n2014.\\nRen Li, Jared S Johansen, Hamad Ahmed, Thomas V Ilyevsky, Ronnie B Wilbur, Hari M Bharadwaj, and Jeffrey Mark\\nSiskind. The perils and pitfalls of block design for EEG classification experiments. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, 43(1):316–333, 2020.\\nYan Liu, Zehao Zhao, Minpeng Xu, Haiqing Yu, Yanming Zhu, Jie Zhang, Linghao Bu, Xiaoluo Zhang, Junfeng Lu,\\nYuanning Li, et al. Decoding and synthesizing tonal language speech from brain activity. Science Advances, 9(23):\\neadh0478, 2023.\\nWeijian Mai and Zhijun Zhang. Unibrain: Unify image reconstruction and captioning all in one diffusion model from\\nhuman brain activity. arXiv preprint arXiv:2308.07428, 2023.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 11}, page_content='Ben McCartney, Barry Devereux, and Jesus Martinez-del Rincon. A zero-shot deep metric learning approach to\\nbrain–computer interfaces for image retrieval. Knowledge-Based Systems, 246:108556, 2022.\\nJohannes Mehrer, Courtney J Spoerer, Emer C Jones, Nikolaus Kriegeskorte, and Tim C Kietzmann. An ecologically\\nmotivated image dataset for deep learning yields better models of human vision. Proceedings of the National Academy\\nof Sciences, 118(8):e2011417118, 2021.\\nSean L Metzger, Kaylo T Littlejohn, Alexander B Silva, David A Moses, Margaret P Seaton, Ran Wang, Maximilian E\\nDougherty, Jessie R Liu, Peter Wu, Michael A Berger, et al. A high-performance neuroprosthesis for speech decoding\\nand avatar control. Nature, pages 1–10, 2023.\\nDavid A Moses, Sean L Metzger, Jessie R Liu, Gopala K Anumanchipalli, Joseph G Makin, Pengfei F Sun, Josh\\nChartier, Maximilian E Dougherty, Patricia M Liu, Gary M Abrams, et al. Neuroprosthesis for decoding speech in a\\nparalyzed person with anarthria. New England Journal of Medicine, 385(3):217–227, 2021.\\nShinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, and Jack L Gallant. Reconstructing visual\\nexperiences from brain activity evoked by natural movies. Current biology, 21(19):1641–1646, 2011.\\nJohn O’Keefe and Lynn Nadel. The hippocampus as a cognitive map. Behavioral and Brain Sciences, 2(4):487–494,\\n1979.\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv\\npreprint arXiv:1807.03748, 2018.\\nFurkan Ozcelik and Rufin VanRullen. Natural scene reconstruction from fmri signals using generative latent diffusion.\\nScientific Reports, 13(1):15666, 2023.\\nSimone Palazzo, Concetto Spampinato, Isaak Kavasidis, Daniela Giordano, Joseph Schmidt, and Mubarak Shah.\\nDecoding brain representations by multimodal learning of neural activity and visual features. IEEE Transactions on\\nPattern Analysis and Machine Intelligence, 43(11):3833–3849, 2020.\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\\nV. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:\\nMachine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\\nAskell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models\\nfrom natural language supervision, 2021.\\nYannick Roy, Hubert Banville, Isabela Albuquerque, Alexandre Gramfort, Tiago H Falk, and Jocelyn Faubert. Deep\\nlearning-based electroencephalography analysis: a systematic review. Journal of neural engineering, 16(5):051001,\\n2019.\\nMartin Schrimpf, Idan Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua Tenenbaum,\\nand Evelina Fedorenko. Artificial neural networks accurately predict language processing in the brain. BioRxiv,\\npages 2020–06, 2020.\\nPaul S Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan J Dempster,\\nNathalie Verlinde, Elad Yundler, David Weisberg, et al. Reconstructing the mind’s eye: fMRI-to-image with\\ncontrastive learning and diffusion priors. arXiv preprint arXiv:2305.18274, 2023.\\nKatja Seeliger, Umut Güçlü, Luca Ambrogioni, Yagmur Güçlütürk, and Marcel AJ van Gerven. Generative adversarial\\nnetworks for reconstructing natural images from brain activity. NeuroImage, 181:775–785, 2018.\\nYu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from human brain\\nactivity. bioRxiv, 2023. doi: 10.1101/2022.11.18.517004. https://www.biorxiv.org/content/early/2023/03/11/2022.\\n11.18.517004.\\nJerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth. Semantic reconstruction of continuous language\\nfrom non-invasive brain recordings. Nature Neuroscience, pages 1–9, 2023.\\nArmin Thomas, Christopher Ré, and Russell Poldrack.\\nSelf-supervised learning of brain dynamics from broad\\nneuroimaging data. Advances in Neural Information Processing Systems, 35:21255–21269, 2022.\\nStefan Van der Walt, Johannes L Schönberger, Juan Nunez-Iglesias, François Boulogne, Joshua D Warner, Neil Yager,\\nEmmanuelle Gouillart, and Tony Yu. scikit-image: image processing in python. PeerJ, 2:e453, 2014.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 12}, page_content='Rufin VanRullen and Leila Reddy. Reconstructing faces from fMRI patterns using deep generative neural networks.\\nCommunications biology, 2(1):193, 2019.\\nFrancis R Willett, Erin M Kunz, Chaofei Fan, Donald T Avansino, Guy H Wilson, Eun Young Choi, Foram Kamdar,\\nMatthew F Glasser, Leigh R Hochberg, Shaul Druckmann, et al. A high-performance speech neuroprosthesis. Nature,\\npages 1–6, 2023.\\nDaniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J DiCarlo. Performance-\\noptimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the national academy\\nof sciences, 111(23):8619–8624, 2014.\\nBohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang Liu, and Baochang\\nZhang. Controllable mind visual diffusion model. arXiv preprint arXiv:2305.10135, 2023.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 13}, page_content='Appendix\\nA\\nAdditional details on the brain module architecture\\nWe provide additional details on the brain module fθ described in Section 2.3.\\nThe brain module first applies two successive linear transformations in the spatial dimension to an input MEG\\nwindow. The first linear transformation is the output of an attention layer conditioned on the MEG sensor\\npositions. The second linear transformation is learned subject-wise, such that each subject ends up with\\ntheir own linear projection matrix W subj\\ns\\n∈RC×C, with C the number of input MEG channels and s ∈[[1, S]]\\nwhere S is the number of subjects. The module then applies a succession of 1D convolutional blocks that\\noperate in the temporal dimension and treat the spatial dimension as features. These blocks each contain\\nthree convolutional layers (dilated kernel size of 3, stride of 1) with residual skip connections. The first two\\nlayers of each block use GELU activations while the last one use a GLU activation. The output of the last\\nconvolutional block is passed through a learned linear projection to yield a different number of features F ′\\n(fixed to 2048 in our experiments).\\nThe resulting features are then fed to a temporal aggregation layer which reduces the remaining temporal\\ndimension. Given the output of the brain module backbone ˆYbackbone ∈RF ′×T , we compare three approaches\\nto reduce the temporal dimension of size T: (1) Global average pooling, i.e., the features are averaged across\\ntime steps; (2) Learned affine projection in which the temporal dimension is projected from RT to R using a\\nlearned weight vector wagg ∈RT and bias bagg ∈R; (3) Bahdanau attention layer (Bahdanau et al., 2014)\\nwhich predicts an affine projection from RT to R conditioned on the input ˆYbackbone itself. Following the\\nhyperparameter search of Appendix B, we selected the learned affine projection approach for our experiments.\\nFinally, the resulting output is fed to CLIP and MSE head-specific MLP projection heads where a head\\nconsists of repeated LayerNorm-GELU-Linear blocks, to project from F ′ to the F dimensions of the target\\nlatent.\\nWe refer the interested reader to Défossez et al. (2022) for a description of the original architecture, and to\\nthe code available at https://github.com/facebookresearch/brainmagick.\\nB\\nHyperparameter search\\nWe run a hyperparameter grid search to find an appropriate configuration (MEG preprocessing, optimizer,\\nbrain module architecture and CLIP loss) for the MEG-to-image retrieval task. We randomly split the 79,392\\n(MEG, image) pairs of the adapted training set (Section 2.8) into 60%-20%-20% train, valid and test splits\\nsuch that all presentations of a given image are contained in the same split. We use the validation split to\\nperform early stopping and the test split to evaluate the performance of a configuration.\\nFor the purpose of this search we pick CLIP-Vision (CLS) latent as a representative latent, since it achieved\\ngood retrieval performance in preliminary experiments. We focus the search on the retrieval task, i.e., by\\nsetting λ = 1 in Eq. 3, and leave the selection of an optimal λ to a model-specific sweep using a held-out\\nset (see Section 2.3). We run the search six times using two different random seed initializations for the\\nbrain module and three different random train/valid/test splits. Fig. S1 summarizes the results of this\\nhyperparameter search.\\nBased on this search, we use the following configuration: MEG window (tmin, tmax) of [−0.5, 1.0] s, learning\\nrate of 3 × 10−4, batch size of 128, brain module with two convolutional blocks and both the spatial attention\\nand subject layers of Défossez et al. (2022), affine projection temporal aggregation layer with a single block in\\nthe CLIP projection head, and adapted CLIP loss from Défossez et al. (2022) i.e., with normalization along\\nthe image axis only, the brain-to-image term only (first term of Eq. 1) and a fixed temperature parameter\\nτ = 1. The final architecture configuration is presented in Table S1.\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 14}, page_content='Figure S1 Hyperparameter search results for the MEG-to-image retrieval task, presenting the impact of (A) optimizer\\nlearning rate and batch size, (B) number of convolutional blocks and use of spatial attention and/or subject-specific\\nlayers in the brain module, (C) MEG window parameters, (D) type of temporal aggregation layer and number of blocks\\nin the CLIP projection head of the brain module, and (E) CLIP loss configuration (normalization axes, use of learned\\ntemperature parameter and use of symmetric terms). Chance-level performance top-5 accuracy is 0.05%.\\nC\\nImage embeddings\\nWe evaluate the performance of linear baselines and of a deep convolutional neural network on the MEG-\\nto-image retrieval task using a set of classic visual embeddings. We grouped these embeddings by their\\ncorresponding paradigm:\\nSupervised learning.\\nThe last layer, with dimension 1000, of VGG-19.\\nText/Image alignment.\\nThe last hidden layer of CLIP-Vision (257x768), CLIP-Text (77x768), and their CLS\\nand MEAN pooling.\\nSelf-supervised learning.\\nThe output layers of DINOv1, DINOv2 and their CLS and MEAN pooling. The\\nbest-performing DINOv2 variation reported in tables and figures is ViT-g/14.\\nVariational autoencoders.\\nThe activations of the 31 first layers of the very deep variational-autoencoder\\n(VDVAE), and the bottleneck layer (4x64x64) of the Kullback-Leibler variational-autoencoder (AutoKL) used\\n15'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 15}, page_content='Table S1 Brain module configuration adapted from Défossez et al. (2022) for use with a target latent of size 768 (e.g.\\nCLIP-Vision (CLS), see Section 2.4) in retrieval settings.\\nLayer\\nInput shape\\nOutput shape\\n# parameters\\nSpatial attention block\\n(272, 181)\\n(270, 181)\\n552,960\\nLinear projection\\n(270, 181)\\n(270, 181)\\n73,170\\nSubject-specific linear layer\\n(270, 181)\\n(270, 181)\\n291,600\\nResidual dilated conv block 1\\n(270, 181)\\n(320, 181)\\n1,183,360\\nResidual dilated conv block 2\\n(320, 181)\\n(320, 181)\\n1,231,360\\nLinear projection\\n(320, 181)\\n(2048, 181)\\n1,518,208\\nTemporal aggregation\\n(2048, 181)\\n(2048, 1)\\n182\\nMLP projector\\n(2048, 1)\\n(768, 1)\\n1,573,632\\nTotal\\n6,424,472\\nin the generative module (Section 2.5).\\nEngineered features.\\nThe color histogram of the seen image (8 bins per channels); the local binary patterns\\n(LBP) using the implementation in OpenCV 2 (Bradski, 2000) with ’uniform’ method, P = 8 and R = 1; the\\nHistogram of Oriented Gradients (HOG) using the implementation of sk-image (Van der Walt et al., 2014)\\nwith 8 orientations, 8 pixels-per-cell and 2 cells-per-block.\\nD\\n7T fMRI dataset\\nThe Natural Scenes Dataset (NSD) (Allen et al., 2022) contains fMRI data from 8 participants viewing a total\\nof 73,000 RGB images. It has been successfully used for reconstructing seen images from fMRI in several\\nstudies (Takagi and Nishimoto, 2023; Ozcelik and VanRullen, 2023; Scotti et al., 2023). In particular, these\\nstudies use a highly preprocessed, compact version of fMRI data (“betas”) obtained through generalized linear\\nmodels fitted across multiple repetitions of the same image.\\nEach participant saw a total of 10,000 unique images (repeated 3 times each) across 37 sessions. Each session\\nconsisted in 12 runs of 5 minutes each, where each image was seen during 3 s, with a 1-s blank interval between\\ntwo successive image presentations. Among the 8 participants, only 4 (namely 1, 2, 5 and 7) completed all\\nsessions.\\nTo compute the three latents used to reconstruct the seen images from fMRI data (as described in Section 2.5)\\nwe follow Ozcelik and VanRullen (2023) and train and evaluate three distinct Ridge regression models using the\\nexact same split. That is, for each of the four remaining participants, the 9,000 uniquely-seen-per-participant\\nimages (and their three repetitions) are used for training, and a common set of 1000 images seen by all\\nparticipant is kept for evaluation (also with their three repetitions). We report reconstructions and metrics\\nfor participant 1.\\nThe α coefficient for the L2-regularization of the regressions are cross-validated with a 5-fold scheme on the\\ntraining set of each subject. We follow the same standardization scheme for inputs and predictions as in\\nOzcelik and VanRullen (2023).\\nFig. S2 presents generated images obtained using the NSD dataset (Allen et al., 2022).\\nE\\nLinear Ridge regression scores on pretrained image representations\\nWe provide a (5-fold cross-validated) Ridge regression baseline (Table S2) for comparison with our brain\\nmodule results of Section 3, showing considerable improvements for the latter.\\n16'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 16}, page_content='Figure S2 Examples of generated images conditioned on fMRI-based latent predictions. The groups of three stacked\\nrows represent best, average and worst retrievals, as evaluated by the sum of (minus) SwAV and SSIM.\\nTable S2 Image retrieval performance of a linear Ridge regression baseline on pretrained image representations.\\nTop-5 acc (%) ↑\\nMedian relative rank ↓\\nLatent kind\\nLatent name\\nSmall set\\nLarge set\\nSmall set\\nLarge set\\nText/Image\\nalignment\\nCLIP-Vision (CLS)\\n10.5\\n0.50\\n0.23\\n0.34\\nCLIP-Text (mean)\\n6.0\\n0.25\\n0.42\\n0.43\\nCLIP-Vision (mean)\\n5.5\\n0.46\\n0.32\\n0.37\\nFeature\\nengineering\\nColor histogram\\n7.0\\n0.33\\n0.31\\n0.40\\nLocal binary patterns (LBP)\\n3.5\\n0.37\\n0.34\\n0.44\\nFFT 2D (as real)\\n4.5\\n0.46\\n0.40\\n0.45\\nHOG\\n3.0\\n0.42\\n0.45\\n0.46\\nFFT 2D (log-PSD and angle)\\n2.0\\n0.37\\n0.47\\n0.46\\nVariational\\nautoencoder\\nAutoKL\\n7.5\\n0.54\\n0.24\\n0.38\\nVDVAE\\n8.0\\n0.50\\n0.33\\n0.43\\nSelf-supervised\\nlearning\\nDINOv2 (CLS)\\n7.5\\n0.46\\n0.25\\n0.35\\nSupervised\\nVGG-19\\n11.5\\n0.67\\n0.17\\n0.31\\nF\\nImpact of choice of layer in supervised models\\nWe replicate the analysis of Fig. 2 on different layers of the supervised model (VGG-19). As shown in Table S3,\\nsome of these layers slightly outperform the last layer. Future work remains necessary to further probe which\\nlayer, or which combination of layers and models may be optimal to retrieve images from brain activity.\\n17'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 17}, page_content='Table S3 Image retrieval performance of intermediate image representations of the VGG-19 supervised model.\\nTop-5 acc (%) ↑\\nMedian relative rank ↓\\nLatent kind\\nLatent name\\nSmall set\\nLarge set\\nSmall set\\nLarge set\\nSupervised\\nVGG-19 (last layer)\\n70.333\\n12.292\\n0.005\\n0.013\\nVGG-19 (avgpool)\\n73.833\\n17.417\\n0.000\\n0.006\\nVGG-19 (classifier_dropout_2)\\n73.833\\n17.375\\n0.000\\n0.005\\nVGG-19 (classifier_dropout_5)\\n74.500\\n16.403\\n0.000\\n0.007\\nVGG-19 (maxpool2d_35)\\n64.333\\n13.278\\n0.005\\n0.014\\nG\\nMEG-based image retrieval examples\\nFig. S3 shows examples of retrieved images based on the best performing latents identified in Section 3.\\nTo get a better sense of what time-resolved retrieval yields in practice, we present the top-1 retrieved images\\nfrom an augmented retrieval set built by concatenating the “large” test set with an additional set of 3,659\\nimages that were not seen by the participants (Fig. S4).\\nH\\nMEG-based image generation examples\\nFig. S5 shows representative examples of generated images obtained with our diffusion pipeline3.\\nFig. S6 specifically shows examples of failed generations. Overall, they appear to encompass different types\\nof failures. Some generations appear to miss the correct category of the true object (e.g. bamboo, batteries,\\nbullets and extinguisher in columns 1-4), but generate images with partially similar textures. Other generations\\nappear to recover some category-level features but generate unrealistic chimeras (bed: weird furniture, alligator:\\nswamp beast; etc. in columns 5-6). Finally, some generations seem to be completely wrong, with little-to-no\\npreservation of low- or high-level features (columns 7-8). We speculate that these different types of failures\\nmay be partially resolved with different methods, such as better generation modules (for chimeras) and\\noptimization on both low- and high-level features (for category errors).\\nI\\nPerformance of temporally-resolved image retrieval with growing windows\\nTo complement the results of Fig. 3 on temporally-resolved retrieval with sliding windows, we provide a\\nsimilar analysis in Fig. S7, instead using growing windows. Beginning with the window spanning -100 to\\n0 ms around image onset, we grow it by increments of 25 ms until it spans both stimulus presentation and\\ninterstimulus interval regions (i.e., -100 to 1,500 ms). Separate models are finally trained on each resulting\\nwindow configuration.\\nConsistent with the decoding peaks observed after image onset and offset (Fig. 3), the retrieval performance\\nof all growing-window models considerably improves after the offset of the image. Together, these results\\nsuggest that the brain activity represents both low- and high-level features even after image offset. This\\nfinding clarifies mixed results previously reported in the literature. Carlson et al. (2011, 2013) reported\\nsmall but significant decoding performances after image offset. However, other studies (Cichy et al., 2014;\\nHebart et al., 2023) did not observe such a phenomenon. In all these cases, decoders were based on pairwise\\nclassification of object categories and on linear classifiers. The improved sensitivity brought by (1) our deep\\nlearning architecture, (2) its retrieval objective and (3) its use of pretrained latent features may thus help\\nclarify the dynamics of visual representations in particular at image offset. We speculate that such offset\\nresponses could reflect an intricate interplay between low- and high-level processes that may be difficult to\\ndetect with a pairwise linear classifier. We hope that the present methodological contribution will help shine\\nlight on this understudied phenomenon.\\n3Images may look slightly different from those in Fig. 4 due to different random seeding.\\n18'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 18}, page_content='Table S4 Quantitative evaluation of reconstruction quality from MEG data on THINGS-MEG for each participant. We\\nuse the same metrics as in Table 1.\\nLow-level\\nHigh-level\\nParticipant\\nPixCorr ↑\\nSSIM ↑\\nAlexNet(2) ↑\\nAlexNet(5) ↑\\nInception ↑\\nCLIP ↑\\nSwAV ↓\\n1\\n0.070 ± 0.009\\n0.338 ± 0.015\\n0.741\\n0.814\\n0.672\\n0.768\\n0.590 ± 0.007\\n2\\n0.081 ± 0.010\\n0.341 ± 0.015\\n0.788\\n0.879\\n0.710\\n0.799\\n0.560 ± 0.008\\n3\\n0.073 ± 0.010\\n0.335 ± 0.015\\n0.725\\n0.825\\n0.675\\n0.770\\n0.588 ± 0.008\\n4\\n0.082 ± 0.009\\n0.328 ± 0.014\\n0.701\\n0.797\\n0.634\\n0.744\\n0.599 ± 0.008\\nJ\\nPer-participant image generation performance\\nTable S4 provides the image generation metrics at participant-level. For each participant, we compute metrics\\nover the 200 generated images obtained by averaging the outputs of the brain module for all 12 presentations\\nof the stimulus.\\nK\\nAnalysis of temporal aggregation layer weights\\nWe inspect our decoders to better understand how they use information in the time domain. To do so, we\\nleverage the fact that our architecture preserves the temporal dimension of the input up until the output of\\nits convolutional blocks. This output is then reduced by an affine transformation learned by the temporal\\naggregation layer (see Section 2.3 and Appendix A). Consequently, the weights wagg ∈RT can reveal on\\nwhich time steps the models learned to focus. To facilitate inspection, we initialize wagg to zeros before\\ntraining and plot the mean absolute weights of each model (averaged across seeds).\\nThe results are presented in Fig. S8. While these weights are close to zero before stimulus onset, they deviate\\nfrom this baseline after stimulus onset, during the maintenance period and after stimulus offset. Interestingly,\\nand unlike high-level features (e.g. VGG-19, CLIP-Vision), low-level features (e.g. color histogram, AutoKL\\nand DINOv2) have close-to-zero weights in the 0.2-0.5 s interval.\\nThis result suggests that low-level representations quickly fade away at that moment. Overall, this analysis\\ndemonstrates that the models rely on these three time periods to maximize decoding performance, including\\nthe early low-level responses (t =0-0.1 s).\\nL\\nTemporally-resolved image generation metrics\\nAkin to the time-resolved analysis of retrieval performance shown in Fig. 3, we evaluate the image reconstruction\\nmetrics used in Table 1 on models trained on 100-ms sliding windows. Results are shown in Fig. S9.\\nLow-level metrics peak in the first 200 ms while high-level metrics reach a performance plateau that is\\nmaintained throughout the image presentation interval. As seen in previous analyses (Fig. 3, S7 and S8), a\\nsharp performance peak is visible for low-level metrics after image offset.\\n19'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 19}, page_content='Figure S3 Representative examples of retrievals (top-4) using models trained on full windows (from -0.5 s to 1 s after\\nimage onset). Retrieval set: N =6,059 images from 1,196 categories.\\n20'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 20}, page_content='Figure S4 Representative examples of dynamic retrievals using CLIP-Vision (CLS) and models trained on 250-ms\\nnon-overlapping sliding windows (Image onset: t = 0, retrieval set: N =6,059 from 1,196 categories). The groups\\nof three stacked rows represent best, average and worst retrievals, obtained by sampling examples from the <10%,\\n45-55% and >90% percentile groups based on top-5 accuracy.\\n21'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 21}, page_content='Figure S5 Representative examples of generated images conditioned on MEG-based latent predictions. The groups of\\nthree stacked rows represent best, average and worst generations, as evaluated by the sum of (minus) SwAV and SSIM.\\n22'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 22}, page_content='Figure S6 Examples of failed generations. (A) Generations obtained on growing windows starting at image onset (0 ms)\\nand ending at the specified time. (B) Full-window generations (-500 to 1,000 ms).\\n23'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 23}, page_content='Figure S7 Retrieval performance of models trained on growing windows (from -100 ms up to 1,500 ms relative to\\nstimulus onset) for different image embeddings. The shaded gray area indicates the 500-ms interval during which\\nimages were presented to the participants and the horizontal dashed line indicates chance-level performance. Accuracy\\nplateaus a few hundreds of milliseconds after both image onset and offset.\\nFigure S8 Mean absolute weights learned by the temporal aggregation layer of the brain module. Retrieval models\\nwere trained on five different latents. The absolute value of the weights of the affine transformation learned by the\\ntemporal aggregation layer were then averaged across random seeds and plotted against the corresponding timesteps.\\nThe shaded gray area indicates the 500-ms interval during which images were presented to the participants.\\n24'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-15T01:29:53+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain decoding_ toward real-time reconstruction of visual perception.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-03-15T01:29:53+00:00', 'trapped': '', 'modDate': 'D:20240315012953Z', 'creationDate': 'D:20240315012953Z', 'page': 24}, page_content='Figure S9 Temporally-resolved evaluation of reconstruction quality from MEG data. We use the same metrics as in\\nTable 1 to evaluate generation performance from sliding windows of 100 ms with no overlap. (A) Normalized metric\\nscores (min-max scaling between 0 and 1, metric-wise) across the post-stimulus interval. (B) Unnormalized scores\\ncomparing, for each metric, the score at stimulus onset and the maximum score obtained across all windows in the\\npost-stimulus interval. Dashed lines indicate chance-level performance and error bars indicate the standard error of\\nthe mean for PixCorr, SSIM and SwAV.\\n25'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 0}, page_content='Adaptive neural network classiﬁer for decoding MEG signals\\nIvan Zubareva,∗, Rasmus Zettera, Hanna-Leena Halmea, Lauri Parkkonena,b\\naDepartment of Neuroscience and Biomedical Engineering, Aalto University School of Science, FI-00076 Aalto, Finland\\nbAalto NeuroImaging, Aalto University, FI-00076 Aalto, Finland\\nAbstract\\nConvolutional Neural Networks (CNN) outperform traditional classiﬁcation methods in many domains. Recently these\\nmethods have gained attention in neuroscience and particularly in brain–computer-interface (BCI) community. Here,\\nwe introduce a CNN optimized for classiﬁcation of brain states from magnetoencephalographic (MEG) measurements.\\nOur CNN design is based on a generative model of the electromagnetic (EEG and MEG) brain signals and is readily\\ninterpretable in neurophysiological terms. We show here that the proposed network is able to decode event-related\\nresponses as well as modulations of oscillatory brain activity and that it outperforms more complex neural networks and\\ntraditional classiﬁers used in the ﬁeld. Importantly, the model is robust to inter-individual diﬀerences and can successfully\\ngeneralize to new subjects in oﬄine and online classiﬁcation.\\nKeywords:\\nconvolutional neural network, magnetoencephalography, brain–computer interface\\n1. Introduction\\nDeep Neural Networks have revolutionized many do-\\nmains such as image recognition and natural language\\nprocessing. To date, their application in the analysis of\\nelectro- and magnetoencephalographic (EEG and MEG)\\ndata has been limited by several domain-speciﬁc factors.\\nFirst of all, electromagnetic brain signals are charac-\\nterized by very low signal-to-noise ratio (SNR). Here, the\\nterm \"noise\" is understood widely and includes external\\ninterference, physiological (e.g. cardiac or oculomotor) arti-\\nfacts as well as background brain activity unrelated to the\\nstudied phenomena. SNR in single-trial EEG and MEG\\nmeasurements is typically assumed to be < 1 for evoked\\nresponses and ≈1 for oscillatory activity, which puts these\\ndata to stark contrast with those in traditional applications\\nof deep learning. Typical EEG/MEG analysis employs a\\nwide range of techniques to increase the SNR, e.g. spatial\\nand temporal ﬁltering, averaging a large number of obser-\\nvations, source-separation algorithms, and other complex\\nfeature extraction methods (e.g. wavelet transform). Thus,\\neﬃcient noise suppression is required for high-accuracy\\nclassiﬁcation of EEG/MEG signals.\\nSecond, these data have a complex, high-dimensional\\nspatiotemporal structure. Modern EEG and MEG systems\\ncomprise several hundreds of sensors capable of sampling\\nbrain activity with sub-millisecond temporal resolution. In\\ncase of MEG, these sensors may also measure diﬀerent\\ncomponents of the neuromagnetic ﬁeld. On one hand, this\\nmultitude of data points enables sophisticated experimen-\\ntal designs and analysis methods to extract ﬁner details of\\n∗Corresponding author ivan.zubarev@aalto.ﬁ\\nbrain function. On the other hand, manual analysis and\\ninterpretation of these data becomes increasingly complex\\nand time-consuming. Machine-learning algorithms can be\\nof great help in such tasks but the mere classiﬁcation re-\\nsult is often not suﬃcient; ideally, the experimenter should\\nunderstand why the algorithm is able classify the data, i.e.,\\nthe learned model should be interpretable in neurophysio-\\nlogical terms. A model able to reliably identify those neural\\nsources that contribute to the discrimination between given\\nexperimental conditions could enable eﬃcient exploitative\\nanalysis of these complex data sets and ultimately allow\\nmore complex experimental designs.\\nFinally, sample sizes in EEG/MEG data sets are typi-\\ncally too small for deep-learning models. One reason for\\nthis is the high cost and time limit of collecting these data.\\nRunning an experiment on a single human subject for many\\nhours to collect a large enough data set is often not feasible.\\nOn the other hand, pooling data from many subjects is\\na promising strategy to overcome this limitation, but it\\nrequires the classiﬁer to be robust to high inter-individual\\nvariability stemming from diﬀerences in cortical anatomy,\\nphysiological state etc.\\nTaken together, these factors may easily lead to over-\\nﬁtting (especially in more complex models) and poor in-\\nterpretability of the models. To address these challenges,\\nwe propose a Convolutional Neural Network (CNN) whose\\narchitecture is based on a generative model of non-invasive\\nelectromagnetic measurements of the brain activity (Dau-\\nnizeau and Friston, 2007).\\nThis network utilizes spatiotemporal structure in the\\nMEG data to extract informative components of MEG sig-\\nnal from the noisy observations. Since the model structure\\nreﬂects our understanding of the data generation process,\\nPreprint submitted to Neuroimage\\nFebruary 12, 2019\\narXiv:1805.10981v2  [cs.LG]  10 Feb 2019'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 1}, page_content='the extracted components can be interpreted in terms of\\nthe underlying neural activity.\\nSpeciﬁcally, this model\\nassumes that the MEG measurements are generated by\\na linear (spatial) mixture of a limited number of latent\\nsources, which evolve non-linearly over time. Each of these\\nsources is characterized by a spatial distribution and a time\\ncourse, which are relatively stable within each individual\\nbut may vary across individuals. For example, the spatial\\ntopography of a sensory evoked response may vary across\\nsubjects due to small diﬀerences in cortical anatomy but its\\nlatency can be relatively constant. Conversely, the phase\\nof an event-related (induced) oscillatory response may vary\\nconsiderably across trials while its spectral content and\\nspatial distribution remain the same.\\nWe demonstrate that utilizing a generative model makes\\nthe algorithm robust to inter-individual diﬀerences in spa-\\ntial, temporal and spectral properties of the signal. Criti-\\ncally, such across-subject generalization makes it possible\\nto train the model on pooled data from multiple subjects\\nand successfully apply it to new subjects. Here, we applied\\nthis neural network to classify evoked responses to visual,\\nauditory and somatosensory stimuli (5 classes; Experiment\\n1) and induced responses to hand motor imagery (3 classes;\\nExperiment 2). Finally, we ran the algorithm in real time\\nand tested it in a brain–computer interface (BCI) (2 classes;\\nExperiment 3).\\n2. Methods\\n2.1. Generative latent state-space model\\nMagnetoencephalography (MEG) is a non-invasive, time-\\nresolved technique for measuring electric brain activity\\nthrough the magnetic ﬁeld it generates (Hämäläinen et al.,\\n1993). The MEG signal is complementary to that of elec-\\ntroencephalography (EEG), in which the potential distri-\\nbution caused by electric brain activity is measured using\\nelectrodes placed on the scalp. MEG is considered to have\\nhigher spatial resolution than EEG, as the EEG signal is\\ndistorted by the heterogeneous conductivity proﬁle of head\\ntissues to a much larger extent than the MEG signal (see\\ne.g. Baillet, 2017).\\nMEG data typically include 1) stereotyped evoked re-\\nsponses (event-related ﬁelds; ERF) that are phase-locked\\nto speciﬁc sensory, cognitive or motor events, and 2) in-\\nduced modulations of ongoing oscillatory brain activity\\nthat is not phase-locked to external events. MEG measure-\\nments are typically contaminated by noise and interference\\noriginating from external sources as well as by ongoing\\nunrelated brain activity. Single-trial ERFs typically have a\\nsignal-to-noise ratio (SNR) ∼1.\\nAn MEG measurement can be represented by an n × t\\ndata matrix X containing measurements from n sensors\\n(magnetometers or gradiometers; typ. 200–300) at t time\\npoints sampled at a high temporal frequency (typ. ∼1000\\nHz). These data have a complex spatiotemporal structure\\nbecause an activation of a single neural source is picked up\\nby several sensors at diﬀerent spatial locations and these\\nsignals exhibit temporal correlations. Thus, simultaneously\\nactive neural sources result in a high degree of linear spatial\\nmixing as well as non-linear temporal dependencies in the\\nmeasured data. Fortunately, dense spatial and temporal\\nsampling allow eﬃcient source-separation by utilizing local\\nspatiotemporal correlations(Cardoso, 1998). We argue that\\nan eﬀective approach towards decoding brain states should\\ntake into account these properties of the signal.\\nThe proposed network architecture is broadly based\\non an extension of a model describing the generation of\\nMEG signal (Daunizeau and Friston, 2007). The model is\\nmotivated by the assumption that a single event-related\\nMEG observation X ∈Rn×t is generated by a mixture of\\nk latent sources s such that at each time point t\\nxt = Cst + ϵ\\n(1)\\nwhere C is an n × k matrix describing the spatial mixing\\nof the k underlying latent sources whose time courses s are\\nthe rows of a matrix S and ϵ is additive Gaussian white\\nobservation noise. The number of such latent sources is\\nsmall relative to the number of MEG channels (i.e., k ≪n).\\nThese k sources evolve in time in a structured way and\\nmay or may not be statistically dependent. Importantly,\\nwe do not restrict the deﬁnition of a latent source to neu-\\nral activity as some of these sources may correspond to\\ne.g. ocular or muscular artifacts. In the simplest case,\\nthe mapping from S to X is linear, and non-linearities in\\ntemporal dynamics of S can be locally approximated by a\\nlinear autoregressive (AR) model A of order L with inno-\\nvation noise ω. Given the fast temporal sampling of MEG,\\nsuch local linearity in the temporal domain is a reasonable\\nassumption. Thus,\\nst =\\nL\\nX\\nl=1\\nAlst−l\\n(2)\\nAssuming no interaction between the latent sources, Al\\nbecomes a diagonal submatrix, where the k-th diagonal\\nelement of each of the L submatrices form a L−th order\\nunivariate AR model of the temporal dynamics of the k-\\nth source. The coeﬃcients of these AR models contain\\ninformation about spectral properties of the sources. Fur-\\nthermore, if Al is a full submatrix, its oﬀ-diagonal elements\\nmodel the interactions between the latent sources, leading\\nto A being a full L-th order Vector-Autoregressive (VAR)\\nmodel of k interacting sources.\\n2.2. Network architecture\\nThe proposed classiﬁer incorporates the assumptions\\nof the generative model described above into the discrim-\\ninative neural network model. The ﬁrst and the second\\nlayers of the network learn spatial and temporal ﬁlters,\\nwhich extract a compact representation of MEG signal\\nfeatures contributing to the discrimination between the\\nclasses. These features make use of spatial and temporal\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 2}, page_content='correlations in the data to suppress noise and to obtain suf-\\nﬁcient separation between the simultaneously active neural\\nsources. The l1-regularized output layer assigns non-zero\\nweights only to features that are informative for each class\\nof the stimuli. Finally, the spatial and temporal ﬁlters\\nare further optimized by back-propagating errors from the\\noutput-layer nodes with non-zero weights.\\nInput layer: Spatial de-mixing. The linear input layer\\ntrains a set of spatial ﬁlters W with each column wk\\nextracting a timecourse of k-th latent source. These ﬁlters\\nare related to the spatial activation patterns C of the latent\\nsources in the generative model (Eq. 1) via\\nWT xt = ˆst\\n(3)\\nC = ΣxWΣ−1\\nˆs\\n(4)\\nwhere Σx is the spatial data covariance and Σ−1\\nˆs\\nis the\\nprecision matrix of the latent time courses(Haufe et al.,\\n2014).\\nThe input layer can be viewed as a linear projection\\nlayer performing dimensionality reduction in the spatial\\ndomain. The weights of the input layer are applied to each\\ntime point in the MEG epoch by computing dot product\\nbetween W and the whole MEG epoch X. This layer im-\\nplements a set of spatial ﬁlters that project the channel\\ndata onto a k-dimensional subspace. This layer has several\\nfunctions: (1) it obtains a lower-dimensional and spatially-\\ndecorrelated representation of the signal time courses, (2)\\nit learns and projects out irrelevant activity such as physio-\\nlogical artifacts, and (3) it provides an interpretable linear\\nmapping from model weights to the channels in the original\\nsignal space. These weights can be used to extract spatial\\nactivation patterns (Haufe et al., 2014), or topographic\\nmaps, corresponding to neural sources informing the classi-\\nﬁers. Alternatively, this layer can be viewed as a spatial\\nconvolution layer with ’valid’ padding applied to all chan-\\nnels at each time-point. Here, we prefer to refer to it as\\na linear projection layer for ease of interpretation. Linear\\nprojections similar those for the function (2) above are typ-\\nically used to suppress ocular and cardiac artifacts in MEG\\ndata. However, in contrast to e.g. independent component\\nanalysis (ICA), the projection basis is deﬁned by back-\\npropagation without introducing an explicit assumption of\\nstatistical independence.\\nTemporal convolution layer: Extraction of activation dy-\\nnamics. This layer operates on the time courses of the\\nlatent sources (corresponding to the rows of S) and imple-\\nments a ﬁlter, which extracts a temporal activation pattern\\nof the informative neural event (e.g. a peak of an evoked\\nresponse).\\nWe used two variants of this layer. The simpler one\\n(LF-CNN) applies separate 1-dimensional convolution ﬁl-\\nters of the l-th order to the time courses of the k spatial\\ncomponents produced by the input layer. The model as-\\nsumes that these time courses do not interact and that\\nthey have unique spectral ﬁngerprints. This layer variant\\ncan be viewed as applying linear ﬁnite-impulse-response\\nﬁlters (hence LF) that speciﬁcally capture the ﬁngerprint\\nof each spatial component.\\nThe more complex variant allows estimating the inter-\\nactions between the spatial components and can be viewed\\nas a vector autoregressive model (VAR-CNN) of the com-\\nponent time courses. This structure is implemented by\\napplying k spatiotemporal convolution kernels of shape\\nl × k and it results in a larger set of trainable parameters\\n(lk2) in this layer. Thus, each spatial pattern from the\\ninput layer has a corresponding impulse-response function\\nlearned by the temporal convolution layer. These functions\\nextract the frequency bands speciﬁc for each component.\\nFor both variants of this layer, the convolution is fol-\\nlowed by a non-linearity using rectiﬁed linear units (ReLU)\\nand a max-pooling layer with a pooling factor of 2 and\\na stride of 2 applied to the time dimension. Temporal\\nmax-pooling provides robustness against variation in the\\nlatency of the informative responses across subjects.\\nOutput layer: Imposing sparsity. The mapping from the\\ntemporal convolution layer to the output is provided by a\\nsingle, fully-connected layer followed by a soft-max normal-\\nization. Sparsity is imposed on the weights of the output\\nlayer using l1-norm regularization suppressing most of the\\nactivity that is unrelated to the classiﬁcation. Exploring\\nthe non-zero weights in the output layer thus allows to iden-\\ntify the temporal and spatial patterns which contribute to\\nthe discrimination.\\n2.3. Model inspection and parameter interpretation\\nInterpretation of the model parameters in terms of the\\nunderlying neural activity is a desirable property. Our\\nnetwork design is based on a generative model of MEG\\nsignal to allow such interpretation.\\nTo identify spatial and temporal features that con-\\ntribute to assignment of a given sample to a particular\\nclass, we identiﬁed the nodes of the ﬁnal classiﬁcation layer\\ncontaining the maximum positive weights (contributions)\\nto each particular class. Since the input to the ﬁnal layer\\n(before ﬂattening) has the dimensions corresponding to:\\n1. number of latent components\\n2. number of (pooled) time points\\n3. number of classes\\nwe can identify the index of the latent component (or several\\ncomponents) that has a maximum contribution to this class\\nas well as the (approximate) timepoint corresponding to\\nthe maximum activation of this component. These indices\\nare then used to extract the corresponding spatial ﬁlter\\nfrom the input layer and temporal convolutional ﬁlter from\\nthe hidden layer. Thus, a single feature in the output layer\\nmaps to the information about spatial pattern, approximate\\nlatency and spectral properties of the latent component.\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 3}, page_content='Figure 1: Architecture of the two variants of the CNN.\\nSpatial activation patterns and source estimates. Assuming\\nstatistical independence between the latent components one\\ncan extract the corresponding activation pattern by multi-\\nplying the spatial ﬁlter by the spatial covariance matrix of\\nthe data (Haufe et al., 2014). To relax the independence\\nassumption further, one can also consider multiplying the\\nresulting pattern by the precision matrix of the latent time\\ncourses that can be obtained by inverting the covariance\\nmatrix of the data, projected onto the latent component\\nspace with the spatial ﬁlters extracted during the previous\\nstep. In this study this latter step was omitted, because it\\nis unclear whether the spatial ﬁlters optimized to extract\\nonly the informative parts of the latent time course could\\nbe used to estimate the latent precision matrix without\\ndistortion (Supplementary Figure 1).\\nThe obtained sensor-level spatial pattern can further\\nbe mapped onto the individual source space using standard\\nsource estimation algorithms (Figure 2). To demonstrate\\nthis mapping we trained an LF-CNN model on pooled data\\nfrom Experiment 1 and updated it with a single training\\nepoch performed on the data of a single held-out subject.\\nFor each of the 5 classes of stimuli, we extracted the model\\nparameters corresponding to spatial ﬁlters of the single\\nmost informative component using the procedure described\\nabove. These patterns were compared to components that\\nhad minimal or no contribution to any of the classes as\\ndeﬁned by the absolute sum of the corresponding weights\\nin the output layer (Figure 3). We then estimated the\\nneural sources of each informative component by comput-\\ning dynamic statistical parametric maps (dSPM) (Dale\\net al., 2000) for sources constrained onto the individual\\ncortical surface and with orientations favoring the direction\\nperpendicular to the local cortical surface (loose orienta-\\ntion constraint 0.2) as implemented in the MNE-Python\\nsoftware package (Gramfort et al., 2013b).\\nWe then conducted a standard evoked-response analysis\\nby averaging the MEG responses within each class of stim-\\nuli. We compared the features extracted from our model\\nto the most prominent components of the corresponding\\naveraged evoked responses in terms of their latency and\\nspatial distribution and the corresponding source estimates.\\nSource estimation of the averaged evoked responses uti-\\nlized noise covariance estimated from a 300-ms pre-stimulus\\nbaseline whereas identity covariance was used for activation\\npatterns.\\nInvestigating spectral properties of extracted latent compo-\\nnents. Since each temporal convolution ﬁlter in the hidden\\nlayer of the LF-CNN represents a univariate autoregressive\\nmodel of the temporal dynamics of k-th latent source, we\\ncan use the coeﬃcients of this ﬁlter to obtain an estimate\\nof the power spectral density properties of the time course\\nof this latent source. To test whether this approach can be\\nused to identify oscillatory activity that is informative for\\nclassiﬁcation we extracted the spatial and spectral ﬁlters\\nfrom the model trained on the data from Experiment 2. Be-\\ncause we expected informative activity to be extended over\\nthe whole 1.5 s time window, since event-related desynchro-\\nnization associated with motor imagery is not phase-locked\\nto the stimulus onset, we used a diﬀerent approach to iden-\\ntify the most informative latent component compared to\\nthe Experiment 1. Instead of identifying a single feature in\\nthe output layer, corresponding to highest activation at a\\nsingle time-point, we took the index of spatial component\\nthat had the largest sum of weights over all time-points. We\\nextracted spatial patterns corresponding to two most infor-\\nmative latent components for each class. We also estimated\\nthe frequency content of the component’s time course by\\ncomputing power spectral density using the weights of the\\ncorresponding temporal convolutional ﬁlters.\\n2.4. Implementation and training\\nDesign choices and hyperparameters. The neural network\\nwas implemented using the Tensorﬂow library (Abadi et al.,\\n2016). The code is publicly available at https://version.\\naalto.fi/gitlab/zubarei1/aalto-megnet.\\nModel de-\\nvelopment and hyperparameter tuning were performed on\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 4}, page_content='the data of a single randomly-picked subject from Dataset\\n1. The model was then applied to across-subject classiﬁca-\\ntion and other experiments as is. Table 1 summarizes the\\ntested values of tunable hyperparameters.\\nInitialization and training. The initial values of the weight\\nmatrices were drawn from a uniform distribution following\\nthe procedure introduced by He et al. (2015).\\nWe initialized the bias variables to a constant value\\nof 0.1. We used the Adam optimization algorithm with a\\nbatch size of 100 and learning rate of 3.0 · 10−4 to optimize\\nmultinomial cross-entropy between the model predictions\\nand true labels. Higher learning rates were also used but\\nthey did not improve performance.\\nWe used an early-\\nstopping strategy to prevent over-ﬁtting; for every 1000\\niterations, we computed the validation cost (multinomial\\ncross-entropy) and stopped the iterations immediately if\\nthe cost function value was increasing or decreasing by less\\nthan 1.0 · 10−5. The early-stopping criteria were typically\\nmet within 20 000 iterations, corresponding to a maximum\\ntraining time of 32 minutes using a normal workstation\\nCPU only.\\nRegularization. We examined several regularization ap-\\nproaches including drop-out, l1 and l2 penalties on the\\nmodel weights as well as the pairwise combinations of\\ndrop-out and weight penalties. A combination of drop-out\\nregularization applied to the output layer and l1 penalty\\napplied to all weight variables resulted in the highest model\\nperformance and was used with all datasets.\\nPerformance evaluation. Since our main focus was on de-\\nveloping a model that generalizes across subjects, we used\\nthe leave-one-subject-out method to evaluate model perfor-\\nmance. Thus, for a dataset of m subjects, the training (90%\\nof all trials) and validation (10% of all trials) sets comprised\\npooled data from randomly-selected m −1 subjects. The\\nmodel was then applied to the data of the held-out subject,\\nand the following two scores were computed. As all our\\ndatasets comprised an equal number of trials for each cat-\\negory, we used classiﬁcation accuracy as the performance\\nmetric. Initial test accuracy was deﬁned as the proportion\\nof correct predictions on the held-out subject. Pseudo-real-\\ntime accuracy was deﬁned as the mean prediction accuracy\\nin a simulated real-time design where the model predicted\\nnew observations in batches of 20 trials and was updated\\nafter each prediction. For the true real-time BCI experi-\\nment (Experiment 3), the actual BCI accuracy (with and\\nwithout model updates) is reported.\\n2.5. Benchmark classiﬁers\\nRBF and linear SVMs. SVMs (Vapnik, 2000) are widely\\nused in classiﬁcation of MEG data (e.g. Gramfort et al.,\\n2013a; Westner et al., 2018). We used incremental ver-\\nsions of linear and radial basis function (RBF) -kernel\\nSupport Vector Machines (SVM) as benchmark classiﬁers.\\nData preprocessing, scaling and classiﬁer training proce-\\ndure for these methods was identical to the one reported\\nfor LF-CNN and VAR-CNN. Data points from all chan-\\nnels/timepoints were concatenated forming a single feature\\nvector.\\nNo additional feature extraction methods were\\nused. We refer the reader to our previous study (Halme\\nand Parkkonen, 2018) comparing various feature extraction\\nmethods in combination with linear classiﬁers for across-\\nsubject classiﬁcation of MEG data. Nyström RBF kernel\\napproximation was used for incremental RBF-SVM as im-\\nplemented in the Scikit-Learn package(Pedregosa et al.,\\n2011). The SVM inverse regularization parameter C and\\nthe kernel lengthscale parameter for RBF kernel γ were set\\nby performing a search over a 2-d grid of 5 logarithmically\\nspaced values from 103 to 105 for C and from 10−2 to 10−7\\nfor γ. The classiﬁer that gave the highest validation set\\naccuracy was evaluated on the test set.\\nCNNs developed for EEG classiﬁcation. We used two CNN\\nmodels developed for classiﬁcation of EEG data. Shallow\\nFBCSP-CNN (Schirrmeister et al., 2017) is a model inspired\\nby Filter-Bank Common Spatial Pattern (FBCSP), a state-\\nof-the-art method for extracting band-power features in\\nEEG/MEG. Its architecture comprises a 1-d temporal-\\nconvolution input layer (40 ﬁlters) followed by a spatial-\\nﬁlter layer (40 ﬁlters) and mean pooling. The outputs of\\nthe pooling layer are then combined linearly to produce\\nlabel predictions by applying the softmax function. We\\nused the Shallow FBCSP-CNN implementation provided\\nin the Braindecode library with default parameters, only\\nmodifying temporal ﬁlters and pooling factors to match\\nthe sampling rate of our data (125 Hz). We were not able\\nto perform pseudo-real time test for FBCSP-CNN due to\\nthe diﬀerences in implementation.\\nEEGnet (Lawhern et al., 2018) is a compact model de-\\nsigned speciﬁcally to optimize across-subject generalization.\\nThe model uses a combination of 1-d depth-wise and sepa-\\nrable convolution layers (a total of 4 layers) and has been\\nshown to generalize well across subjects in a large number\\nof datasets. We implemented EEGNet-8 in Tensorﬂow fol-\\nlowing the description provided in Lawhern et al. (2018)\\nand tested it in a simulated real-time set-up, similarly to\\nVAR-CNN and LF-CNN.\\nDeep CNNs developed for image classiﬁcation. As an exam-\\nple of general-purpose deep convolutional network, we used\\nVGG19 – a 19-layer convolutional network and winner of\\nImageNet Challenge 2014 (Simonyan and Zisserman, 2015).\\nThe VGG19 architecture includes 5 blocks of convolutional\\nlayers followed by three fully-connected layers. Each of\\nthe 5 blocks includes a stack of several (2, 2, 4, 4, and\\n4) convolutional layers with (64, 128, 256, 512, and 512)\\n3×3 convolution kernels and a 2×2 max-pooling layer with\\nstride 2. Due to the fact that the scaling of MEG data\\nwas diﬀerent from that of the ImageNet dataset, we have\\nintroduced a batch-normalization layer after each block of\\nconvolutional layers to mitigate the risk of exploding or van-\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 5}, page_content='Table 1: Tested and optimal hyperparameter values.\\nParameter\\nTested\\nOptimal\\nNumber of latent sources\\n16, 32, 64\\n32\\nTemporal ﬁlter length\\n3, 5, 7, 9, 11\\n7\\nLearning rate\\n1 · 10−3, 3 · 10−4, 1 · 10−4\\n3 · 10−4\\nl1-penalty\\n1 · 10−3, 3 · 10−4, 1 · 10−4\\n3 · 10−4\\nPooling\\nmax\\nmax\\nPooling factor\\n2, 3, 5\\n2\\nDrop-out coeﬃcient\\n0.25, 0.50, 0.75, 0.90\\n0.50\\nInput layer link function\\nidentity, ReLU, ELU, tanh\\nidentity\\nHidden layer link function\\nidentity, ReLU, ELU, tanh\\nReLU\\nOutput nonlinearity\\nsigmoid, softmax\\nsoftmax\\nNumber of dense hidden layers\\n1, 2\\n1\\nishing gradients. The ﬁnal layer uses softmax non-linearity\\nwhile all hidden layers are equipped with the ReLU non-\\nlinearity. For further details on VGG19 implementation,\\nsee Simonyan and Zisserman (2015).\\n3. Datasets\\nMEG recordings were acquired using an Elekta Neuro-\\nmag Vectorview (MEGIN / Elekta Oy, Helsinki, Finland)\\nMEG system, which includes 306 sensors at 102 positions\\naround the head; two orthogonal planar gradiometers and\\na magnetometer at each position.\\nOnly data from the\\nplanar gradiometers (204 channels) were used in this work.\\nAll experiments had their respective approvals from Aalto\\nUniversity Committee on Research Ethics.\\nAll MEG data were sampled at 1000 Hz, band-pass ﬁl-\\ntered to 1–45 Hz and thereafter downsampled to 125 Hz, as\\nhigher bandwidth and sampling rates did not signiﬁcantly\\nimprove the performance while increasing computational\\ntime. We used the same basic preprocessing and scaling\\napproaches for all three experiments. Each MEG epoch was\\nscaled independently by subtracting the mean and dividing\\nby the standard deviation of the MEG signal in all channels\\nduring the pre-stimulus interval (the –300 ... 0-ms interval\\nwith the zero time corresponding to the stimulus onset).\\nThis approach provided reasonable scaling suitable for a\\nreal-time experiment while preserving the spatial structure\\nof the signal.\\nThe MEG dataset of Experiment 1 was preprocessed us-\\ning a state-of-the-art oﬀ-line pipeline that is not applicable\\nin a real-time BCI setting. External magnetic interference\\nwas suppressed and head movements compensated for us-\\ning the temporally-extended signal-space separation (tSSS)\\nmethod implemented in the MaxFilter software (version\\n2.2; MEGIN / Elekta Oy, Helsinki, Finland) (Taulu and\\nSimola, 2006). Thereafter, cardiac and ocular artifacts were\\nprojected out using the FastICA algorithm as implemented\\nin the MNE-Python software (Gramfort et al., 2013b)\\nPreprocessing the datasets from Experiments 2 and 3\\nwas performed exclusively using methods available for real-\\ntime processing. The rationale for diﬀerent preprocessing\\napproaches was to progress from an optimal MEG pipeline\\n(Experiment 1) to a reduced one, comprising only those\\nmethods that are available in a real-time setting (Experi-\\nment 2), and ﬁnally to demonstrate a real BCI experiment\\n(Experiment 3). Thus, no external magnetic interference or\\nother artifact suppression was performed in Experiments\\n2 and 3, and the cardiac and oculomotor artifacts, typical\\nfor MEG measurements were present in these data.\\n3.1. Experiment 1: Classiﬁcation of 5 types of sensory\\nevent-related ﬁelds\\nDataset 1 comprised single-trial event-related-ﬁeld (ERF)\\nresponses in 7 healthy human subjects (mean age =30.1,\\n4 males, 3 females, 1 left-handed) to 5 types of sensory\\nstimuli; checkerboard patterns presented in the right or left\\nvisual hemiﬁeld (Classes 1 and 2), 1-kHz 50-ms auditory\\ntones presented to the left or right ear (pooled into Class\\n3), and transient transcutaneous electrical stimulation of\\nthe median nerve at the left or right wrist (Classes 4 and\\n5). This dataset comprised 500-ms segments of MEG mea-\\nsurements sampled at 1000 Hz (500 samples measured by\\n204 MEG channels) starting at the onset of each stimulus.\\nTotal trial counts per subject were 1622 ± 322 (mean ±\\nSD).\\n3.2. Experiment 2: Classiﬁcation of event-related oscilla-\\ntory activity in a 3-class motor imagery task\\nDataset 2 comprised MEG measurements of 17 healthy\\nhuman subjects who performed a motor imagery task (for\\ndetails, see Halme and Parkkonen (2018)), in which they\\nimagined moving either their left or right hand (without any\\nactual movement) when a visual cue was presented. The\\ndata comprised 1500-ms segments of MEG measurements\\nsampled at 1000 Hz (1500 samples measured by 204 MEG\\nchannels) starting at the onset of the visual cue. Based on\\nthe measured MEG signal, we decoded whether the subject\\nimagined moving his/her left or right hand, or nothing at\\nall (rest condition). Total trial counts per subject were\\n120 ± 5 (mean ± SD).\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 6}, page_content='3.3. Experiment 3: Real-time motor imagery BCI\\nHere we applied the same experimental paradigm as in\\nExperiment 2, but with true real-time decoding and updat-\\ning of the model. For this experiment, a network trained\\nusing a subset of trials from 17 subjects (Experiment 2) in-\\ncluding only two classes (left vs. right motor imagery) was\\nintegrated into a real-time motor imagery BCI. Two sub-\\njects performed a task where they had to imagine moving\\nthe left or right hand following the presentation of a visual\\ncue (an arrow pointing to the left or to the right). The\\nVAR-CNN model which showed the highest performance\\nin Experiment 2 performed 2-class classiﬁcation (left vs.\\nright hand motor imagery) in real time.\\nNone of the subjects had used motor imagery-based\\nBCIs before or were involved in the collection of the MEG\\ndata used to train the model, i.e in Experiment 2. The\\nexperiment comprised three sessions. In the ﬁrst session,\\nthe classiﬁer was applied without the online updates to\\nestimate the baseline performance. In sessions 2 and 3, the\\nmodel parameters were updated following each trial where\\nthe classiﬁer decoded the subject’s intention correctly. The\\nupdate was performed with a single back-propagation step\\nusing the MEG data from this trial and the associated la-\\nbel. Since our subjects had no prior experience with motor\\nimagery tasks we chose to avoid updating the classiﬁer after\\ntrials where the misclassiﬁcation was due to the subject\\nstruggling to perform the mental task (as opposed to clas-\\nsiﬁer errors). Updating the decoder with these erroneous\\ntrials would likely yield decreased decoding accuracy in\\nsubsequent trials. For the same reason we constrained this\\nexperiment to have only two classes. Trials in each session\\nwere presented in three diﬀerent pre-deﬁned sequences (50\\ntrials per session) such that the true labels were available\\nfor real-time accuracy estimation and incremental model\\noptimization.\\n4. Results\\n4.1. Experiment 1\\nIn this 5-class decoding of sensory ERFs, VAR-CNN\\noutperformed other models in terms of accuracy on a pooled\\nvalidation set (95.8% ± 0.7%), when applied to held-out\\nsubjects (85.9% ± 7.4%), and in pseudo-real time tests\\n(94.2%± 2.8%). The results from LF-CNN were the second\\nbest, and RBF-SVM was the closest benchmark. Detailed\\nstatistical comparisons between the performance of LF-\\nCNN, VAR-CNN and the benchmark classiﬁers in Experi-\\nment 1 are summarized in Supplementary Table 2.\\nIn the simulated real-time experiments, the CNN-based\\nmodels EEGNet-8, LF-CNN and VAR-CNN signiﬁcantly\\nimproved their performance (+12.4,+10.2, and +8.3 per-\\ncent points, respectively compared to the initial test accu-\\nracies. In case of SVM classiﬁers, this improvement was\\nconsiderably smaller (+6.8 for Linear SVM, and +1.2 per-\\ncent points for RBF-SVM). These results are summarized\\nin Table 2.\\n4.2. Experiment 2\\nWith the 3-class motor imagery dataset, VAR-CNN\\nagain gave the highest classiﬁcation accuracies when ap-\\nplied to pooled validation set (86.7% ± 7.4%), held-out sub-\\njects (76.3% ± 6.8%) and pseudo real-time tests (82.3% ±\\n6.1%). The closest bechmarks classiﬁers were RBF-SVM\\nfor held-out subjects (74.1% ± 8.4%), and EEGnet-8 for\\nthe validation set (80.8%±2.4%) and pseudo real-time test\\n(80.9% ± 6.7%). Detailed statistical comparisons between\\nthe performance of LF-CNN, VAR-CNN and the bench-\\nmark classiﬁers in Experiment 2 are summarized in Supple-\\nmentary Table 3. Similarly to Experiment 1, EEGNet-8,\\nLF-CNN and VAR-CNN were able to signiﬁcantly improve\\ntheir performance in a simulated real-time test using in-\\ncremental updates (+8.9,+3.8, and +5.7 percent points,\\nrespectively) compared to the initial test accuracies. The\\nlinear SVM classiﬁer performance improved to a smaller\\nextent through pseudo-real-time updates (+3.2 percent\\npoints), while the performance of RBF-SVM did not im-\\nprove at all (−0.5 percent points).\\n4.3. Experiment 3: Real-time motor imagery BCI\\nResults of the real-time application of VAR-CNN are\\nsummarized in Table 4. Comparing the accuracies achieved\\nby VAR-CNN with and without stochastic updates clearly\\nshows the signiﬁcant increase in performance. We note\\nthat since the subjects had no prior experience with motor\\nimagery BCIs, the improvement in performance after Ses-\\nsion 1 may be partly attributed to their improved motor\\nimagery skills.\\n4.4. Interpretation of learned model parameters\\nFigure 2 shows the activation patterns and the cor-\\nresponding informative time-windows of the components\\nwith the maximum contribution to the decoding of each\\nclass in the LF-CNN model, trained on the pooled data\\nfrom Experiment 1 and updated using the pseudo-real time\\nupdate procedure described above on single held-out sub-\\nject. For all of the ﬁve classes, our model extracted spatial\\npatterns whose source estimates showed overall good corre-\\nspondence to the locations and lateralizations of the peaks\\nof the evoked responses (Figure 2).\\nSimilarly, activation patterns extracted from the data\\nof Experiment 2 following a similar training and updating\\nprocedure resulted in spatial patterns focused over parietal\\nas well as the more posterior occipital sensors contralateral\\nto the imagined movement. Interestingly, spectral esti-\\nmates of the most informative latent components resulted\\nin density estimates peaking at around 10 Hz correspond-\\ning to well-known µ-rhythm desynchronization associated\\nwith motor imagery (Figure 4). Although anatomical in-\\nformation was not available for this dataset these results\\nsuggest that apart from motor and pre-motor cortices more\\nposterior sources might also contribute to motor imagery.\\nWe also estimated spatial properties of the latent com-\\nponents that had the least overall contribution to any of the\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 7}, page_content='Table 2: Across-subject performance in a 5-class sensory stimulation task. Grand-average accuracy scores (mean ± SD) from leave-one-subject-\\nout cross-validation. Highest-performing model in each test is indicated in bold.\\nModel\\nValidation (%)\\nInitial test (%)\\nPseudo-real-time (%)\\nLF-CNN\\n95.0 ± 0.8\\n83.1 ± 8.4\\n93.3 ± 3.6\\nVAR-CNN\\n95.8 ± 0.7\\n85.9 ± 7.4\\n94.2 ± 2.8\\nLinear SVM\\n93.3 ± 1.2\\n80.2 ± 9.7\\n87.0 ± 5.4\\nRBF-SVM\\n93.6 ± 1.7\\n82.7 ± 8.3\\n83.9 ± 8.4\\nShallowFBCSP-CNN\\n85.3 ± 2.4\\n60.1 ± 11.7\\nn.a.\\nEEGNet-8\\n88.7 ± 2.0\\n76.8 ± 11.7\\n89.2 ± 5.0\\nVGG19\\n80.5 ± 3.3\\n70.1 ± 12.8\\n73.9 ± 10.5\\nTable 3: Across-subject performance in a 3-class motor imagery task. Grand-average accuracy scores (mean ± SD) from leave-one-subject-out\\ncross-validation. Highest-performing model in each test is indicated in bold.\\nModel\\nValidation (%)\\nInitial test (%)\\nPseudo-real-time (%)\\nLF-CNN\\n84.3 ± 2.7\\n74.2 ± 6.5\\n78.0 ± 6.5\\nVAR-CNN\\n86.7 ± 7.4\\n76.3 ± 6.8\\n82.3 ± 6.1\\nLinear SVM\\n76.9 ± 3.0\\n68.2 ± 7.2\\n71.4 ± 7.3\\nRBF-SVM\\n80.3 ± 2.6\\n74.1 ± 8.4\\n73.6 ± 8, 8\\nShallowFBCSP-CNN\\n70.2 ± 4.1\\n60.2 ± 10.3\\nn.a.\\nEEGNet-8\\n80.8 ± 2.4\\n72.1 ± 5.8\\n80.9 ± 6.7\\nVGG19\\n71.4 ± 9.6\\n60.2 ± 6.8\\n57.4 ± 9.2\\nTable 4: VAR-CNN classiﬁcation accuracy in the real-time motor imagery BCI experiment.\\nSubject\\nRun 1, no updates (%)\\nRun 2, online updates (%)\\nRun 3, online updates (%)\\ns01\\n80.0\\n88.0\\n92.0\\ns02\\n62.0\\n90.0\\n82.0\\nclasses. Inspecting the weights of the output (classiﬁcation)\\nlayer (Figure 3) further allowed us to identify 5 components\\nwhich provided minimum contribution to either class. Al-\\nthough none of these components directly corresponded to\\nknown signatures of e.g. oculomotor artifacts, their overall\\nlimited contribution to either class may suggest that these\\npatterns were used for out-projecting irrelevant activity.\\n5. Discussion\\nIn this paper, we report two neural network models\\noptimized for across-subject classiﬁcation of electromag-\\nnetic brain signals. These models outperform traditional\\napproaches as well as more complex deep neural networks\\nin terms of accuracy, across-subject generalization, and sim-\\nulated real-time performance. Arising from inter-individual\\nvariability in structural and functional cortical anatomy,\\nacross-subject generalization has proved to be a challeng-\\ning problem for machine-learning methods applied to EEG\\nand particularly to MEG data due to its higher spatial\\nresolution. To illustrate the severity of this problem, we\\nestimated the correlations between the spatial patterns\\nextracted from our model ﬁne-tuned to individual subjects.\\nMean spatial correlation coeﬃcient between the most con-\\nsistent components was r = 0.67 in Experiment 1 (N = 7)\\nand r = 0.52 in Experiment 2 (N = 17). One advantage of\\nour models is that they introduce reasonable assumptions\\nbased on the signal generation model. These assumptions\\ninclude e.g. linear separability in spatial domain, consis-\\ntency of temporal or spectral properties of the signal across\\ntrials etc.\\nThus, we would expect it to be most sensi-\\ntive to features common across subjects while allowing for\\nreasonable variability.\\nWe further show that the relatively low complexity\\nof our LF-CNN architecture allows interpretation of the\\nlearned model parameters in terms of the underlying neural\\nactivity. Such interpretation can prove to be a convenient\\ntool for quickly exploring complex, high-dimensional MEG\\nand EEG datasets and ultimately allow extracting more\\ninformation from these rich data. Interpretation of discrim-\\ninative models can provide valuable insights into these data\\nby e.g. allowing to dissociate \"most active\" sources from\\nthose that contribute the most to discrimination. One po-\\ntential application of these methods could be investigating\\nneural sources contributing to BCI control. This argument,\\nhowever, holds only for LF-CNN, because the more complex\\nVAR-CNN variant allows to capture non-linear interactions\\nbetween the latent sources and thus cannot be interpreted\\nin a straightforward way.\\nMoreover, we propose a procedure where the classiﬁer is\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 8}, page_content='Figure 2: Interpretation of informative LF-CNN model parameters in a representative subject from Experiment 1. A. Components having the\\nmaximum contribution to the decoding of each class were extracted from the model and interpreted in terms of their spatial topographies\\n(top), source estimates (middle) and latency estimates (bottom). B. Spatial topographies(top), source estimates (middle) and peak latencies\\n(bottom) of the corresponding evoked responses. Source estimates visualization is thresholded to 95% of the peak source activity.\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 9}, page_content='Figure 3: Identiﬁcation of informative latent components in a single representative subject from Experiment 1. The weights of the contributions\\nof the ﬁnal layer to each class are represented as a raster plot (top) with rows corresponding to index of latent component and columns\\ncorresponding to (pooled) time points. Informative components (middle) are found by identifying single features (indicated by red boxes) with\\nmaximum positive weight for each class. Non-informative components are deﬁned as having minimal absolute sum of weights across all classes.\\nComponent topographies are scaled to interval [0,1] for comparability.\\nFigure 4: Interpretation of informative LF-CNN model parameters\\nin a representative subject from Experiment 2. Latent components\\nhaving the maximum positive (red) and negative (blue) sum of weights\\nover all time points for each class were extracted from the model\\nand interpreted in terms of their spatial topographies, and spectral\\nestimates.\\ninitialized from the data of other subjects and then updated\\nonline during the real-time experiment. We demonstrate\\nthat using this approach our models perform accurately\\non new subjects in a real-time BCI experiment, allowing\\nnew subjects to eﬃciently use the system without separate\\ncalibration. Importantly, this procedure allows to omit a\\ndedicated BCI calibration session, facilitating the use of\\nBCIs in research and clinical settings.\\nThe results reported here show considerable improve-\\nment in performance compared to our previous results in\\nacross-subject decoding of MEG. Using state-of-the-art\\nfeature extraction methods in combination with linear clas-\\nsiﬁers Halme and Parkkonen (2018) achieved a classiﬁcation\\naccuracy of 70.6%. Our best-performing model was able\\nto classify 3 classes of stimuli on the same dataset with\\n76.3% accuracy. Furthermore, adding incremental updates\\nto the classiﬁer allowed us to achieve even higher accuracy\\n(82.3%) in a simulated real-time test. We conﬁrmed that\\nthe feasibility of the latter approach in a real-time BCI ex-\\nperiment, achieving similarly high accuracy in Experiment\\n3.\\nSeveral studies report applying deep neural networks to\\nsingle-trial classiﬁcation of non-invasive neurophysiological\\nmeasurements, typically multichannel EEG data. Most\\nsuccessful models make use of the spatiotemporal structure\\nof EEG (Bashivan et al., 2015; Hajinoroozi et al., 2016;\\nLawhern et al., 2018; Schirrmeister et al., 2017) including\\nthe real-time applications (Burget et al., 2017; Fahimi et al.,\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 10}, page_content='2019). Bashivan et al. (2015) exploit the spatiotemporal\\nand spectral structure of EEG by transforming the signals\\ninto a sequence of multidimensional spatio–spectral images\\nusing time–frequency and polar transforms to achieve sig-\\nniﬁcant performance improvement in classifying cognitive\\nload. By contrast, we use a simpler linear projection in\\nthe spatial domain. Similarly to our approach, Hajinoroozi\\net al. (2016) tested spatial ICA as a preprocessing step\\nfollowed by temporal 1-d convolution ﬁlters.\\nHowever,\\nsuch a combination did not result in a signiﬁcant improve-\\nment in model performance in that study, as compared\\nto using EEG channel data as the input. We argue that –\\nwhen performed separately from the classiﬁcation – ICA\\ndecomposition may not be optimal due to the indepen-\\ndence assumption which may not hold for real EEG and\\nMEG signals. In our design, analogous linear projection\\nin the spatial domain was obtained by back-propagation.\\nWhen trained in conjunction with the frequency ﬁlters\\nin the temporal convolution layer, such projection results\\nin a separable decomposition related to a combination\\nof Linear Discriminant Analysis (McLachlan, 1992), and\\nSpatio–Spectral Decomposition (Nikulin et al., 2011) used\\nin EEG–MEG analysis.\\nLawhern et al. (2018) introduced the EEGNet model\\nas a compact, interpretable CNN architecture that can be\\napplied to diﬀerent EEG-based BCI paradigms (including\\nboth evoked and induced responses). EEGNet has also\\nbeen shown to generalize well to held-out subjects. Impor-\\ntantly, in our simulated real-time tests, EEGNet came close\\nto the accuracies of our models, demonstrating the advan-\\ntages of neural networks in combination with stochastic\\noptimization.\\nSimilarly to other models (e.g. Schirrmeister et al.,\\n2017), Lawhern et al. (2018) applied 1-d temporal con-\\nvolutions to the raw EEG channel data. Our results also\\nindicate that models using 1-d convolutions are better\\nsuited for classiﬁcation of MEG data than the deeper im-\\nage classiﬁcation networks relying on 2-d convolutions. In\\ncontrast to previous studies, we apply spatial decomposi-\\ntion ﬁrst, followed by a temporal depth-wise (LF-CNN)\\nor spatio–temporal (VAR-CNN) convolution. The motiva-\\ntion for this is three-fold: ﬁrst, it allows eﬀective spatial\\ndecorrelation and dimensionality reduction; second, when\\nnested into the CNN architecture, it allows the network to\\nlearn and project out physiological artifacts similarly to\\nother linear projection methods; ﬁnally, it can contribute\\nto improved generalization across subjects by increasing\\nmodel robustness to inter-individual diﬀerences in spatial\\ndistributions of the informative features. The latter argu-\\nment is particularly important for MEG, since its spatial\\nresolution is considerably higher than that of EEG, and\\neven minor diﬀerences in source location or orientation\\nmay change the signal patterns at the sensors and lead to\\ndiﬀerent channels being most sensitive to the brain activity\\nof interest. This is the likely reason why the CNN models\\noptimized for EEG decoding did not perform optimally in\\nour study.\\nOur results demonstrate that with an adequate choice of\\nhyper-parameters, support vector machines may perform\\nequally well as the best-performing CNN models in oﬀ-\\nline classiﬁcation. We thus recommend using SVMs as\\nbenchmark models in future studies. Our simulated real-\\ntime tests, however, demonstrate that incremental versions\\nof SVMs do not gain as much in performance from the\\nreal-time updates as the CNN-based models do.\\nThis\\nresult probably reﬂects the fact that incremental SVMs\\nrely on kernel approximations and thus their ﬁne-tuning\\nto individual subjects is limited.\\nThus, the proposed models outperformed the bench-\\nmark methods and provided means to investigate spatial\\nand temporal patterns that contribute to discrimination\\nbetween the stimuli. Inspecting these patterns may prove\\na useful tool to obtain fast preliminary estimate of the\\nneural activity informing the classiﬁcation. Yet, the degree\\nof correspondence between the patterns informing the clas-\\nsiﬁcation and the actual neural activity is not clear and\\nrequires further systematic testing.\\nIn this study, we restrict the interpretation of the model\\nparameters to the single most-informative latent compo-\\nnent. However, it is expected that several distinct compo-\\nnents can contribute to a single class and provide valuable\\ninformation (as e.g. shown in Figure 4). We suggest two\\nheuristic approaches to identify such informative compo-\\nnents. Similarly, we limit ourselves to the interpretation\\nof the simpler variant (LF-CNN) of the proposed networks\\ndue to its low complexity and predominantly linear activa-\\ntion functions. A more comprehensive model investigation\\nwould require a more systematic approach, suggested e.g.\\nin Kindermans et al. (2018) and Alber et al. (2018).\\nFinally, we show that the proposed models outperform\\nother neural networks designed for EEG classiﬁcation when\\napplied to MEG data. Given a large number of channels\\nand a higher spatial resolution MEG datasets may beneﬁt\\nfrom these designs. It remains to be seen, however, how\\nthe proposed models can perform when applied to EEG\\ndata.\\n6. Conclusions\\nWe have introduced a Convolutional Neural Network\\nmodel optimized for oﬀ- and on-line (real-time) classiﬁca-\\ntion of MEG data. Incorporating prior knowledge about the\\nprocesses generating MEG observations allowed us to sub-\\nstantially reduce model complexity while preserving high\\naccuracy and interpretability. We show that this model suc-\\ncessfully classiﬁes evoked as well as oscillatory activity and\\ngeneralizes eﬃciently across subjects. When combined with\\nincremental real-time model updates, the time-consuming\\ncalibration sessions in MEG-based brain–computer inter-\\nfaces could be omitted provided that a suﬃcient amount\\nof training data from other subjects is available.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-02-12T02:58:39+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Adaptive neural network classifier for decoding MEG signals.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-12T02:58:39+00:00', 'trapped': '', 'modDate': 'D:20190212025839Z', 'creationDate': 'D:20190212025839Z', 'page': 11}, page_content='7. Acknowledgements\\nResearch reported here was supported by the Academy\\nof Finland, Grant/Award Number: “NeuroFeed” / 295075\\nand the European Research Council under ERC Grant\\nAgreement n. 678578. The content is solely the responsi-\\nbility of the authors and does not necessarily represent the\\noﬃcial views of the funding organizations.\\nReferences\\nReferences\\nM. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, G. Irving, M. Isard, et al. Tensorﬂow: A system for\\nlarge-scale machine learning. In OSDI, volume 16, pages 265–283,\\n2016.\\nM. Alber, S. Lapuschkin, P. Seegerer, M. Hägele, K. T. Schütt,\\nG. Montavon, W. Samek, K.-R. Müller, S. Dähne, and P.-J.\\nKindermans. iNNvestigate neural networks!, 2018. URL https:\\n//github.com/albermax/innvestigate.\\nS. Baillet. Magnetoencephalography for brain electrophysiology and\\nimaging. Nature Neuroscience, 20(3):327–339, 2017. ISSN 1097-\\n6256. doi: 10.1038/nn.4504.\\nP. Bashivan, I. Rish, M. Yeasin, and N. Codella. Learning Repre-\\nsentations from EEG with Deep Recurrent-Convolutional Neural\\nNetworks. 2015. ISSN 03610926. doi: 10.1080/03610928808829796.\\nF. Burget, L. D. J. Fiederer, D. Kuhner, M. Völker, J. Aldinger,\\nR. T. Schirrmeister, C. Do, J. Boedecker, B. Nebel, T. Ball, and\\nW. Burgard. Acting Thoughts: Towards a Mobile Robotic Service\\nAssistant for Users with Limited Communication Skills. 7 2017.\\ndoi: 10.1109/ECMR.2017.8098658. URL http://arxiv.org/abs/\\n1707.06633http://dx.doi.org/10.1109/ECMR.2017.8098658.\\nJ.-F. Cardoso. Blind signal separation: statistical principles. Proceed-\\nings of the IEEE, 86(10):2009–2025, 1998. ISSN 00189219. doi:\\n10.1109/5.720250. URL http://ieeexplore.ieee.org/document/\\n720250/.\\nA. M. Dale, A. K. Liu, B. R. Fischl, R. L. Buckner, J. W. Belliveau,\\nJ. D. Lewine, and E. Halgren. Dynamic statistical parametric\\nmapping: combining fmri and meg for high-resolution imaging of\\ncortical activity. Neuron, 26(1):55–67, 2000.\\nJ. Daunizeau and K. J. Friston. A mesostate-space model for EEG\\nand MEG. NeuroImage, 38(1):67–81, 2007. ISSN 10538119. doi:\\n10.1016/j.neuroimage.2007.06.034.\\nF. Fahimi,\\nZ. Zhang,\\nW. B. Goh,\\nT.-S. Lee,\\nK. K. Ang,\\nand C. Guan.\\nInter-subject transfer learning with an end-\\nto-end\\ndeep\\nconvolutional\\nneural\\nnetwork\\nfor\\nEEG-based\\nBCI.\\nJournal\\nof\\nNeural\\nEngineering,\\n16(2):026007,\\n4\\n2019.\\nISSN\\n1741-2560.\\ndoi:\\n10.1088/1741-2552/aaf3f6.\\nURL http://stacks.iop.org/1741-2552/16/i=2/a=026007?key=\\ncrossref.7c244fde88bbf289575afd594f61eb05.\\nA. Gramfort, M. Luessi, E. Larson, D. A. Engemann, D. Strohmeier,\\nC. Brodbeck, R. Goj, M. Jas, T. Brooks, L. Parkkonen, and\\nM. Hämäläinen. MEG and EEG data analysis with MNE-Python.\\nFrontiers in neuroscience, 7:267, 12 2013a. ISSN 1662-4548. doi:\\n10.3389/fnins.2013.00267.\\nA. Gramfort, M. Luessi, E. Larson, D. A. Engemann, D. Strohmeier,\\nC. Brodbeck, R. Goj, M. Jas, T. Brooks, L. Parkkonen, et al. Meg\\nand eeg data analysis with mne-python. Frontiers in neuroscience,\\n7:267, 2013b.\\nM. Hajinoroozi, Z. Mao, T.-P. Jung, C.-T. Lin, and Y. Huang. EEG-\\nbased prediction of driver’s cognitive performance by deep convolu-\\ntional neural network. Signal Processing: Image Communication,\\n47:549–555, 2016.\\nH.-L. Halme and L. Parkkonen. Across-subject oﬄine decoding of\\nmotor imagery from MEG and EEG.\\nScientiﬁc reports, 8(1):\\n10087, 7 2018. doi: 10.1038/s41598-018-28295-z. URL https:\\n//www.nature.com/articles/s41598-018-28295-z.\\nM. S. Hämäläinen, R. Hari, R. J. Ilmoniemi, J. Knuutila, and O. V.\\nLounasmaa. Magnetoencephalography—theory, instrumentation,\\nand applications to noninvasive studies of the working human\\nbrain. Reviews of Modern Physics, 65(2):413–505, apr 1993. ISSN\\n0034-6861. doi: 10.1103/RevModPhys.65.413.\\nS. Haufe,\\nF. Meinecke,\\nK. Görgen,\\nS. Dähne,\\nJ.-D. Haynes,\\nB. Blankertz, and F. Bießmann. On the interpretation of weight\\nvectors of linear models in multivariate neuroimaging. NeuroImage,\\n87:96–110, 2014. ISSN 10538119. doi: 10.1016/j.neuroimage.2013.\\n10.067.\\nK. He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rectiﬁers:\\nSurpassing Human-Level Performance on ImageNet Classiﬁcation.\\n2 2015. URL http://arxiv.org/abs/1502.01852.\\nP.-J. Kindermans, G. Brain, K. T. SchüttSch, M. T. Alber Berlin,\\nT. Berlin, and S. Dähne. Learning how to explain Neural Networks:\\nPaternNet and PaternAttribution, 2018. URL https://arxiv.org/\\npdf/1705.05598.pdf.\\nV. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon,\\nC. P. Hung, and B. J. Lance.\\nEEGNet:\\na compact con-\\nvolutional neural network for EEG-based brain–computer in-\\nterfaces.\\nJournal of Neural Engineering, 15(5):056013, 10\\n2018.\\nISSN\\n1741-2560.\\ndoi:\\n10.1088/1741-2552/aace8c.\\nURL http://stacks.iop.org/1741-2552/15/i=5/a=056013?key=\\ncrossref.926d8be8d11477a9c1ce0a164d5c869a.\\nG. J. McLachlan. Discriminant Analysis and Statistical Pattern\\nRecognition. Wiley Series in Probability and Statistics. John Wiley\\n& Sons, Inc., Hoboken, NJ, USA, 3 1992. ISBN 9780471725299.\\ndoi: 10.1002/0471725293.\\nV. V. Nikulin, G. Nolte, and G. Curio. A novel method for reliable\\nand fast extraction of neuronal EEG/MEG oscillations on the basis\\nof spatio-spectral decomposition. NeuroImage, 55(4):1528–1535,\\n2011.\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,\\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al.\\nScikit-learn: Machine learning in Python. Journal of Machine\\nLearning Research, 12(Oct):2825–2830, 2011.\\nR. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstet-\\nter, K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard,\\nand T. Ball. Deep learning with convolutional neural networks for\\nEEG decoding and visualization. Human brain mapping, 38(11):\\n5391–5420, 2017.\\nK. Simonyan and A. Zisserman. Very deep convolutional networks\\nfor large-scale image recognition. Technical report, 2015. URL\\nhttp://www.robots.ox.ac.uk/.\\nS. Taulu and J. Simola.\\nSpatiotemporal signal space separation\\nmethod for rejecting nearby interference in MEG measurements.\\nPhysics in Medicine & Biology, 51(7):1759, 2006.\\nV. N. Vapnik. The nature of statistical learning theory. Springer,\\n2000. ISBN 9781441931603.\\nB. U. Westner, S. S. Dalal, S. Hanslmayr, and T. Staudigl. Across-\\nsubjects classiﬁcation of stimulus modality from human MEG high\\nfrequency activity. PLoS Computational Biology, 14(3):e1005938,\\n3 2018. ISSN 15537358. doi: 10.1371/journal.pcbi.1005938.\\n12'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='arXiv:2505.18185v1  [eess.SP]  18 May 2025\\nBrainOmni: A Brain Foundation Model for\\nUnified EEG and MEG Signals\\nQinfan Xiao1,2∗, Ziyun Cui1,2∗, Chi Zhang1,2, Siqi Chen2, Wen Wu1\\nAndrew Thwaites3,4, Alexandra Woolgar3,5, Bowen Zhou1,2, Chao Zhang2,1,4†\\n1 Shanghai Artificial Intelligence Laboratory, China\\n2 Department of Electronic Engineering, Tsinghua University, China\\n3 Department of Psychology, University of Cambridge, UK\\n4 Speech Hearing and Phonetic Sciences, University College London, UK\\n5 MRC Cognition and Brain Sciences Unit, University of Cambridge, UK.\\n{xiaoqf22, cui-zy24}@mails.tsinghua.edu.cn, cz277@tsinghua.edu.cn\\nAbstract\\nElectroencephalography (EEG) and magnetoencephalography (MEG) measure\\nneural activity non-invasively by capturing electromagnetic fields generated by\\ndendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit\\ndistinct signal patterns, further complicated by variations in sensor configurations\\nacross modalities and recording devices. Existing approaches typically rely on\\nseparate, modality- and dataset-specific models, which limits the performance\\nand cross-domain scalability. This paper proposes BrainOmni, the first brain\\nfoundation model that generalises across heterogeneous EEG and MEG recordings.\\nTo unify diverse data sources, we introduce BrainTokenizer, the first tokenizer that\\nquantises spatiotemporal brain activity into discrete representations. Central to\\nBrainTokenizer is a novel Sensor Encoder that encodes sensor properties such as\\nspatial layout, orientation, and type, enabling compatibility across devices and\\nmodalities. Building upon the discrete representations, BrainOmni learns unified\\nsemantic embeddings of brain signals by self-supervised pretraining. To the best\\nof our knowledge, it is the first foundation model to support both EEG and MEG\\nsignals, as well as the first to incorporate large-scale MEG pretraining. A total\\nof 1,997 hours of EEG and 656 hours of MEG data are curated and standardised\\nfrom publicly available sources for pretraining. Experiments show that BrainOmni\\noutperforms both existing foundation models and state-of-the-art task-specific\\nmodels on a range of downstream tasks. It also demonstrates strong generalisation\\nto unseen EEG and MEG devices. Further analysis reveals that joint EEG-MEG\\n(EMEG) training yields consistent improvements across both modalities. Code and\\nmodel checkpoints will be released upon acceptance.\\n1\\nIntroduction\\nNeuronal activity underpins human brain function. This activity generates electrical currents in the\\ncortex, which in turn produces secondary electrical and magnetic fields. These fields can be indirectly\\nmeasured using non-invasive techniques such as electroencephalography (EEG) and magnetoen-\\ncephalography (MEG) [40]. EEG measures electrical potentials through electrodes placed on the\\nscalp, while MEG uses either gradient or amplitude sensors to measure the magnetic field at specific\\n∗Equal contribution.\\n†Corresponding author.\\nPreprint. Under review.'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='locations and orientations outside the head. As rich sources of neural activity with high temporal res-\\nolutions, EEG and MEG have been widely used in applications such as motor imagery [57], emotion\\nrecognition [20], multimodal neural decoding [38, 14, 7], and clinical assessments [33, 86, 44, 67].\\nHowever, the majority of these applications are developed separately for each domain, often tailored\\nto specific tasks, datasets, or recording setups [69, 84, 33, 26, 22]. Such specialised models suffer\\nfrom limited generalisation and poor scalability across tasks and domains. Recently, EEG foundation\\nmodels have emerged to address these limitations by learning general-purpose neural representations\\nfrom large-scale data [82, 32, 76]. Although MEG signals offer significantly higher spatial resolution\\nthan EEG, foundation models for MEG remain largely unexplored, likely due to the modality’s\\ncomplexity and limited data availability. Notably, EEG and MEG share a common biophysical origin,\\nboth capturing neural activity through electromagnetic fields generated by dendritic currents. This\\nshared foundation raises a compelling question: can we develop a unified model that learns from both\\nEEG and MEG data to generalise across modalities and enhance performance in each?\\nIntegrating EEG and MEG signals into a unified foundation model faces two key challenges. First,\\nEEG and MEG exhibit distinct signal characteristics and patterns, posing a cross-modality integration\\nchallenge. Second, device heterogeneity and lack of standardisation present a significant cross-device\\ngeneralisation challenge, both within and across EEG and MEG modalities. This heterogeneity\\nincludes differences in electrode/sensor configuration (e.g., count, type, position, placement) and\\nnaming conventions – particularly pronounced in MEG. MEG sensors can differ in both type\\n(gradiometer v.s. magnetometer) and measurement directions (perpendicular v.s tangenital), which\\nadds further complexity to unified modelling.\\nThis paper proposes BrainOmni, the first foundation model for unified EEG and MEG signals. The\\ntraining of BrainOmni consists of two stages: (i) unifying heterogeneous data into the same feature\\nspace; (ii) capturing semantic features of brain activity. In the first stage, we introduce BrainTokenizer.\\nInspired by source activity estimation [1], BrainTokenizer learns to infer spatiotemporal patterns of\\nbrain activity from the observed EEG/MEG signals and generate quantised discrete tokens.\\nTo address device heterogeneity, we propose a novel Sensor Encoder that utilises each sensor’s\\nphysical characteristics, such as spatial coordinates, orientation, and type, rather than relying solely\\non channel naming conventions that are often inconsistent across devices and datasets. This design\\nenables BrainTokenizer to handle arbitrary EEG/MEG signal inputs, laying the foundation for large-\\nscale joint pretraining. By integrating EEG/MEG signals with sensor metadata, BrainTokenizer infers\\na set of latent source variables that represent the dominant generative factors underlying electroen-\\ncephalographic and magnetoencephalographic measurements across diverse sensor configurations,\\nthereby unifying heterogeneous data into a common feature space for downstream modelling.\\nBuilding on the discrete brain representations produced by BrainTokenizer, BrainOmni learns rich\\nsemantic representations of neural activity through self-supervised pretraining in the second stage.\\nTo support this, we curated and standardised a large-scale dataset comprising 1,997 hours of EEG\\nand 656 hours of MEG recordings. Experimental results show that BrainOmni: (i) outperforms\\nexisting methods across a range of downstream tasks; (ii) generalises effectively to previously unseen\\nEEG and MEG devices; and (iii) consistently benefits from joint EEG-MEG (EMEG) training across\\nmodalities. Our key contributions are as follows:\\n• Joint EMEG pretraining. To the best of our knowledge, BrainOmni is the first single\\nmodel to perform unified pretraining on both EEG and MEG signals.\\n• Modelling physical heterogeneity across devices. To address the heterogeneity of EEG\\nand MEG recording devices, we propose a novel Sensor Encoder that operates independently\\nof electrode naming conventions or fixed topologies, enabling compatibility across both\\ndevices and signal modalities.\\n• Spatiotemporal brain signal quantisation. To our knowledge, BrainTokenizer is the first\\nmodel that enables spatiotemporal quantisation of brain signals.\\n2\\nMethod\\nBrainOmni consists of two training stages. In Stage 1, BrainTokenizer is developed to discretise\\nheterogeneous EMEG signals into semantically rich representations. In Stage 2, the BrainOmni\\n2'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Reconstructor\\nEMEG Device Layout\\nRaw EMEG\\nSignal\\nReconstructed\\nEMEG Signal\\nBrainTokenizer\\nPosition \\nEmbed\\nLayer\\nType \\nEmbed \\nLayer\\nSensor Embedding\\nRMSNorm\\nEMEG Device Layout\\nMLP\\nSensor Encoder\\nSEANet\\nEncoder\\nSEANet Decoder\\nSensor\\nEncoder\\nCross Attention\\nCross Attention\\nLearnable Q\\nK\\nV\\nRVQ\\nK\\nV\\nQ\\nEMEG Device\\nLayout\\nRaw EMEG\\nSignal\\nReconstructed EMEG Signal\\nBrainTokenizer\\nReconstructor\\nReconstruction \\nLoss\\nLatent Source Variable\\nFigure 1: Illustration of the training pipeline of BrainTokenizer. Left: Overview of the autoencoder\\nscheme for BrainTokenizer training. Middle: Structure of the BrainTokenizer and Reconstructor.\\nRight: Structure of the Sensor Encoder.\\nmodel is trained based on BrainTokenizer’s outputs with a masked token prediction framework,\\nleveraging longer continuous samples to capture extended temporal dependencies.\\n2.1\\nEMEG Signals\\nEEG measures electrical potentials through electrodes placed on the scalp, while MEG uses either\\ngradiometer (GRAD) or magnetometer (MAG) sensors to measure the magnetic field at specific\\nlocations and orientations outside the head. The measured EMEG signals are multichannel time-series\\ndata, which can be represented as X ∈RC×T , where C denotes the number of sensor channels, and\\nT denotes the number of sampling points. Taking the sensor type, locations and orientations into\\naccount, an EMEG sample can be donated as Ω= (X, L, S), where X is the raw signal, L ∈RC×6\\nis the physical position and orientation information of each sensor in Cartesian coordinates, and\\nS = {s1, s2, . . . , sc|si ∈{0 (EEG), 1 (GRAD), 2 (MAG)}} is the type of each sensor.\\n2.2\\nBrainTokenizer\\nThe overall pipeline for Stage 1 is illustrated in Fig. 1. BrainTokenizer is trained using a masked\\nautoencoder scheme [15], where it quantises EMEG samples into compact discrete tokens representing\\nlatent features, and a reconstructor module recovers the original EMEG signals from these tokens.\\nStage 1 is trained using a reconstruction loss between the original and reconstructed signals, which\\nenables the BrainTokenizer to generate quantised latent representations which effectively capture\\ntemporal and spatial features of the raw EMEG signals and device layout.\\nThe BrainTokenizer consists of a SEANet encoder [72], which extracts temporal representations from\\nEMEG signals, and a novel Sensor Encoder, which encodes sensor physical information into sensor\\nembeddings. The temporal representation and the sensor embedding are then fused via a specialised\\ncross-attention block, followed by residual vector quantisation (RVQ) [89] for quantisation.\\nSEANet Encoder.\\nA SEANet encoder [72] is used to extract efficient and compact representations\\nfrom EMEG signals in the temporal dimension, which is a convolutional temporal encoder with multi-\\nlayer stacked residual 1-dimensional (-dim) convolutional block and strided convolutional layers.\\nIt encodes the raw data X to a temporal representation Ztime ∈RC×W ×D, where W represents for\\nlatent steps and D for feature dimensions.\\nSensor Encoder.\\nTo handle input from different EMEG devices, a novel Sensor Encoder is proposed\\nto support learnable sensor position encoding, which hierarchically fuses physical priors with learned\\n3'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='representations. As illustrated in Fig. 1, the Sensor Encoder contains a position embedding layer\\nto encode the 3-dim Cartesian coordinates of sensor positions and sensor orientations L ∈RC×6,\\nand a type embedding layer to encode sensor types S = {s1, s2, . . . , sc|si ∈{0, 1, 2}}. The sensor\\nphysical information L and S are then integrated to produce sensor embedding V ∈RC×D.\\nChannel Compression.\\nThe temporal representation Ztime and the sensor embedding V are then\\nfed into a specialised cross-attention block, which performs channel compression to unify signals\\nwith varying numbers of channels into a fixed number of latent source variable. The Value of the\\ncross-attention block is set to the temporal representation, Key is the sum of temporal representation\\nand sensor embedding, and a set of learnable Query is used to adaptively aggregate information from\\ntemporal and spatial representations, producing intermediate latent features Zsrc ∈RC′×W ×D, where\\nC′ is the number of latent source variables determined as a hyperparameter.\\nQuantization.\\nThe latent features Zsrc are discretised by a 4-layer RVQ module to discrete tokens\\nQ ∈RC′×W ×4, which is used to train the BrainOmni model in Stage 2.\\nReconstructor.\\nIn the reconstructor, the discrete tokens are first decoded into ˆZsrc by the RVQ\\ndecoder. Another cross-attention block is developed to convert the quantised latent source variables to\\ncontinuous channel-wise temporal representations ˆZtime, followed by a SEANet decoder to reconstruct\\nˆX, which is an inverted mirror of the encoder using transposed convolutions. It is important to note\\nthat the BrainTokenizer and the reconstructor correspond to the backward and forward solution,\\nrespectively, in traditional EEG/MEG source current activity estimation; a detailed explanation can\\nbe found in Appendix B.\\nTraining the BrainTokenizer.\\nThe BrainTokenizer is trained following an autoencoder scheme\\nwhere 25% of the input channels are randomly dropped and the reconstructor is required to recon-\\nstruct all original channels from discrete tokens. The overall training loss combines a multi-level\\nreconstruction loss with RVQ commitment losses, enabling joint optimisation of the network and the\\ncodebooks. The multi-level reconstruction loss includes (i) a time-domain loss between the original\\nwaveform X and the reconstructed waveform ˆX:\\nLtime = ||X −ˆX||,\\n(1)\\nwhere || · || denotes L1 distance; (ii) a frequency-domain loss between the original and reconstructed\\namplitude spectrum A and phase spectrum Φ:\\nLfreq = ||A −ˆA|| + ||Φ −ˆΦ||;\\n(2)\\n(iii) an auxiliary loss based on Pearson correlation coefficient (PCC) to regularise waveform trend\\nconsistency:\\nLpcc = e−PCC(X, ˆX).\\n(3)\\nThe codebooks of RVQ is updated using exponential moving average (EMA), with rotation trick\\nemployed as the gradient estimator. Donate zi and zqi as the residual and nearest entry of ith layer in\\nthe codebook, respectively, the RVQ commitment loss is defined as:\\nLrvq =\\nXNq\\ni=1 ||zi −zqi||2,\\n(4)\\nwhere Nq is the depth of codebooks. The BrainTokenizer is trained to optimise the following loss:\\nLtoken = Ltime + Lfreq + Lpcc + Lrvq.\\n(5)\\n2.3\\nBrainOmni\\nBuilt on BrainTokenizer in Stage 1, the BrainOmni model is trained with a masked token prediction\\nframework, to jointly model spatial and temporal information, thus learning coherent spatiotemporal\\nrepresentations of the brain activity. The training framework of BrainOmni is illustrated in Fig. 2.\\nThe BrainTokenizer encodes continuous EMEG recordings into a sequence of tokens with shape\\n(C′, T, Nq), where C′ is the number of the channels after compression, T is the sequence length, and\\nNq is the codebook depth, which is set to 4 in our implementation. Tokens are randomly masked at a\\n4'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='CE Loss\\n2\\n6\\n4\\n1\\n7\\n9\\n5\\n3\\n80% mask token\\n20% random replace\\nBrainTokenizer\\nBrainOmni\\nCriss-Cross Transformer\\nCriss-Cross Transformer\\n…\\n× N\\nEMEG Device\\nLayout\\nRaw EMEG\\nSignal\\nTime\\nChannel\\nFigure 2: Illustration of the training framework of BrainOmni.\\npredefined ratio and subsequently projected through an embedding layer into representations of shape\\n(C′, T, D). The embeddings are then processed by multiple layers of the Criss-Cross Transformer [76]\\nto jointly model the spatial and temporal dependencies. Finally, the output representations from\\nTransformer blocks are utilised to predict the masked tokens.\\nCriss-Cross Transformer.\\nThe BrainOmni model consists of multiple Criss-Cross Transformer\\nblocks [76]. Given the multi-channel time-series nature of EMEG data, joint modelling of spatial\\nand temporal contexts is crucial. The Criss-Cross Transformer divides the input into two halves\\nalong the feature dimension: one half is used for spatial attention computation, and the other half for\\ntemporal attention. These two halves are then concatenated and passed through a feedforward layer.\\nAdditionally, rotary position embedding (RoPE) [70] is applied specifically to temporal attention to\\nencode positional information along the time dimension.\\nTraining the BrainOmni Model.\\nDuring BrainOmni training, 50% of the positions in the (C′, T)\\ntoken grid are randomly masked. For each masked position, all four RVQ layers are masked and\\nsimultaneously predicted in a non-autoregressive manner.\\nTo prevent over-reliance on special mask tokens, 80% of the masked positions are replaced with\\ndedicated mask tokens, while the remaining 20% are substituted with randomly sampled tokens from\\nthe sequence. The model training loss is:\\nLmodel = 1\\nM\\nXM\\ni=1\\nXNq\\nj=1 Lce(qij, yij),\\n(6)\\nwhere qij is the ground-truth codebook index at jth RVQ layer of token i, yij is the corresponding\\nmodel prediction, M denotes the number of masked tokens, and Lce(·, ·) is the cross-entropy loss.\\n3\\nExperimental Setup\\n3.1\\nPretraining\\nPretraining Datasets.\\n1997 hours of EEG data and 656 hours of MEG data collected from open-\\nsource datasets are used for pretraining (see Appendix D for details), including EEG devices with\\nchannel numbers ranging from 19 to 128 and MEG devices with channel numbers ranging from 157\\nto 306. Among them, 85% of the data is used for training, 10% of the data is used for validation, and\\n5% of the data is used for testing. Additionally, one EEG dataset and one MEG dataset, which were\\nboth collected with unique device systems different from those in the training data, were excluded\\nfrom the training data to evaluate the model’s cross-device generalisation ability.\\nData Preprocessing.\\nMinimal and standardised preprocessing was applied to maximise data\\nutilisation. A 0.1Hz-96 Hz bandpass filter and 50/60 notch filters were first applied to remove slow\\ndrifts and power-line noise, and MEG recordings with HPI coils were additionally filtered in their\\nHPI frequency bands. All signals were then resampled to 256 Hz. Bad channels were identified\\n5'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Table 1: Overview of the downstream datasets. Duration in seconds.\\nModality\\nTask\\nDataset\\n# Subject\\n# Channel\\nDuration\\n# Segment\\n# Class\\nEEG\\nAlzheimer’s Disease\\nAD65 [41]\\n65\\n19\\n10.0\\n7013\\n2\\nDepression\\nMDD [42]\\n35\\n20\\n10.0\\n7302\\n2\\nParkinson’s Disease\\nPD31 [54]\\n31\\n32\\n10.0\\n882\\n2\\nAbnormal\\nTUAB [45]\\n2328\\n21\\n10.0\\n408853\\n2\\nEvent\\nTUEV [45]\\n294\\n21\\n5.0\\n112237\\n6\\nEmotion\\nFACED [9]\\n123\\n30\\n10.0\\n10332\\n3\\nMotor Imagery\\nWBCIC_SHU [81]\\n51\\n58\\n4.0\\n30591\\n2\\nMotor Imagery\\nPhysioNet-MI [56]\\n109\\n64\\n4.0\\n19571\\n4\\nMEG\\nAutism Spectrum Disorder\\nASD74 [19]\\n74\\n306\\n10.0\\n12320\\n2\\nEMEG\\nMotor Response\\nSomatoMotor [36]\\n5\\n372\\n2.0\\n1208\\n2\\nvia a power-spectral-density-based detection algorithm and subsequently interpolated. To address\\nreference inconsistencies and wide amplitude variations, reference values are first subtracted across\\nall sensors of each type within each sample. Then, each channel is normalised to zero mean and\\nunit variance at the sample level. Sensor coordinates and orientations are obtained from either the\\ndataset-provided positions or the device’s standard montage.\\n3.2\\nEvaluation\\nDownstream Datasets.\\nTen different datasets for nine separate tasks are collected to evaluate the\\nperformance of BrainOmni. Overview of datasets, including modality, tasks, number of subjects,\\nchannels, segments, categories, and duration of each segment, are listed in Table 1, and details are\\nlisted in Appendix E. To prevent information leakage, all datasets used for downstream testing were\\nexcluded from the pretraining process. Consistent with pretraining, bandpass filtering, notch filtering,\\nresampling and normalisation were applied for downstream dataset preprocessing.\\nBaselines and Downstream Settings.\\nThe proposed method is compared to four specialised\\nmodels without pretraining on both EEG and MEG data: CNN-Transformer [50], ContraWR [83]\\nSPaRCNet [34] and ST-Transformer [68]. Two additional EEG foundation models are included for\\nEEG data: LaBraM [32] and CBraMod [76]. Details of these baselines can be found in Appendix F.\\nTo ensure fair comparison, BrainOmni is compared to all baselines using the same preprocessing\\nprocedures, training pipeline and downstream evaluation strategy. The only exception is that in\\nthe preprocessing of LaBraM and CBraMod, we followed the same setting of sampling rate and\\nbandpass filter as their preprocessing. All experiments used a fixed random seed 42 and trained for\\n30 epochs to ensure reproducibility. Cross-subject 3-fold cross-validation was conducted, and the\\nmean and standard deviation of three runs were reported. The model checkpoint that achieved the\\nhighest validation-set BACC was selected for testing. Additionally, experiments were conducted with\\nlearning rates of [3 × 10−6, 1 × 10−5, 3 × 10−5] respectively for each model and chose the learning\\nrate yielding the best validation performance. Given class imbalances in some datasets, balanced\\naccuracy (BACC) was chosen as the primary evaluation metric.\\n4\\nResults\\n4.1\\nDownstream Evaluation\\nBrainOmni was evaluated on downstream EEG, MEG, and EMEG tasks. Results on EEG datasets\\nare shown in Table 2. Results on MEG and EMEG datasets are listed in Table 3. It can be observed\\nthat BrainOmni achieved the highest performance on nearly all tasks, with a close second-best place\\non the MDD dataset. Specifically, on EEG data, BrainOmni_base’s balanced accuracy on the AD65\\nand PD31 datasets was about 10% higher than the best pretrained baseline LaBraM. On the MEG\\nASD74 dataset, it exceeded the strongest baseline by about 7%, and on the EMEG SomatoMotor\\ntask, it outperformed the top baseline by 20%. The results demonstrate that BrainOmni consistently\\noutperforms both specialised and other pretrained foundation models across diverse downstream\\ntasks and modalities, including EEG, MEG, and EMEG data, exhibiting strong generalisation and\\nversatility.\\n6'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Table 2: Baseline comparison on the eight EEG datasets. Balanced accuracy is reported with mean\\n± standard deviation. “PT” stands for whether the model involves pretraining. The best results are\\nshown in bold, and the second-best results are underlined.\\n# Param\\nPT\\nAD65\\nMDD\\nPD31\\nTUAB\\nCNN-Transformer [50]\\n3.2M\\n✗\\n0.641 ± 0.100\\n0.709 ± 0.148\\n0.513 ± 0.028\\n0.809 ± 0.004\\nContraWR [83]\\n1.6M\\n✗\\n0.609 ± 0.038\\n0.706 ± 0.067\\n0.591 ± 0.011\\n0.806 ± 0.005\\nSPaRCNet [34]\\n1.3M\\n✗\\n0.563 ± 0.057\\n0.764 ± 0.048\\n0.496 ± 0.124\\n0.789 ± 0.002\\nST-Transformer [68]\\n2.6M\\n✗\\n0.663 ± 0.011\\n0.786 ± 0.001\\n0.535 ± 0.028\\n0.811 ± 0.008\\nLaBraM [32]\\n5.8M\\n✓\\n0.715 ± 0.065\\n0.843 ± 0.042\\n0.600 ± 0.109\\n0.807 ± 0.003\\nCBraMod [76]\\n4.9M\\n✓\\n0.649 ± 0.006\\n0.811 ± 0.006\\n0.617 ± 0.162\\n0.773 ± 0.001\\nBrainOmni_tiny\\n8.4M\\n✓\\n0.800 ± 0.035\\n0.834 ± 0.036\\n0.658 ± 0.062\\n0.816 ± 0.001\\nBrainOmni_base\\n33M\\n✓\\n0.806 ± 0.053\\n0.841 ± 0.044\\n0.733 ± 0.102\\n0.813 ± 0.007\\n# Param\\nPT\\nTUEV\\nFACED\\nWBCIC_SHU\\nPhysioNet-MI\\nCNN-Transformer [50]\\n3.2M\\n✗\\n0.312 ± 0.012\\n0.353 ± 0.008\\n0.618 ± 0.015\\n0.309 ± 0.009\\nContraWR [83]\\n1.6M\\n✗\\n0.342 ± 0.036\\n0.351 ± 0.007\\n0.509 ± 0.007\\n0.336 ± 0.033\\nSPaRCNet [34]\\n1.3M\\n✗\\n0.357 ± 0.032\\n0.375 ± 0.010\\n0.738 ± 0.017\\n0.367 ± 0.015\\nST-Transformer [68]\\n2.6M\\n✗\\n0.374 ± 0.023\\n0.406 ± 0.008\\n0.782 ± 0.014\\n0.367 ± 0.035\\nLaBraM [32]\\n5.8M\\n✓\\n0.596 ± 0.016\\n0.444 ± 0.015\\n0.819 ± 0.024\\n0.372 ± 0.011\\nCBraMod [76]\\n4.9M\\n✓\\n0.416 ± 0.042\\n0.430 ± 0.021\\n0.532 ± 0.005\\n0.313 ± 0.009\\nBrainOmni_tiny\\n8.4M\\n✓\\n0.597 ± 0.019\\n0.454 ± 0.012\\n0.818 ± 0.017\\n0.434 ± 0.006\\nBrainOmni_base\\n33M\\n✓\\n0.618 ± 0.034\\n0.466 ± 0.008\\n0.819 ± 0.022\\n0.439 ± 0.011\\nTable 3: Baseline comparison on the ASD74 (MEG) and SomatoMotor (EMEG). Balanced accuracy\\n(BACC) and F1 score are reported in mean ± standard deviation. “PT” stands for whether the model\\ninvolves pretraining. The best results are shown in bold and second-best results are underlined.\\n# Param\\nPT\\nASD74\\nSomatoMotor\\nBACC\\nF1\\nBACC\\nF1\\nCNN-Transformer [50]\\n3.2M\\n✗\\n0.508 ± 0.011\\n0.069 ± 0.074\\n0.508 ± 0.010\\n0.524 ± 0.066\\nContraWR [83]\\n1.6M\\n✗\\n0.533 ± 0.047\\n0.343 ± 0.246\\n0.505 ± 0.003\\n0.233 ± 0.222\\nSPaRCNet [34]\\n1.3M\\n✗\\n0.565 ± 0.039\\n0.463 ± 0.047\\n0.548 ± 0.008\\n0.543 ± 0.037\\nST-Transformer [68]\\n2.6M\\n✗\\n0.532 ± 0.024\\n0.298 ± 0.122\\n0.624 ± 0.063\\n0.646 ± 0.094\\nBrainOmni_tiny\\n8.4M\\n✓\\n0.606 ± 0.019\\n0.526 ± 0.024\\n0.787 ± 0.112\\n0.794 ± 0.105\\nBrainOmni_base\\n33M\\n✓\\n0.624 ± 0.024\\n0.561 ± 0.047\\n0.801 ± 0.128\\n0.788 ± 0.150\\n4.2\\nCross Device Generalisation\\nTo evaluate BrainTokenizer’s generalisation to entirely unseen EEG and MEG device systems,\\nwe selected two datasets, PerceiveImagine (EEG, SynAmps2 system) and Gloups-MEG (MEG,\\nNeuroImaging system), whose recording setups were not included in any pretraining data. We\\ncompare the zero-shot reconstruction losses of these unseen datasets against those of the known-\\ndevice EEG and MEG test set whose training partitions have been used in training BrainTokenizer.\\nResults are listed in Table 4. For EEG, the zero-shot results on PerceiveImagine even consistently\\noutperform our EEG test set across all metrics, highlighting strong generalisation to a new EEG\\ndevice system. For MEG, although the reconstruction losses of BrainTokenizer on Gloups-MEG\\nare slightly worse than those of our own MEG test set, the relatively strong PCC value of 0.695 still\\nindicates the model’s competitive generalisation capability for MEG devices. The slight performance\\ndegradation on MEG devices may be attributed to the relatively limited amount of MEG training data.\\n4.3\\nEMEG Joint Pretraining\\nTo investigate the effect of joint EEG and MEG training on model performance, two additional\\nBrainOmni variants were trained using only EEG or only MEG data. The variants are compared to\\nthe BrainOmni model on EEG datasets TUAB, TUEV, MEG dataset ASD74, and the multimodal\\nSomatoMotor dataset. Specifically for the SomatoMotor datasets, downstream experiments were\\nconducted with EEG-only, MEG-only, and combined EMEG inputs separately. As shown in Table 5,\\njoint EMEG pretraining consistently outperforms single-modality pretraining across all datasets.\\nNotably, for the MEG modality of the SomatoMotor dataset, the jointly pretrained EMEG model\\n7'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Table 4: Zero-shot reconstruction results of BrainTokenizer on unseen devices. “AMP” stands for\\namptitude MAE. PHASE stands for phase MAE. “↑” indicates the higher the better. “↓” indicates\\nthat the lower the better.\\nModality\\nData\\nSeen Device\\nMSE ↓\\nMAE ↓\\nAMP ↓\\nPHASE ↓\\nPCC ↑\\nEEG\\nour EEG Test Set\\n✓\\n0.404\\n0.449\\n0.142\\n1.506\\n0.748\\nPerceiveImagine [6]\\n✗\\n0.343\\n0.415\\n0.137\\n1.457\\n0.802\\nMEG\\nour MEG Test Set\\n✓\\n0.473\\n0.522\\n0.220\\n1.469\\n0.711\\nGloups-MEG [74]\\n✗\\n0.567\\n0.572\\n0.206\\n1.517\\n0.695\\nTable 5: Comparision of BrainOmni models pretrained using EEG-only, MEG-only, and joint EMEG\\ndata. Tiny version is used for all models.\\nDataset\\nTUAB\\nTUEV\\nASD74\\nSomatoMotor\\nModality\\nEEG\\nEEG\\nMEG\\nEEG\\nMEG\\nEMEG\\nEEG only\\n0.807 ± 0.006\\n0.555 ± 0.003\\n–\\n0.737 ± 0.042\\n–\\n–\\nMEG only\\n–\\n–\\n0.530 ± 0.032\\n–\\n0.683 ± 0.168\\n–\\nEMEG\\n0.816 ± 0.001\\n0.596 ± 0.019\\n0.606 ± 0.019\\n0.747 ± 0.088\\n0.786 ± 0.121\\n0.787 ± 0.112\\nimproves balanced accuracy by 10% compared with the MEG-only model. The results highlight the\\nbenefit of multimodal training, particularly for the MEG modality with less accessible pretraining\\ndata. Additionally, comparing the EEG, MEG, and EMEG inputs processed by the EMEG model\\non SomatoMotor, the combined EMEG input achieves better performance than both EEG and MEG\\ninputs, underscoring the effectiveness of jointly leveraging EEG and MEG data in downstream tasks.\\n4.4\\nEffectiveness of Sensor Encoder\\nAn ablation study is conducted to assess the effectiveness of the proposed Sensor Encoder in modelling\\nthe spatial characteristics of sensors. Apart from directly removing the sensor embedding, we also\\ndevelop a pure temporal version of BrainTokenizer where multi-channel EMEG data is treated as\\nuncorrelated single-channel data. Results shown in Table 6, it can be observed that the standard\\nBrainOmni consistently outperforms the pure temporal model. And the exclusion of sensor embedding\\nsignificantly undermines the downstream performance, which demonstrates the effectiveness of the\\nproposed Sensor Encoder.\\n5\\nAnalysis\\n5.1\\nPretraining Stability and Codebook Contribution\\nTo analyse the stability when incorporating diverse devices and signals into pretraining, we plot the\\ntraining curves in Fig. 3a. Overall, BrainTokenizer converges relatively quickly, and the validation\\nloss remains consistent with the training loss. This demonstrates that the proposed BrainTokenizer\\ntraining framework generalises well across different devices and signal types. Fig. 3b plots the mask\\nprediction performance for each RVQ layer. It can be seen that the first codebook aggregates richer\\nsemantic information and achieves higher accuracy in mask prediction, while the prediction accuracy\\ndecreases progressively for the subsequent lower-level codebooks.\\n5.2\\nNumber of Latent Source Variables\\nTo investigate the effect of the number of latent source activities, we trained variants of BrainTokenizer\\nwith different numbers of source activities. Results are shown in Fig. 3c. As the number of source\\nactivities increases, the EMEG signal reconstruction loss gradually decreases. When the number of\\nTable 6: Ablation study of sensor embedding (SE). Tiny version used. Balanced accuracy reported.\\nModel\\nTUAB\\nTUEV\\nAD65\\nASD74\\nSomatoMotor\\nBrainOmni\\n0.816 ± 0.001\\n0.597 ± 0.019\\n0.800 ± 0.035\\n0.606 ± 0.019\\n0.787 ± 0.112\\n–w/o SE\\n0.785 ± 0.007\\n0.604 ± 0.018\\n0.730 ± 0.056\\n0.549 ± 0.007\\n0.643 ± 0.066\\nPure temporal\\n0.786 ± 0.003\\n0.538 ± 0.010\\n0.774 ± 0.060\\n0.575 ± 0.031\\n0.740 ± 0.099\\n8'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='0\\n5\\n10\\n15\\nEpochs\\n1.8\\n2.0\\n2.2\\n2.4\\nLoss\\nTrain Loss\\nEval Loss\\n(a) Learning curve\\n0\\n10\\n20\\n30\\nEpochs\\n0.0\\n0.2\\n0.4\\nAccuracy\\nLayer 0\\nLayer 1\\nLayer 2\\nLayer 3\\n(b) Different codebook layers\\n1\\n2\\n4\\n8\\n16\\n32\\n64\\nNumber of latent source variables\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nLoss\\n(c) Latent source variables\\nFigure 3: (a) The training loss and validation loss curves during the training phase of BrainTokenizer.\\n(b) The accuracy curves for parallel mask prediction on the labels of each codebook layer during\\nthe training phase of BrainOmni on the training set. (c) Trend of signal reconstruction loss over the\\nnumber of latent source variables.\\nRaw Wave\\nRaw Topomap\\nRec Wave\\nRec Topomap\\n(a) Device seen in pretraining.\\nRaw Wave\\nRaw Topomap\\nRec Wave\\nRec Topomap\\n(b) EEG new device.\\nRaw Wave\\nRaw Topomap\\nRec Wave\\nRec Topomap\\n(c) MEG new device.\\nFigure 4: The waveforms and topographies of the reconstructed and original EEG signals: (a) the\\nstandard 10-20 system that was seen during the pre-training phase; (b) the Synnaps system not seen\\nduring the pretraining phase; (c) the NeuroImaging system not seen during the pretraining phase.\\nsource variables reaches beyond 16, the trend of loss reduction weakened with further increases in the\\nnumber of latent source activities. To balance the amount of information retained and computational\\nefficiency, we chose 16 as the number of latent source variables.\\n5.3\\nVisualisation of Reconstruction Results\\nFig. 4 illustrates the reconstruction performance of BrainTokenizer. We compare the waveforms\\nand topographic maps before and after reconstruction to highlight the model’s ability to capture the\\nspatiotemporal electromagnetic fields of EEG and MEG in both temporal and spatial domains. The\\nresults show that the model effectively preserves the main trends and finer details of the waveforms,\\nwhile smoothing out high-frequency noise spikes. In terms of spatial representation, the reconstructed\\ntopographic maps maintain the original activation patterns and structural integrity. Notably, the\\nmodel also achieves strong reconstruction performance on previously unseen devices and samples,\\ndemonstrating robust generalisation across different recording setups.\\n6\\nConclusion\\nIn this paper, we introduce BrainOmni, the first brain foundation model that generalises across\\nheterogeneous EEG and MEG signals. To model signals from different devices and modalities, we\\npropose BrainTokenizer, which infers spatiotemporal patterns of brain activity from the observed\\nEMEG signals and generates quantised discrete tokens. To address the inherent heterogeneity arising\\nfrom various recording devices, we develop a flexible Sensor Encoder that leverages physical sensor\\nproperties. Through large-scale self-supervised joint pretraining on EEG and MEG data, BrainOmni\\nsignificantly exceeds the performance of existing foundation models and state-of-the-art task-specific\\nbaselines across various downstream tasks, and demonstrates effective generalisation capability to\\nunseen devices and modalities. Furthermore, extensive ablation analysis highlights the consistent\\nbenefits of joint EMEG pretraining, underscoring the advantage of unified modelling strategies. We\\nbelieve BrainOmni represents an important step towards building versatile and scalable foundation\\nmodels for neural recordings, opening new pathways for unified brain signal representation learning.\\n9'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='References\\n[1] Seppo P. Ahlfors and Matti S. Hämäläinen. Handbook of Neural Activity Measurement, chapter\\nMEG and EEG: Source estimation, page 257–286. Cambridge University Press, 2012.\\n[2] Kristijan Armeni, Umut Güçlü, Marcel van Gerven, and Jan-Mathijs Schoffelen. A 10-hour\\nwithin-participant magnetoencephalography narrative dataset to test models of language com-\\nprehension. Scientific Data, 9(1):278, 2022.\\n[3] Ümit Aydin, Johannes Vorwerk, Matthias Dümpelmann, Philipp Küpper, Harald Kugel, Marcel\\nHeers, Jörg Wellmer, Christoph Kellinghaus, Jens Haueisen, Stefan Rampp, et al. Combined\\nEEG/MEG can outperform single modality EEG or MEG source reconstruction in presurgical\\nepilepsy diagnosis. PloS one, 10(3):e0118753, 2015.\\n[4] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2Vec 2.0: A\\nframework for self-supervised learning of speech representations. In Proc. NeurIPS, 2020.\\n[5] Sylvain Baillet, John C Mosher, and Richard M Leahy. Electromagnetic brain mapping. IEEE\\nSignal Processing Magazine, 18(6):14–30, 2001.\\n[6] Imad J. Bajwa1, Andre S. Nilsen1, 3 René Skukies1, Arnfinn Aamodt1, Gernot Ernst2,\\nJohan F. Storm1, and 2 Bjørn E. Juel1.\\n\"a repeated awakening study exploring\\nthe capacity of complexity measures to capture dreaming during propofol sedation\".\\ndoi:10.18112/openneuro.ds005620.v1.0.0, 2024.\\n[7] Yohann Benchetrit, Hubert Banville, and Jean-Remi King. Brain decoding: Toward real-time\\nreconstruction of visual perception. In Proc. ICLR, 2024.\\n[8] Geeling Chau, Christopher Wang, Sabera J Talukder, Vighnesh Subramaniam, Saraswati Soedar-\\nmadji, Yisong Yue, Boris Katz, and Andrei Barbu. Population Transformer: Learning population-\\nlevel representations of intracranial activity. In Proc. ICLR, 2025.\\n[9] Jingjing Chen, Xiaobin Wang, Chen Huang, Xin Hu, Xinke Shen, and Dan Zhang. A large\\nfiner-grained affective computing EEG dataset. Scientific Data, 10(1):740, 2023.\\n[10] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li,\\nNaoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. WavLM: Large-scale self-supervised pre-\\ntraining for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing,\\n16(6):1505–1518, 2022.\\n[11] Dorottya Cserpan, Ece Boran, Richard Rosch, San Pietro Lo Biundo, Georgia Ramantani, and\\nJohannes Sarnthein. \"dataset of EEG recordings of pediatric patients with epilepsy based on the\\n10-20 system \". 10.18112/openneuro.ds003555.v1.0.1, 2021.\\n[12] Anders M Dale and Martin I Sereno. Improved localizadon of cortical activity by combining\\nEEG and MEG with MRI cortical surface reconstruction: A linear approach. Journal of\\ncognitive neuroscience, 5(2):162–176, 1993.\\n[13] Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo\\nMiranda, and Slawomir J. Nasuto. \"an EEG dataset recorded during affective music listening\".\\ndoi:10.18112/openneuro.ds002721.v1.0.3, 2024.\\n[14] Alexandre Défossez, Charlotte Caucheteux, Jérémy Rapin, Ori Kabeli, and Jean-Rémi King.\\nDecoding speech perception from non-invasive brain recordings. Nature Machine Intelligence,\\n5(10):1097–1107, 2023.\\n[15] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio\\ncompression. Transactions on Machine Learning Research, 2023.\\n[16] Arnaud Delorme.\\n\"go-nogo categorization and detection task\".\\n10.18112/open-\\nneuro.ds002680.v1.2.0, 2021.\\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional Transformers for language understanding. In Proc. ACL, 2019.\\n10'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='[18] Vasileios S Dimakopoulos, Marios Antonakakis, Gabriel Moeddel, Jörg Wellmer, Stefan Rampp,\\nMichalis Zervakis, and Carsten H Wolters. Combined EEG/MEG source reconstruction of\\nepileptic activity using a two-phase spike clustering approach. In Proc. BIBE, 2019.\\n[19] Kirill A. Fadeev, Ilacai V. Romero Reyes, Dzerassa D. Goiaeva, Tatyana S. Obukhova,\\nTatyana M. Ovsiannikova, Andrey O. Prokofyev, Anna M. Rytikova, Artyom Y. Novikov,\\nVladimir V. Kozunov, Tatyana A. Stroganova, and Elena V. Orekhova. \"perception of vowel\\nsounds in children with autism spectrum disorders and typically developing children (MEG/ERF\\nstudy)\". doi:10.18112/openneuro.ds005234.v2.1.7, 2024.\\n[20] Sofien Gannouni, Arwa Aledaily, Kais Belwafi, and Hatim Aboalsamh. Emotion detection\\nusing electroencephalography signals and a zero-time windowing-based epoch estimation and\\nrelevant electrode identification. Scientific Reports, 11(1):7071, 2021.\\n[21] Phoebe Gaston, Christian Brodbeck, Colin Phillips, and Ellen Lau. \"auditory single word\\nrecognition in MEG\". doi:10.18112/openneuro.ds004276.v1.0.0, 2022.\\n[22] Antonio Giovannetti, Gianluca Susi, Paola Casti, Arianna Mencattini, Sandra Pusil, María Eu-\\ngenia López, Corrado Di Natale, and Eugenio Martinelli. Deep-MEG: spatiotemporal CNN\\nfeatures and multiband ensemble classification for predicting the early signs of Alzheimer’s dis-\\nease with magnetoencephalography. Neural Computing and Applications, 33(21):14651–14667,\\n2021.\\n[23] Tijl Grootswagers, Amanda Robinson, Sofia Shatek, and Thomas Carlson. \"features-EEG\".\\ndoi:10.18112/openneuro.ds004357.v1.0.1, 2024.\\n[24] Laura Gwilliams, Graham Flick, Alec Marantz, Liina Pylkkänen, David Poeppel, and Jean-Rémi\\nKing. Introducing MEG-MASC a high-quality magneto-encephalography dataset for evaluating\\nnatural speech processing. Scientific data, 10(1):862, 2023.\\n[25] Matti S Hämäläinen and Risto J Ilmoniemi. Interpreting magnetic fields of the brain: Minimum\\nnorm estimates. Medical & biological engineering & computing, 32:35–42, 1994.\\n[26] Ahmad Hasasneh, Nikolas Kampel, Praveen Sripad, N Jon Shah, and Jürgen Dammers. Deep\\nlearning approach for automatic classification of ocular and cardiac artifacts in MEG data.\\nJournal of Engineering, 2018(1):1350692, 2018.\\n[27] Christoffer Hatlestad-Hall, Trine Waage Rygvold, and Stein Andersson. \"SRM resting-state\\nEEG\". doi:10.18112/openneuro.ds003775.v1.2.1, 2022.\\n[28] Olaf Hauk. Keep it simple: a case for using classical minimum norm estimation in the analysis\\nof EEG and MEG data. NeuroImage, 21(4):1612–1621, 2004.\\n[29] Olaf Hauk, Matti Stenroos, and Matthias S Treder.\\nTowards an objective evaluation of\\nEEG/MEG source estimation methods–The linear approach. Neuroimage, 255:119177, 2022.\\n[30] Martin N. Hebart, Oliver Contier, Lina Teichmann, Adam H. Rockter, Charles Zheng, Alexis\\nKidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris I. Baker.\\n\"things-MEG\".\\ndoi:10.18112/openneuro.ds004212.v2.0.1, 2024.\\n[31] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\\nand Abdelrahman Mohamed. HuBERT: Self-supervised speech representation learning by\\nmasked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language\\nprocessing, 29:3451–3460, 2021.\\n[32] Weibang Jiang, Liming Zhao, and Bao-liang Lu. Large brain model for learning generic\\nrepresentations with tremendous EEG data in BCI. In Proc. ICLR, 2024.\\n[33] Jin Jing, Wendong Ge, Shenda Hong, Marta Bento Fernandes, Zhen Lin, Chaoqi Yang, Sungtae\\nAn, Aaron F Struck, Aline Herlopian, Ioannis Karakis, et al. Development of expert-level clas-\\nsification of seizures and rhythmic and periodic patterns during EEG interpretation. Neurology,\\n100(17):e1750–e1762, 2023.\\n11'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='[34] Jin Jing, Wendong Ge, Shenda Hong, Marta Bento Fernandes, Zhen Lin, Chaoqi Yang, Sungtae\\nAn, Aaron F Struck, Aline Herlopian, Ioannis Karakis, et al. Development of expert-level\\nclassification of seizures and rhythmic and periodic patterns during eeg interpretation. Neurology,\\n100(17):e1750–e1762, 2023.\\n[35] Demetres Kostas, Stephane Aroca-Ouellette, and Frank Rudzicz. BENDR: Using transformers\\nand a contrastive self-supervised learning task to learn from massive amounts of EEG data.\\nFrontiers in Human Neuroscience, 15:653659, 2021.\\n[36] Fa-Hsuan Lin, Deirdre Foxe von Pechmann, Kaisu Lankinen, Seppo Ahlfors, Bruce\\nRosen,\\nJyrki Ahveninen,\\nMatti Hämäläinen,\\nand Tommi Raij.\\n\"somatomotor\".\\ndoi:10.18112/openneuro.ds006035.v1.0.0, 2025.\\n[37] Fa-Hsuan Lin, Thomas Witzel, Seppo P Ahlfors, Steven M Stufflebeam, John W Belliveau, and\\nMatti S Hämäläinen. Assessing and improving the spatial accuracy in MEG source localization\\nby depth-weighted minimum-norm estimates. NeuroImage, 31(1):160–171, 2006.\\n[38] Kaylo T Littlejohn, Cheol Jun Cho, Jessie R Liu, Alexander B Silva, Bohan Yu, Vanessa R\\nAnderson, Cady M Kurtz-Miott, Samantha Brosler, Anshul P Kashyap, Irina P Hallinan, et al.\\nA streaming brain-to-voice neuroprosthesis to restore naturalistic communication. Nature\\nNeuroscience, 28(4):902–912, 2025.\\n[39] Alexander LM, Escalera J, Ai L, Andreotti C, Febre K, Mangone A, Vega-Potler N, Langer\\nN, Alexander A, Kovacs M, Litke S, O’Hagan B, Andersen J, Bronstein B, Bui A, Bushey M,\\nButler H, Castagna V, Camacho N, Chan E, Citera D, Clucas J, Cohen S, Dufek S, Eaves M,\\nFradera B, Gardner J, Grant-Villegas N, Green G, Gregory C, Hart E, Harris S, Horton M, Kahn\\nD, Kabotyanski K, Karmel B, Kelly SP, Kleinman K, Koo B, Kramer E, Lennon E, Lord C,\\nMantello G, Margolis A, Merikangas KR, Milham J, Minniti G, Neuhaus R, Levine A, Osman\\nY, Parra LC, Pugh KR, Racanello A, Restrepo A, Saltzman T, Septimus B, Tobe R, Waltz R,\\nWilliams A, Yeo A, Castellanos FX, Klein A, Paus T, Leventhal BL, Craddock RC, Koplewicz\\nHS, and Milham MP. \"HBN EO/EC task\". doi:10.18112/openneuro.ds004186.v2.0.0, 2022.\\n[40] Zhong-Lin Lu and Lloyd Kaufman. Magnetic source imaging of the human brain. Psychology\\nPress, 2003.\\n[41] Andreas Miltiadous, Katerina D. Tzimourta, Theodora Afrantou, Panagiotis Ioannidis, Niko-\\nlaos Grigoriadis, Dimitrios G. Tsalikakis, Pantelis Angelidis, Markos G. Tsipouras, Evri-\\npidis Glavas, Nikolaos Giannakeas, and Alexandros T. Tzallas.\\n\"a dataset of 88 EEG\\nrecordings from:\\nAlzheimer’s disease, Frontotemporal dementia and healthy subjects\".\\ndoi:10.18112/openneuro.ds004504.v1.0.2, 2023.\\n[42] Wajid Mumtaz.\\nMDD patients and healthy controls EEG data (New).\\ndoi:\\n10.6084/m9.figshare.4244171.v2, 2016.\\n[43] Guiomar Niso, Christine Rogers, Jeremy T. Moreau, Li-Yuan Chen, Cecile Madjar, Samir Das,\\nElizabeth Bock, François Tadel, Alan C. Evans, Pierre Jolicoeur, and Sylvain Baillet. OMEGA:\\nThe open MEG archive. NeuroImage, 124:1182–1187, 2016.\\n[44] Allison C Nugent, Elizabeth D Ballard, Jessica R Gilbert, Prejaas K Tewarie, Matthew J\\nBrookes, and Carlos A Zarate Jr. Multilayer MEG functional connectivity as a potential marker\\nfor suicidal thoughts in major depressive disorder. NeuroImage: Clinical, 28:102378, 2020.\\n[45] Iyad Obeid and Joseph Picone. The temple university hospital EEG data corpus. Frontiers in\\nNeuroscience, 10:196, 2016.\\n[46] Thom F Oostendorp and Adriaan van Oosterom. Source parameter estimation in inhomogeneous\\nvolume conductors of arbitrary shape. IEEE Transactions on Biomedical Engineering, 36(3):382–\\n391, 1989.\\n[47] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\\nfollow instructions with human feedback. In Proc. NeurIPS, 2022.\\n12'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='[48] Roberto D Pascual-Marqui, Christoph M Michel, and Dietrich Lehmann. Low resolution\\nelectromagnetic tomography: A new method for localizing electrical activity in the brain.\\nInternational Journal of psychophysiology, 18(1):49–65, 1994.\\n[49] Dzianok Patrycja and Kublik Ewa. \"a polish electroencephalography, Alzheimer’s risk-genes,\\nlifestyle and neuroimaging (pearl-neuro) database\". doi:10.18112/openneuro.ds004796.v1.0.9,\\n2024.\\n[50] Wei Yan Peh, Yuanyuan Yao, and Justin Dauwels. Transformer convolutional neural networks\\nfor automated artifact detection in scalp EEG. In Proc. EMBC, 2022.\\n[51] Peter Peyk, Harald T Schupp, Thomas Elbert, and Markus Junghöfer. Emotion processing in\\nthe visual brain: A MEG analysis. Brain topography, 20:205–215, 2008.\\n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In Proc. ICML, 2021.\\n[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n[54] Alexander P. Rockhill,\\nNicko Jackson,\\nJobi George,\\nAdam Aron,\\nand Nicole C.\\nSwann.\\n\"UC San Diego resting state EEG data from patients with Parkinson’s disease\".\\ndoi:10.18112/openneuro.ds002778.v1.0.5, 2021.\\n[55] Amilleah Rodriguez, Dan Zhao, Kyra Wilson, Ritika Saboo, Sergey V Samsonau, and Alec\\nMarantz. \"NeuroMorph: A high-temporal resolution MEG dataset for morpheme-based linguis-\\ntic analysis\". doi:10.18112/openneuro.ds005241.v1.1.0, 2024.\\n[56] Gerwin Schalk, Dennis J. McFarland, Thilo Hinterberger, Niels Birbaumer, and Jonathan R. Wol-\\npaw. BCI2000: a general-purpose brain-computer interface (BCI) system. IEEE Transactions\\non Biomedical Engineering, 51(6):1034–43, 2004.\\n[57] Neha Sharma, Avinash Upadhyay, Manoj Sharma, and Amit Singhal. Deep temporal networks\\nfor EEG-based motor imagery recognition. Scientific Reports, 13(1):18813, 2023.\\n[58] Seyed Yahya Shirazi, Alexandre Franco, Maurício Scopel Hoffmann, Nathalia B. Esper, Dung\\nTruong, Arnaud Delorme, Michael Milham, and Scott Makeig. \"healthy brain network (HBN)\\nEEG - Release 1\". doi:10.18112/openneuro.ds005505.v1.0.1, 2025.\\n[59] Seyed Yahya Shirazi, Alexandre Franco, Maurício Scopel Hoffmann, Nathalia B. Esper, Dung\\nTruong, Arnaud Delorme, Michael Milham, and Scott Makeig. \"healthy brain network (HBN)\\nEEG - Release 2\". doi:10.18112/openneuro.ds005506.v1.0.1, 2025.\\n[60] Seyed Yahya Shirazi, Alexandre Franco, Maurício Scopel Hoffmann, Nathalia B. Esper, Dung\\nTruong, Arnaud Delorme, Michael Milham, and Scott Makeig. \"healthy brain network (HBN)\\nEEG - Release 3\". doi:10.18112/openneuro.ds005507.v1.0.1, 2025.\\n[61] Seyed Yahya Shirazi, Alexandre Franco, Maurício Scopel Hoffmann, Nathalia B. Esper, Dung\\nTruong, Arnaud Delorme, Michael Milham, and Scott Makeig. \"healthy brain network (HBN)\\nEEG - Release 4\". doi:10.18112/openneuro.ds005508.v1.0.1, 2025.\\n[62] Seyed Yahya Shirazi, Alexandre Franco, Maurício Scopel Hoffmann, Nathalia B. Esper, Dung\\nTruong, Arnaud Delorme, Michael Milham, and Scott Makeig. \"healthy brain network (HBN)\\nEEG - Release 5\". doi:10.18112/openneuro.ds005509.v1.0.1, 2025.\\n[63] Seyed Yahya Shirazi, Alexandre Franco, Maurício Scopel Hoffmann, Nathalia B. Esper, Dung\\nTruong, Arnaud Delorme, Michael Milham, and Scott Makeig. \"healthy brain network (HBN)\\nEEG - Release 6\". doi:10.18112/openneuro.ds005510.v1.0.1, 2025.\\n[64] Seyed Yahya Shirazi, Alexandre Franco, Maurício Scopel Hoffmann, Nathalia B. Esper, Dung\\nTruong, Arnaud Delorme, Michael Milham, and Scott Makeig. \"healthy brain network (HBN)\\nEEG - Release 7\". doi:10.18112/openneuro.ds005511.v1.0.1, 2025.\\n13'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='[65] Seyed Yahya Shirazi, Alexandre Franco, Maurício Scopel Hoffmann, Nathalia B. Esper, Dung\\nTruong, Arnaud Delorme, Michael Milham, and Scott Makeig. \"healthy brain network (HBN)\\nEEG - Release 8\". doi:10.18112/openneuro.ds005512.v1.0.1, 2025.\\n[66] Johannes J.D. Singer, Radoslaw M. Cichy, and Martin N. Hebart.\\n\"the spatiotempo-\\nral neural dynamics of object recognition for natural images and line drawings (MEG)\".\\ndoi:10.18112/openneuro.ds004330.v1.0.0, 2022.\\n[67] Jessica Slater, Ridha Joober, Brenda Lynn Koborsy, Samantha Mitchell, Ella Sahlas, and\\nCaroline Palmer. Can electroencephalography (EEG) identify ADHD subtypes? A systematic\\nreview. Neuroscience & Biobehavioral Reviews, 139:104752, 2022.\\n[68] Yonghao Song, Xueyu Jia, Lie Yang, and Longhan Xie. Transformer-based spatial-temporal\\nfeature learning for EEG decoding. arXiv preprint arXiv.2106.11170, 2021.\\n[69] Yonghao Song, Qingqing Zheng, Bingchuan Liu, and Xiaorong Gao. EEG Conformer: Convo-\\nlutional transformer for EEG decoding and visualization. IEEE Transactions on Neural Systems\\nand Rehabilitation Engineering, 31:710–719, 2022.\\n[70] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. RoFormer:\\nEnhanced transformer with rotary position embedding. Neurocomput., 568(C), 2024.\\n[71] Li Su, Isma Zulfiqar, Fawad Jamshed, Elisabeth Fonteneau, and William Marslen-Wilson.\\nMapping tonotopic organization in human temporal cortex: Representational similarity analysis\\nin EMEG source space. Frontiers in neuroscience, 8:368, 2014.\\n[72] Marco Tagliasacchi, Yunpeng Li, Karolis Misiunas, and Dominik Roblek. SEANet: A multi-\\nmodal speech enhancement network. In Proc. Interspeech, 2020.\\n[73] Jason R. Taylor, Nitin Williams, Rhodri Cusack, Tibor Auer, Meredith A. Shafto, Marie Dixon,\\nLorraine K. Tyler, Cam-CAN, and Richard N. Henson. The Cambridge centre for ageing and\\nneuroscience (Cam-CAN) data repository: Structural and functional MRI, MEG, and cognitive\\ndata from a cross-sectional adult lifespan sample. NeuroImage, 144:262–269, 2017.\\n[74] Snezana Todorovic, Elin Runnqvist, Valerie Chanoine, and Jean-Michel Badier. \"gloups_MEG\".\\ndoi:10.18112/openneuro.ds005261.v3.0.0, 2025.\\n[75] J Vorwerk, J-H Cho, S Rampp, H Hamer, TR Knösche, and CH Wolters. A guideline for head\\nvolume conductor modeling in EEG and MEG. NeuroImage, 100:590–607, 2014.\\n[76] Jiquan Wang, Sha Zhao, Zhiling Luo, Yangxuan Zhou, Haiteng Jiang, Shijian Li, Tao Li, and\\nGang Pan. CBraMod: A criss-cross brain foundation model for EEG decoding. In Proc. ICLR,\\n2025.\\n[77] Shaonan Wang,\\nXiaohan Zhang,\\nJiajun Zhang,\\nand Chengqing Zong.\\n\"a syn-\\nchronized\\nmultimodal\\nneuroimaging\\ndataset\\nto\\nstudy\\nbrain\\nlanguage\\nprocessing\".\\ndoi:10.18112/openneuro.ds004078.v1.2.1, 2023.\\n[78] Yulin Wang, Wei Duan, Debo Dong, Lihong Ding, and Xu Lei. \"a test-retest resting and\\ncognitive state EEG dataset\". doi:10.18112/openneuro.ds004148.v1.0.1, 2022.\\n[79] M.P. Weisend, F.M. Hanlon, R. Montano, S.P. Ahlfors, A.C. Leuthold, D. Pantazis,\\nJ.C. Mosher, A.P. Georgopoulos, M.S. Hamalainen, and C.J. Aine.\\n\"MIND DATA\".\\ndoi:10.18112/openneuro.ds004107.v1.0.0, 2022.\\n[80] Toby Wise, Yunzhe Liu, Fatima Chowdhury, and Raymond J. Dolan. \"model-based aver-\\nsive learning in humans is supported by preferential task state reactivation\". 10.18112/open-\\nneuro.ds003682.v1.0.0, 2021.\\n[81] Banghua Yang, Fenqi Rong, Yunlong Xie, Du Li, Jiayang Zhang, Fu Li, Guangming Shi, and\\nXiaorong Gao. A multi-day and high-quality EEG dataset for motor imagery brain-computer\\ninterface. Scientific Data, 12(1):488, 2025.\\n[82] Chaoqi Yang, M Brandon Westover, and Jimeng Sun. BIOT: Biosignal transformer for cross-\\ndata learning in the wild. In Proc. NeurIPS, 2023.\\n14'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='[83] Chaoqi Yang, Cao Xiao, M Brandon Westover, and Jimeng Sun. Self-supervised electroen-\\ncephalogram representation learning for automatic sleep staging: Model development and\\nevaluation study. JMIR AI, 2:e46769, 2023.\\n[84] Chaoqi Yang, Cao Xiao, M Brandon Westover, Jimeng Sun, et al. Self-supervised electroen-\\ncephalogram representation learning for automatic sleep staging: Model development and\\nevaluation study. JMIR AI, 2(1):e46769, 2023.\\n[85] ChenTianyi Yang, Oliver Parish, Anastasia Klimovich-Gray, Cai Wingfield, William D. Marslen-\\nWilson, Chao Zhang, Alex Woolgar, and Andrew Thwaites. \"kymata soto language dataset: an\\nelectro-magnetoencephalographic dataset for natural speech processing\", 2025.\\n[86] Su Yang, Jose Miguel Sanchez Bornot, Kongfatt Wong-Lin, and Girijesh Prasad. M/EEG-based\\nbio-markers to predict the MCI and Alzheimer’s disease: a review from the ML perspective.\\nIEEE Transactions on Biomedical Engineering, 66(10):2924–2935, 2019.\\n[87] Ke Yi, Yansen Wang, Kan Ren, and Dongsheng Li. Learning topology-agnostic EEG represen-\\ntations with geometry-aware modeling. In Proc. NeurIPS, 2023.\\n[88] Zhizhang Yuan, Fanqi Shen, Meng Li, Yuguo Yu, Chenhao Tan, and Yang Yang. BrainWave: A\\nbrain signal foundation model for clinical applications. arXiv preprint arXiv:2402.10251, 2024.\\n[89] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi.\\nSoundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech,\\nand Language Processing, 30:495–507, 2021.\\n[90] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An instruction-tuned audio-visual\\nlanguage model for video understanding. In Proc. EMNLP, 2023.\\n15'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='A\\nBroader Impacts and Limitations\\nAs the first foundation model enabling unified analysis across EEG and MEG modalities, BrainOmni\\nrepresents a crucial step toward generalizable and scalable neural representation learning. BrainOmni,\\nas the first foundation-model support for MEG and joint EMEG analysis, offers substantial application\\npotential in neuroscience research, health monitoring, and clinical diagnosis. Additionally, the model’s\\nenhanced generalisation on heterogeneous devices could reduce the performance degradation typically\\nassociated with device and dataset variability.\\nHere we analyse the limitations of the work. Firstly, the scale of EEG and MEG data used for\\npretraining is still relatively limited. In particular, due to the limited availability of publicly accessible\\nMEG datasets, only 656 hours of MEG data were collected for the pretraining phase. Secondly,\\nthe downstream evaluation on MEG and EMEG modalities is constrained by both the scarcity of\\nsuitable datasets and the current lack of standardised test paradigms. In future work, we plan to\\nincorporate a larger and more diverse collection of datasets, especially across the MEG and EMEG\\nmodalities, to strengthen model performance and validate generalisation. Furthermore, we will extend\\nour framework to include additional modalities of neural signals beyond EEG and MEG to develop a\\nmore comprehensive and unified representation learning approach for neural activity recordings.\\nB\\nNeuroscience Background\\nB.1\\nSource Current Estimation\\nSource current estimation seeks to infer the spatiotemporal distribution of neural currents within the\\nbrain from non-invasive scalp measurements. It is an “ill-posed” inverse problem: it is impossible to\\nunambiguously determine the three-dimensional source current distribution inside the brain that gave\\nrise to those measurements even perfectly record full electric and magnetic field distribution around\\nthe head [29]. The distributed source model [12, 48] was developed to solve this problem, using\\na large number of dipoles or monopoles distributed within the brain volume or the cortex, making\\nthe problem linear. A commonly used and well-tested method is minimun norm estimation (MNE),\\nwhich obtains the maximum a posteriori probability estimate of source activity by constructing a\\nlinear inversion operator W under regularized conditions [25]. This method has been commonly\\nused in multiple neuroscience domains, including but not limited to audio decoding [71], emotion\\nanalysis [51], and epilepsy diagnosis [3, 18].\\nBroadly speaking, MNE involves two main stages: first, computing the forward solution (also known\\nas the leadfield matrix); and second, estimating the inverse operator (or backward solution) that maps\\nsensor data back to source space.\\nThe Forward Solution\\nThe forward problem in EEG and MEG refers to computing the scalp\\nelectric potentials or magnetic fields that would be measured given a known distribution of source cur-\\nrents within the brain. Under the quasi-static approximation of Maxwell’s equations, the relationship\\nbetween sources and measurements is linear and can be expressed as:\\ny(t) = Lj(t) + n(t)\\n(7)\\nwhere y(t) ∈RM is the vector of sensor measurements at time t, j(t) ∈RN is the source current\\ndistribution, L ∈RM×N is the leadfield matrix, and n(t) is measurement noise [25, 5].\\nEstimating the leadfield Matrix from the Head Geometry and Conductivity\\nThe leadfield\\nmatrix L encapsulates how each source location contributes to the sensor array and is calculated\\nusing a head model. This model is constructed from anatomical MRI, using numerical methods such\\nas the boundary element method (BEM) or finite element method (FEM), and incorporates realistic\\ntissue conductivities.\\nThe human head is typically modelled as a piecewise homogeneous volume conductor with com-\\npartments representing scalp, skull, cerebrospinal fluid (CSF), and brain. Each region is assigned an\\nisotropic or anisotropic conductivity value σi, and the electrical potential ϕ in the volume conductor\\nis governed by the Poisson equation:\\n16'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='∇· (σ(r)∇ϕ(r)) = ∇· Jp(r)\\n(8)\\nwhere σ(r) is the spatially varying conductivity, and Jp(r) is the primary current density, confined to\\nthe gray matter (cortex). For EEG, the scalp potential V at sensor location rm is obtained by solving\\nthis equation using numerical methods like the boundary element method (BEM) or finite element\\nmethod (FEM):\\nV (rm) =\\nZ\\nΩ\\nG(rm, r′)∇· Jp(r′) dr′\\n(9)\\nHere, G(rm, r′) is the Green’s function representing the head’s geometry and conductivity structure,\\nwhile Ωis the brain/source volume. The leadfield matrix L used in the forward model is computed\\nby solving this equation for each source element and each sensor.\\nIn the case of MEG, the magnetic field B generated by a primary current Jp is computed using the\\nquasi-static Biot-Savart law:\\nB(rm) = µ0\\n4π\\nZ\\nΩ\\nJp(r′) × (rm −r′)\\n|rm −r′|3\\ndr′\\n(10)\\nSince MEG is insensitive to radial currents and largely unaffected by conductivity discontinuities\\n(like the low-conductivity skull), it is more robust to inaccuracies in the conductivity model [5]. In\\ncontrast, EEG is highly sensitive to these properties, especially the skull’s low conductivity (∼0.0042\\nS/m compared to the brain’s ∼0.33 S/m) [75].\\nTo construct realistic forward models, anatomical MRI data is segmented to extract surfaces or\\nvolumes of different tissue types. BEM solves the boundary integrals over these surfaces, assum-\\ning piecewise constant conductivities, while FEM can accommodate more complex, anisotropic\\nconductivity distributions within each region [46].\\nThe Backward Solution and Linear Inverse Operator\\nThe inverse problem—estimating the\\nsources j(t) from the measurements y(t)—is ill-posed and requires regularization. Minimum-norm\\nestimation solves this by seeking the source configuration with the smallest ℓ2-norm that still explains\\nthe data:\\nˆj(t) = arg min\\nj\\n∥y(t) −Lj(t)∥2\\n2 + λ2 ∥j(t)∥2\\n2\\n(11)\\nwhere λ is a regularization parameter controlling the trade-off between data fidelity and source\\npower [25, 28].\\nThis yields a closed-form solution:\\nˆj(t) = Wy(t),\\nwhere\\nW = L⊤\\x00LL⊤+ λ2Cn\\n\\x01−1\\n(12)\\nHere, Cn is the noise covariance matrix. When whitening is applied, we define:\\nLw = C−1/2\\nn\\nL,\\nyw(t) = C−1/2\\nn\\ny(t)\\n(13)\\nand compute the inverse operator as:\\nW = GL⊤\\nw\\n\\x00LwL⊤\\nw + λ2I\\n\\x01−1\\n(14)\\nwhere G is a source covariance matrix, often the identity or a depth-weighted diagonal matrix to\\ncompensate for depth bias [37].\\n17'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='SEANet\\nEncoder\\nSEANet Decoder\\nSensor\\nEncoder\\nCross Attention\\nCross Attention\\nLearnable Q\\nK\\nV\\nRVQ\\nK\\nV\\nQ\\nEMEG Device\\nLayout\\nRaw EMEG\\nSignal\\nReconstructed EMEG Signal\\nBackward Solution\\nForward Solution\\nLatent Source Variable\\nForward Solution\\nBackward Solution\\nBrain Source Activity\\nEMEG Device Layout Raw EMEG Signal\\nReconstructed EMEG Signal\\nFigure 5: Correlation between source current estimation and the proposed BrainTokenizer. The\\nBrainTokenizer (excluding RVQ) can be viewed as the backward solution in source current estimation,\\nand the reconstructor can be viewed as the forward solution.\\nB.2\\nModel Explanation From the Perspective of Neuroscience\\nOur BrainTokenizer design closely parallels the classical source current estimation process, as\\nillustrated in Fig. 5. As introduced above, the backward solution in source current estimation infer\\nbrain source activity from externally measured signals and sensor layout information, and forward\\nsolution reconstructs the external signals from the inferred brain source activity.\\nInspired by this conceptual framework, our proposed BrainTokenizer closely mirrors these two steps.\\nSpecifically, the BrainTokenizer (excluding the RVQ quantization step) corresponds directly to the\\nbackward solution stage, where it maps the raw EMEG signals and sensor device layout into latent\\nsource variables through a parametric module. Recall the classical backward solution,\\nˆj(t) = Wy(t),\\nwhere\\nW = L⊤\\x00LL⊤+ λ2Cn\\n\\x01−1\\n(15)\\nwhich relies on a fixed linear inverse operator W constructed from a leadfield metrix L and a noise\\ncovariance matrix Cn. While in our architecture, we use the attention weights in cross-attention\\nmechanisms to construct a time-varying parametric inverse operator Wθ(Ztime, L, S), where the\\noperator is related to sensor prosperity and the measured data since the different fields recorded\\noutside may indicate that there is a change in the pattern of neuro activity, formulated as:\\nK = Ztime + V(L, S)\\n(16)\\nWθ(Ztime, L, S) = softmax(QKT)\\n√dhead\\n(17)\\nwhere Q is a set of learnable query embeddings that dynamically aggregate temporal and spatial\\ninformation, K is the combination of temporal representation Ztime from SEANet encoder and sensor\\nembeddings V(L, S), and dhead is the scaling factor determined by the attention head dimension.\\nDifferent from traditional linear, fixed operators, these learned attention weights naturally serve as\\nflexible and data-driven inverse operators, adaptively integrating spatiotemporal sensor properties\\nand temporal signal dynamics to estimate latent variables.\\nFollowing this analogy, the reconstructor corresponds to the forward solution, reconstructing the\\noriginal observed signals from the inferred latent sources. Similar to the backward process, the\\nreconstructor also uses a cross-attention mechanism, now employing latent source variables as keys\\nand sensor embeddings as values. Through the decoder module (SEANet decoder), these features are\\nfurther translated back into reconstructed EMEG signals.\\n18'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='C\\nRelated Work on Brain Foundation Models\\nA foundation model is a deep learning model pretrained on large-scale datasets, generally approached\\nby self-supervised learning (SSL), to learn signal representations from abundant unlabelled data and\\nreduce dependence on labelled samples. This paradigm has achieved notable success in computer\\nvision [52, 90], natural language processing [17, 53, 47] and speech [4, 31, 10]. Brain signals,\\nespecially EEG and MEG, show low signal-to-noise ratio, high dimensionality and individual\\nvariability, making large-scale pretraining for robust representation and general brain model important.\\nIn EEG domain, BENDR [35], inspired by Wav2Vec 2.0 model, employs contrastive learning method\\nto learn from massive anouts of EEG data. Since then, more model was proposed for more generalized\\nand stronger representation. BIOT [82] enable cross-data learning with mismatched channels, variable\\nlengths, and missing values by tokenizing different biosignals into unified “sentences” structure.\\nMMM [87] adopts a topology-agnostic scheme and implements multi-dimensional position encoding,\\nmulti-level channel hierarchies and multi-stage pretraining. LaBraM [32] segments EEG signals into\\nchannel patches, discretizes them via vector quantization before training with masked EEG modeling\\nto capture high-level semantics. CbraMod [76] introduces a Criss-Cross Transformer to fully leverage\\nEEG’s spatiotemporal characteristics. Beyond EEG-only approaches, multimodal brain foundation\\nmodels have also been developed before. For instance, BrainWave [88] and PopT [8] jointly train on\\nEEG and intracranial EEG (iEEG) data. However, despite MEG’s superior spatiotemporal resolution\\nand its frequent combined use with EEG in neuroscience, foundation model research for MEG and\\njoint EMEG remains largely unexplored.\\nD\\nPretraining Dataset Description\\nBelow, we provide detailed descriptions of the datasets used for pretraining BrainTokenizer and\\nBrainOmni.\\n• MEG-MASC[24]: The MEG-MASC dataset includes MEG recordings ( 208 channels,\\n1000 Hz ) from 27 English speakers listening to 2 hours of naturalistic stories, collected\\nusing an axial-gradiometer KIT system.\\n• MEG-Narrative-Dataset[2]: The MEG-Narrative-Dataset includes MEG recordings ( 275\\nchannels, 1200 Hz ) from 3 English speakers listening to 10 hours of naturalistic stories,\\ncollected using an axial gradiometer CTF system.\\n• OMEGA[43]: The OMEGA dataset includes MEG recordings ( 306 channels, 1000 Hz )\\nfrom 444 healthy subjects and 200 patient volunteers with Parkinson’s disease, ADHD, and\\nchronic pain in a resting state, collected using CTF whole-head MEG systems from VSM\\nMedTech Inc.\\n• CC700[73] The CC700 dataset is a subset of the Cam-CAN dataset, including MEG\\nrecordings ( 306 channels, 1000 Hz ) from nearly 700 subjects performing various tasks and\\nin a resting state, collected using an Elekta-Neuromag system.\\n• Go-Nogo[16]: The Go-Nogo dataset includes EEG recordings ( 32 channels, 1000 Hz )\\nfrom 14 subjects performing an animal categorization task and a recognition task, collected\\nusing a Neuroscan 5083 system.\\n• MusicEEG[13]: The MusicEEG dataset includes EEG recordings ( 19 channels, 1000 Hz\\n) from 31 subjects listening to 40 music clips of 12 s duration each, targeting a range of\\nemotional states, collected using a BrainProducts BrainAmp system.\\n• HFO[11]: The HFO dataset includes sleep EEG recordings ( 23 channels, 1024 Hz ) from\\n30 children and adolescents with focal or generalized epilepsy, collected using a Micromed\\nEEG system.\\n• AversiveMEG[80]: The AversiveMEG dataset includes MEG recordings ( 275 channels,\\n1200 Hz ) from 28 subjects completing an aversive learning task, collected using a CTF\\nOmega system.\\n• SRM[27]: The SRM dataset includes EEG recordings ( 64 channels, 1024 Hz ) from 111\\nsubjects in resting state, collected using a BioSemi ActiveTwo system.\\n• MIND[79]: The MIND dataset includes MEG recordings ( 306 channels, 1250 Hz ) from 8\\nsubjects who received nerve electrical stimuli, collected using an Elekta-Neuromag system.\\n19'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='• RestCog[78]: The RestCog dataset includes EEG recordings ( 64 channels, 500 Hz ) from\\n60 subjects during 3 experimental sessions together with sleep, emotion, mental health, and\\nmind-wandering related measures, collected using a Brain Products GmbH system.\\n• SMN4Lang[77]: The SMN4Lang dataset includes MEG recordings ( 318 channels, 1000\\nHz ) from 12 subjects listening to 6 hours of naturalistic stories, collected using an Elekta-\\nNeuromag system.\\n• HBN EO/EC[39]: The HBN EO/EC dataset includes EEG recordings ( 129 channels, 500\\nHz ) from 2952 children in resting state with eyes-open for 20 seconds and eyes-closed for\\n40 seconds, collected using an EGI system.\\n• THINGS-MEG[30]: The THINGS-MEG dataset includes MEG recordings ( 275 channels,\\n1200 Hz ) of 4 subjects who were shown 22448 unique images of 1854 objects, collected\\nusing a CTF 275 MEG system.\\n• ASWR-MEG[21]: The ASWR-MEG dataset includes MEG recordings ( 160 channels,\\n1000 Hz ) from 24 subjects listening to random word sequences and judging whether a probe\\nword is related to the preceding word, collected using an Elekta-Neuromag system.\\n• ImageLine[66]: The ImageLine dataset includes MEG recordings ( 306 channels, 1000 Hz\\n) from 30 subjects watching images of objects depicted as photographs, line drawings, or\\nsketch-like drawings, collected using an Elekta-Neuromag system.\\n• Features-EEG[23]: The Features-EEG dataset includes EEG recordings ( 128 channels,\\n1000 Hz ) from 16 subjects watching 256 oriented gratings that varied on four feature\\ndimensions, collected using a BrainVision ActiChamp EEG system.\\n• PEARL-Neuro[49]: The PEARL-Neuro dataset includes EEG recordings ( 128 channels,\\n1000 Hz ) from 79 subjects during resting state (eyes opened and closed) and cognitive tasks,\\nincluding the multi-source interference task and Sternberg’s memory task, collected using a\\nBrain Products GmbH system.\\n• NeuroMorph[55]: The NeuroMorph dataset includes MEG recordings ( 208 channels, 1000\\nHz ) from 24 subjects doing a lexical decision task and a localizing task, collected using a\\nKIT/Yokogawa MEG system.\\n• Kymata-SOTO[85]: The Kymata-SOTO dataset includes MEG recordings ( 306 channels,\\n1000 Hz ) and EEG recordings ( 64 channels, 1000 Hz ) from 20 subjects listening to English\\nconversations and from 15 subjects listening to Russian conversations, collected using an\\nElekta-Neuromag system.\\n• HBN-EEG[58, 59, 60, 61, 62, 63, 64, 65]: The HBN-EEG dataset includes EEG recordings\\n( 128 channels, 500 Hz ) from 1897 subjects who did 6 tasks including resting state, surround\\nsuppression, movie watching, contrast change detection, sequence learning, and symbol\\nsearch, collected using a Magstim-EGI system.\\n• Awakening[6]: The Awakening dataset includes EEG recordings ( 65 channels, 5000 Hz )\\nfrom 21 subjects during resting state and propofol sedation, aiming to investigate the effects\\nof propofol on dreaming, collected using a Brain Products GmbH system.\\nE\\nDownstream Finetuning Dataset Description\\nBelow, we provide detailed descriptions of the downstream datasets used for finetuning BrainOmni.\\n• Gloups-MEG[74]: The Gloups-MEG dataset includes MEG recordings ( 248 channels,\\n2034.5 Hz ) of 17 subjects completing a learning task and a resting-state condition, collected\\nusing an Elekta-Neuromag system.\\n• PerceiveImagine[6]: The PerceiveImagine dataset includes EEG recordings ( 64 channels,\\n1000 Hz ) of 52 subjects watching an image for 6 seconds, and then imagining the image\\nthey see for 6 seconds, collected using a Neuroscan Synamps2 system.\\n• TUAB[45]: The TUAB dataset includes EEG recordings ( 21 channels, 256 Hz ) from 2328\\npatients, annotated as normal or abnormal. A total of 408853 10-second samples were used\\nfor classification to predict these labels.\\n20'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='• TUEV[45]: The TUEV dataset includes EEG recordings ( 21 channels, 256 Hz ) from 294\\nsubjects, which are segmented into 112237 5-second samples across 6 classes: (1) spike and\\nsharp wave (SPSW), (2) generalized periodic epileptiform discharges (GPED), (3) periodic\\nlateralized epileptiform discharges (PLED), (4) eye movement (EYEM), (5) artifact (ARTF),\\nand (6) background (BCKG). We perform a classification to predict these event labels.\\n• MDD[42]: The MDD dataset includes EEG recordings ( 20 channels, 256 Hz ) from 35\\npatients with major depressive disorder and 30 normal controls across three sessions: eyes\\nopen, eyes closed, and task. A total of 7302 10-second samples were used for classification\\nto predict the presence of major depressive disorder.\\n• WBCIC_SHU[81]: The WBCIC_SHU dataset includes EEG recordings ( 58 channels,\\n1000 Hz ) from 51 subjects performing motor imagery tasks: left-hand grasping, right-hand\\ngrasping. A total of 30591 4-second samples were used for classification to predict the type\\nof task.\\n• PhysioNet-MI[56]: The PhysioNet-MI dataset includes EEG recordings ( 64 channels, 160\\nHz ) from 109 subjects performing 4 motor imagery and movement tasks. A total of 19571\\n4-second samples were used for classification to predict the type of task.\\n• FACED[9]: The FACED dataset includes EEG recordings ( 30 channels, 1000 or 250\\nHz ) from 123 subjects watching 28 emotion-elicitation video clips covering 9 emotion\\ncategories. The coarse categories (negative, positive and neutral) are utilised. A total of\\n10332 10-second samples were used for classification to predict the emotion categories.\\n• PD31[54]: The PD31 dataset includes EEG recordings ( 32 channels, 512 Hz ) from 16\\nhealthy subjects and 15 subjects with Parkinson’s disease during resting state. A total of\\n882 10-second samples were used for classification to predict the presence of Parkinson’s\\ndisease.\\n• ASD74[19]: The ASD74 dataset includes MEG recordings ( 306 channels, 1000 Hz ) from\\n35 children with autism spectrum disorders (ASD) and 39 typically developing children,\\nwho watched videos while receiving auditory stimuli. A total of 12320 10-second samples\\nwere used for classification to predict the presence of autism spectrum disorders.\\n• SomatoMotor[36]: The SomatoMotor dataset includes both EEG ( 74 channels, 1004\\nHz ) and MEG recordings ( 306 channels, 1004 Hz ) from 5 subjects who received nerve\\nelectrical stimuli at the right wrist. The task was to lift the left index finger as quickly as\\npossible after each right median nerve stimulus. A total of 1208 2-second samples were\\nused for classification to predict whether their fingers were lifted because of nerve stimulus,\\nor spontaneously.\\n• AD65[41]: The AD65 dataset includes EEG recordings ( 19 channels, 500 Hz ) from 36\\nsubjects with Alzheimer’s disease and 29 healthy subjects during resting state. A total of\\n7013 10-second samples were used for classification to predict the presence of Alzheimer’s\\ndisease.\\nF\\nBaseline Models Description\\nWe compare BrainOmni with both pretrained and non-pretrained baseline models on various down-\\nstream tasks. The basic information of the baseline models is as follows:\\n• CNN-Transformer[50]: CNN-Transformer is a non-pretrained neural network that com-\\nbines CNNs and transformer blocks to model long-range dependencies in CNN-derived\\nfeatures.\\n• ContraWR[83]: ContraWR is a non-pretrained neural network that consists of a cascaded\\npipeline beginning with a short-time Fourier transform (STFT), followed by a 2D CNN\\nlayer and three subsequent 2D convolutional blocks.\\n• SPaRCNet[34]: SPaRCNet is a non-pretrained 1D CNN based neural network with dense\\nresidual connections.\\n• ST-Transformer[68]: ST-Transformer is a non-pretrained transformer-based model that\\nleverages the attention mechanism to better utilize both spatial and temporal features in EEG\\ndata.\\n21'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='• LaBraM[32]: LaBraM is a foundational EEG model pretrained on diverse datasets. It\\nemploys vector-quantized neural tokenization and masked channel prediction to effectively\\nlearn robust EEG features.\\n• CBraMod[76]: CBraMod is a foundational EEG model pretrained on diverse datasets. It\\nemploys a Criss-Cross Transformer architecture to separately model spatial and temporal\\ndependencies, thereby effectively learning robust EEG features.\\nG\\nImplementation Details\\nWe trained BrainTokenizer using 2-second segments. For training BrainOmni, we inputted 30-\\nsecond data segments to allow the model to capture longer temporal dependencies. During the\\nsegmented tokenization process, we set the overlap ratio between windows to 25% to incorporate\\npartial contextual information. The training was conducted on 16 A100 GPUs, using the AdamW\\noptimizer and a warmup-cosine-decay learning rate scheduler with a warmup proportion of 10%.\\nBrainTokenizer was trained for 16 epochs with a total batch size of 512 per update step and a\\nmaximum learning rate of 2e-4. BrainOmni was trained for 32 epochs with a total batch size of 256\\nper update step and a maximum learning rate of 4e-4.\\nH\\nAblation Study\\nH.1\\nLatent Source Activity Estimation\\nAs mentioned in the Introduction, modeling at the electrode level can encounter issues such as\\nlow information density and redundant information in adjacent channels. The latent source activity\\nmodeling process involves compressing the sequence length, which, while condensing semantic\\nfeatures, may also introduce some loss of detail. To further demonstrate the advantages of modeling\\nsource activity compared to modeling at the electrode level, we trained a model that integrates spatial\\nfeatures through self-attention between electrodes, which would not involve loss of information. To be\\nmore specific, the BrainTokenizer of this model replaces the cross attention layer with self-attention\\nlayer. Channel random masking and additive noise are also applied before the self-attention layer in\\nthe encoder part of the feature input. In the decoder part, the masked channels are replaced with a\\nmask token, and the waveform of the masked part is recovered by a self-attention layer.\\nTable 7: Balanced accuracy results of ablation study for latent source activity estimation (LSAE). For\\nthe model with LSAE, the BrainTokenizer compresses the channels into 16 channels uniformly.\\nTUAB\\nPD31\\nAD65\\nASD74\\nSomatoMotor\\ntiny w/o NAE\\n0.814 ± 0.002\\n0.595 ± 0.051\\n0.792 ± 0.034\\n0.605 ± 0.052\\n0.805 ± 0.127\\ntiny w/ NAE\\n0.816 ± 0.001\\n0.658 ± 0.062\\n0.800 ± 0.035\\n0.606 ± 0.019\\n0.787 ± 0.112\\nFrom the table above, we can see that even though models using latent source activity estimation\\nmay lose some information during the sequence length compression process, their performance on\\nEEG datasets such as TUAB (23), PD31 (32), ad65 (19), asd74 (306) and SomatoMotor (372) still\\nexceeds that of models built at the electrode level. The numbers in parentheses represent the number\\nof channels in each dataset. For the SomatoMotor (372) dataset, even with a compression ratio of\\naround 23 times, the performance of the model using LSAE is only 0.018 lower than that of the\\nuncompressed model. Moreover, since the computational complexity of the Transformer model grows\\nquadratically with sequence length, models using LSAE have significantly higher computational\\nefficiency compared to those that do not use it. This demonstrates that the process of LSAE can\\neliminate redundant information between channels while retaining core semantic features. It also\\nillustrates the benefits of projecting all data from the electrode level into a unified feature space for\\nmodeling.\\n22'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='H.2\\nFreeze Backbone for Downstream\\nIn Table 8, we report the BACC metrics of BrainOmni and the baseline models under both full\\nfine-tuning and frozen settings on TUAB, TUEV, and AD65 datasets. In TUAB and AD65, the\\nperformance of BrainOmni under the weight freezing scenario is comparable to or even higher than\\nthe metrics of the baseline models with full fine-tuning. This indicates that BrainOmni has learned the\\npattern features of EMEG signals with better generalization capability through unsupervised training.\\nTable 8: Results for freeze the backbone model when downstream training. Results with full\\nfinetuning also listed for comparison. BACC reported.\\nTUAB\\nTUEV\\nAD65\\nLaBraM\\nFreeze\\n0.788 ± 0.005\\n0.394 ± 0.030\\n0.699 ± 0.025\\nFull Finetune\\n0.807 ± 0.003\\n0.596 ± 0.016\\n0.706 ± 0.012\\nCBraMod\\nFreeze\\n0.768 ± 0.001\\n0.398 ± 0.067\\n0.633 ± 0.003\\nFull Finetune\\n0.773 ± 0.001\\n0.416 ± 0.042\\n0.649 ± 0.006\\nBrainOmni_tiny\\nFreeze\\n0.806 ± 0.001\\n0.464 ± 0.021\\n0.772 ± 0.046\\nFull Finetune\\n0.816 ± 0.001\\n0.597 ± 0.019\\n0.800 ± 0.035\\nBrainOmni_base\\nFreeze\\n0.811 ± 0.001\\n0.489 ± 0.016\\n0.780 ± 0.058\\nFull Finetune\\n0.813 ± 0.007\\n0.618 ± 0.034\\n0.806 ± 0.053\\nI\\nHyperparameters for model\\nThis section presents the model parameters of BrainTokenizer and BrainOmni, as well as the training\\nhyperparameters during pre-training and downstream fine-tuning.\\nTable 9: Hyperparameters for BrainTokenizer training\\nHyperparameters\\nValues\\nBrain Tokeniser\\nWindow length\\n512\\nN filters\\n32\\nRatios\\n[8, 4, 2]\\nKernel size\\n5\\nLast Kernel size\\n5\\nHidden dim\\n256\\nCodebook dim\\n256\\nCodebook size\\n512\\nNum quantizers\\n4\\nRotation trick\\nTrue\\nQuantize optimize method\\n\"ema\"\\nLatent source numberz\\n16\\nAttention head number\\n4\\nDropout\\n0.0\\nTotal batch per update\\n512\\nWeight decay\\n1e-2\\nLr\\n2e-4\\nEpochs\\n16\\nOptimizer\\nType\\nAdamW\\nBetas\\n[0.5, 0.9]\\nEps\\n1e-5\\nScheduler\\nType\\nWarmupCosineLR\\nWarmup ratio\\n0.1\\nCos min ratio\\n0.05\\n23'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='Table 10: Hyperparameters for BrainOmni training\\nHyperparameters\\nBrainOmni-tiny\\nBrainOmni-base\\nBrainOmni\\nHidden dim\\n256\\n512\\nAttention head number\\n8\\n16\\nAttention depth\\n12\\n12\\nLr\\n5e-4\\n4e-4\\nOverlap ratio\\n0.25\\nDropout\\n0.1\\nMask ratio\\n0.5\\nTotal batch per update\\n256\\nEpochs\\n32\\nWeight decay\\n5e-2\\nOptimizer\\nType\\nAdamW\\nBetas\\n[0.9, 0.95]\\nEps\\n1e-6\\nScheduler\\nType\\nWarmupCosineLR\\nWarmup ratio\\n0.1\\nCos min ratio\\n0.1\\nTable 11: Hyperparameters for downstream finetuning\\nHyperparameters\\nValues\\nTotal batch per update\\n64\\nEpochs\\n30\\nWeight decay\\n5e-2\\nLabel smoothing\\n0.1\\nOptimizer\\nType\\nAdamW\\nBetas\\n[0.9, 0.95]\\nEps\\n1e-6\\nScheduler\\nType\\nWarmupCosineLR\\nWarmup ratio\\n0.05\\nCos min ratio\\n0.1\\n24'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_BrainOmni_ A Brain Foundation Model for Unified EEG and MEG Signals.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': 'BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals', 'author': 'Qinfan Xiao; Ziyun Cui; Chi Zhang; Siqi Chen; Wen Wu; Andrew Thwaites; Alexandra Woolgar; Bowen Zhou; Chao Zhang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='J\\nAttention Weights of Cross Attention in the BrainTokenizer\\nRecall the content in Appendix B.2. The cross-attention mechanism in BrainTokenizer is inspired\\nby the inverse operator in the Backward Solution. The attention weights can actually be regarded\\nas a parameterised linear inverse matrix, representing the degree of influence of each channel on\\neach latent source activity. From Fig. 6, we can see that each source variable in our model has\\nautomatically learned a hierarchical structure. It is evident that each source variable focuses on\\nextracting features from specific scalp regions. Moreover, in the multi-head attention mechanism, the\\nattention of the first head is relatively concentrated, while the subsequent heads progressively expand\\nthe range of regions.\\nFigure 6: Visualization of the topographic maps of cross-attention for each channel in BrainTokenizer.\\nThe model’s 16 queries are displayed in 8 columns on the top and bottom, with the first eight and the\\nlast eight queries shown separately. The 4 rows represent the attention weights of each head in the\\nmulti-head cross-attention mechanism.\\n25'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 0}, page_content='Brain-to-Text Decoding:\\nA Non-invasive Approach via Typing\\nJarod Lévy1, Mingfang (Lucy) Zhang2,3, Svetlana Pinet4,5, Jérémy Rapin1, Hubert Banville1, Stéphane\\nd’Ascoli1∗, Jean-Rémi King1∗\\n1Meta AI, 2École Normale Supérieure, Université PSL, CNRS, 3Hospital Foundation Adolphe de\\nRothschild, 4Basque Center on Cognition, Brain and Language, San Sebastian, 5Ikerbasque, Basque\\nFoundation for Science, Bilbao\\n∗equal contribution\\nModern neuroprostheses can now restore communication in patients who have lost the ability to speak\\nor move. However, these invasive devices entail risks inherent to neurosurgery. Here, we introduce a\\nnon-invasive method to decode the production of sentences from brain activity and demonstrate its\\nefficacy in a cohort of 35 healthy volunteers. For this, we present Brain2Qwerty, a new deep learning\\narchitecture trained to decode sentences from either electro- (EEG) or magneto-encephalography\\n(MEG), while participants typed briefly memorized sentences on a QWERTY keyboard. With MEG,\\nBrain2Qwerty reaches, on average, a character-error-rate (CER) of 32% and substantially outperforms\\nEEG (CER: 67%). For the best participants, the model achieves a CER of 19%, and can perfectly\\ndecode a variety of sentences outside of the training set. While error analyses suggest that decoding\\ndepends on motor processes, the analysis of typographical errors suggests that it also involves higher-\\nlevel cognitive factors. Overall, these results narrow the gap between invasive and non-invasive\\nmethods and thus open the path for developing safe brain-computer interfaces for non-communicating\\npatients.\\nDate: February 26, 2025\\nCorrespondence: {jarod,sdascoli,jeanremi}@meta.com\\nFigure 1 Approach. Recordings from 35 participants were obtained using electro-encephalography (EEG) and magneto-\\nencephalography (MEG). Sentences were displayed word-by-word on a screen. Following the final word, a visual cue\\nprompted them to begin typing this sentence, without visual feedback. Our Brain2Qwerty model includes three core\\nstages to decode text from brain activity: (1) a convolutional module, input with 500 ms windows of M/EEG signals,\\n(2) a transformer module trained at the sentence level, and (3) a pretrained language model to correct the outputs of\\nthe transformer. Performance is assessed using a Character Error Rate (CER) at the sentence level. An analysis of\\nhow the brain performs typing is described in a companion paper (Zhang et al., 2025).\\n1\\nIntroduction\\nThe past decade has been marked by rapid progress in brain-computer interfaces (BCIs) for individuals who,\\nafter a brain lesion, have lost their ability to speak or communicate. In particular, several patients suffering\\n1\\narXiv:2502.17480v1  [eess.SP]  18 Feb 2025'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 1}, page_content='from anarthria (Metzger et al., 2022; Moses et al., 2021), Amyotrophic Lateral Sclerosis (ALS) (Willett\\net al., 2023), or severe paralysis (Hochberg et al., 2012) have now been able to produce full sentences via a\\nneuroprosthesis, which records and decodes neural activity from motor regions of the brain. Originally limited\\nto decoding small sets of linguistic features (Herff et al., 2019; Angrick et al., 2019; Anumanchipalli et al.,\\n2019; Moses et al., 2021; Card et al., 2024), words (Metzger et al., 2022), and gestures (Willett et al., 2021),\\nthe recent development of AI models has improved the precision and rapidity of brain-to-text decoding to a\\npoint of enabling natural language production at rates close to normal speech (Metzger et al., 2023; Wairagkar\\net al., 2024).\\nHowever, such invasive neuroprostheses require a neurosurgical procedure, and thus expose patients to non-\\nnegligible risks of brain hemorrhage and infection (Chung et al., 2019; Bullard et al., 2020; Leuthardt et al.,\\n2021; Baranauskas, 2014). Additionally, maintaining functional cortical implants over extended time periods\\nremains challenging (Fekete et al., 2023; Zhou et al., 2024; Yasar et al., 2024). As a result, in their current\\nform, invasive BCIs are not easily scalable for diagnosing or restoring communication in the large groups of\\nnon- or poorly-responsive patients (Owen et al., 2006; Claassen et al., 2019).\\nNon-invasive BCIs could potentially address this challenge.\\nHowever, they are usually based on scalp\\nelectroencephalography (EEG), whose limited signal-to-noise ratio (Mak and Wolpaw, 2009) requires users\\nto perform complex tasks. For example, EEG-based BCIs typically require individuals to maintain their\\nattention on flickering stimuli (Abiri et al., 2019) or to imagine moving their hand or foot over long time\\nperiods (Bodien et al., 2024) – two tasks known to produce EEG patterns that can be relatively easily detected\\nby a linear classifier. Even so, decoding performance remains moderate. For instance, a public BCI benchmark\\n(Chevallier et al., 2024) using EEG achieves an accuracy of only 43.3% on a four-class classification task with\\na motor imagery dataset (Yi et al., 2014). In sum, current non-invasive methods fall short of providing a fast\\nand reliable BCI.\\nTwo elements could address these challenges. First, magnetoencephalography (MEG), which measures the\\nfluctations of magnetic fields elicited in the cortex, has higher signal-to-noise ratio than EEG (Hämäläinen\\net al., 1993; Goldenholz et al., 2009; Baillet, 2017). Second, deep learning models trained to reconstruct\\nnatural language from MEG signals in language comprehension paradigms have recently demonstrated major\\nimprovements, especially in comparison to EEG (Défossez et al., 2023). Together, these elements thus indicate\\nthat with modern AI techniques, high-quality MEG signals and natural language tasks could be combined to\\ndecode the production of language from non-invasive recordings of the brain.\\nIn this study, we introduce Brain2Qwerty, an AI model trained to decode text production from non-invasive\\nrecordings of brain activity (Fig. 1). For this, we tasked 35 participants to type briefly memorized sentences\\non a keyboard, while their brain activity was recorded with either EEG or MEG. We then train Brain2Qwerty,\\na three-stage deep neural network trained to decode text from these brain signals and evaluate it on both\\nEEG (20 participants, 146K characters, 23K words and 4K sentences) and MEG recordings (20 participants,\\n193K characters, 30K words and 5K sentences). Note that the present study does not delve into how the\\nbrain produce language during typing. This neuroscientific issue is addressed in a companion paper (Zhang\\net al., 2025).\\n2\\nResults\\n2.1\\nLinear Decoding\\nTo verify that our typing protocol leads to the expected brain responses, we first focus on the differences\\nin evoked responses elicited by left- and right-handed key presses (Fig. 2 A-B). The resulting topographies\\nare typical of those associated with motor activity in the cortex (Donner et al., 2009). In addition, we\\ntrained a linear ridge classifier per subject to categorize left- vs right-handed responses at each time-sample\\nrelative to key presses. The classification accuracy peaks t=40 ms after the key press (Fig. 2). MEG achieves\\na peak accuracy of 74±1.3% (± reports standard-error-of-the-mean (SEM) across subjects), significantly\\noutperforming EEG which achieves 64±0.8% (Mann-Withney U test: p<10−7). We then trained the same\\nmodel to classify the keys pressed. Character accuracy peaks around the same time, reaching a value of\\n22±0.8% for MEG and 16±0.5% for EEG, significantly above the chance level (14%). Overall, these findings\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 2}, page_content='Figure 2 Decoding Performance across models. A. Difference in EEG evoked responses between left vs right hand key\\npresses. Each black line is the differential voltage of a sensor relative to key press. B. Same as A but for MEG. C.\\nLinear classifiers are trained, at each time sample, to predict the left vs right hand relative to each key press. The gray\\nline represents chance level and the error bar is the standard error of the mean across participants. Significant decoding\\nscores (p < 0.05) are marked with a star. D. Same as C but for character classification. E-H. Comparison of baselines\\n(linear and EEGNet), and ablation of our three-step Brain2Qwerty model (Conv+Trans+Language Model), for both\\nhand-error-rate (HER) and character-error-rate (CER). Each point represents the average score of a single participant.\\nStatistical significance is denoted with p < 0.05 (*), p < 0.01 (**), and p < 0.001 (***).\\nconfirm that the present protocol leads to the expected brain responses to key presses (Pinet and Nozari,\\n2020).\\n2.2\\nBrain2Qwerty Performance\\nWe next trained Brain2Qwerty, a new deep learning architecture, to decode individual characters from\\nthese M/EEG signals (see Methods in section 4.2) and evaluated both the hands-error-rate (HER) and the\\ncharacter-error-rate (CER). Brain2Qwerty achieves a CER of 32±0.6% with MEG and 67±1.5% with EEG.\\nThis performance reflects a substantial difference across recording devices (p<10−8). The best and worst\\nEEG subjects reach a CER of 61±2.0% across sentences and 71±2.3%, respectively. Similarly, the best and\\nworst MEG subjects reach a CER of 19±1.1% and 45±1.2%, respectively.\\n2.3\\nComparing Brain2Qwerty to Baseline Models\\nHow does Brain2Qwerty perform in comparison to classic baseline architectures? To address this issue we\\ntrained a linear model as well as EEGNet – a popular architecture used in BCIs (Lawhern et al., 2018) – with\\nthe same approach, and compared their decoding performance to Brain2Qwerty’s with a Wilcoxon test across\\nsubjects (Fig. 2 E-H). EEGNet outperforms the linear model on both HER (p=0.008) and CER (p<10−4)\\nfor MEG – although only on HER for EEG (p=0.03). However, EEGNet remains less effective than our\\nmodel, which achieves, in comparison, a 1.14-fold improvement in CER with EEG (p<10−5) and a 2.25-fold\\nimprovement for MEG (p<10−6), respectively.\\n2.4\\nBrain2Qwerty Ablations\\nTo validate our design choices, we then retrained different ablated versions of our model. Specifically, we\\nre-trained and evaluated (i) the Convolutional Module (i.e. no transformer, no language model) and (ii)\\nthe Conv+Transformer (i.e. without Language Model) with the same hyperparameters. The Convolutional\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 3}, page_content='Module alone outperforms EEGNet both on EEG (HER: p=0.009, CER: p=0.03) and MEG (HER: p<10−5,\\nCER: p<10−6). Adding the transformer only appears beneficial to CER, both for EEG (p<10−4) and MEG\\n(p<10−6). Finally, the use of a Language Model module leads to an additional improvement of the CER of\\nEEG (p<10−5) and MEG (p<10−6). Overall, these results show that the sentence-level contextualization\\nprovided by the transformer together with the leverage of natural language’s statistical regularities effectively\\nimproves the decoding of individual characters.\\n0\\n20\\n40\\n60\\n80\\n100\\nSorted Sentences\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nCER\\nA\\nBest MEG Subject\\n0\\n20\\n40\\n60\\n80\\n100\\nSorted Sentences\\nMedian MEG Subject\\n0\\n20\\n40\\n60\\n80\\n100\\nSorted Sentences\\nWorst MEG Subject\\nB\\nTrue:\\nlas teorias reducen los numeros\\nBest subject:\\nlas teorias reducen los numeros\\nMedian subject:\\nlas teorias exigen los hombros\\nWorst subject:\\nlas rancias revisen los numerad\\nTrue:\\nla estadistica sigue la distribucion\\nBest subject:\\nla estadistica sigue la distribucion\\nMedian subject:\\nstamistosa sigue la distribucion\\nWorst subject:\\nla estadistica figura de petrilla lo\\nFigure 3 Sentence-level performance for Best, Median and Worst MEG subjects.\\nA. Character-error-rate for three representative subjects. Each dot represents a unique sentence, with error bars\\nindicating the standard error of the mean across repetitions. White dots corresponds to the sentences displayed below.\\nB. Decoding predictions for two sentences. Several splitting seeds were used to obtain the predictions across sentences.\\n2.5\\nAnalyses of Decoded Sentences\\nThe CER of all sentences for three representative participants recorded with MEG along with two example\\nsentences from these subjects are displayed in Fig. 3. More decoding examples show that several sentences can\\nbe perfectly decoded for MEG (Tab. 1, right). Interestingly, some of these examples show that Brain2Qwerty’s\\nlanguage model can correct the typographical errors of the participant. For example, el beneficio supera\\nlos riesgos was perfectly decoded, even though the participants typed: ek benefucui syoera kis ruesgis.\\nIn comparison, the poor EEG decoding (Tab. 1, left) rarely leads to comprehensible text. Consistent with the\\nstatistical effects reported earlier, the examples in Tab. 2 highlight the impact of each module of our model,\\nwhich together leads to perfect decoding after the language model.\\n2.6\\nImpact of Word Type and Frequency\\nTo test whether Brain2Qwerty decodes words irrespectively of their grammatical type, we evaluated the CER\\nfor each part-of-speech (POS) categories separately (Fig. 4A). All POS categories are significantly better\\ndecoded than chance, with determiners exhibiting a remarkably low CER (17±1.9%). This phenomenon\\nmay be due to two factors: their short length and their high frequency. To formally test this hypothesis, we\\nfirst analyzed the impact of word frequency on CER (Fig. 4B). The results confirm that frequent words are\\nbetter decoded than rare words (p=10−7). Interestingly, we verify that the words absent from the training set\\n(out-of-vocabulary, OOV) can also be decoded, although with a relatively poor CER (68±2.1%). Note that\\nthis may be due to the fact that on a random partition of train/validation/test splits, OOV words tend to be\\nrare words.\\nSecond, we evaluated whether the frequency of each character also impacts decoding. The results show a\\nsignificant correlation between character frequency and decoding accuracy: R=0.85, p < 10−8 (Fig. 4C).\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 4}, page_content='EEG\\nRead:\\nla ciencia de la idea rompe la vision\\nTyped:\\nla ciencia de la idea rompe la bision\\nDecode:\\nla ciencia de la idea las mas de esos\\nRead:\\nel procesador ejecuta la instruccion\\nTyped:\\nordenador ejecuta la instruccion\\nDecode:\\nlas corrida perita la instruccion\\nRead:\\nla presencia de los tipos impone los retos\\nType:\\nla presencia de los tipos impone los retos\\nDecode:\\nla declarada de los celos eran a los actos\\nMEG\\nRead:\\nla silla ocasiona las lesiones\\nType:\\nla silla ocasioma las lesiomes\\nDecode: la silla ocasiona las lesiones\\nRead:\\nlas teorias reducen los numeros\\nType:\\nlas teorias reducen los numeros\\nDecode: las teorias reducen los numeros\\nRead:\\nel beneficio supera los riesgos\\nType:\\nek benefucui syoera kis ruesgis\\nDecode: el beneficio supera los riesgos\\nTable 1 Examples of best-decoded sentences across subjects for EEG (left) and MEG (right) data.\\nCorrect characters are highlighted in blue, mistakes in red, and typing errors underlined. Note that correct vs incorrect\\nspaces are not visualized here.\\nRead:\\nlas teorias reducen los numeros\\nTyped:\\nlas teorias reducen los numeros\\nConv:\\nlas tangpas re udnndl slindiis\\nConv+Trans:\\nlas geoolas redicen los numeios\\nBrain2Qwerty:\\nlas teorias reducen los numeros\\nRead:\\nel beneficio supera los riesgos\\nTyped:\\nel bemeficio supera los riesfos\\nConv:\\nel gefedisio suiera noa riestii\\nConv+Trans:\\nel geneficon cupera los riesgoo\\nBrain2Qwerty:\\nel beneficio supera los riesgos\\nTable 2 Example of decoded sentences across ablations for MEG data. Color coding identical to Tab. 1.\\nRare characters, such as \"z,\" \"k,\" and \"w\" in Spanish, are not decoded above chance level but only account\\nfor 0.08%, 0.08%, and 0.05% of the characters in our sentences. These results suggest that the number of\\nrepetitions (of words and characters) encountered during training directly affects performance.\\nTo confirm this, we explore how decoding performance scales with the amount of data. For this, we re-trained\\nour model on uniformly sampled subsets of the training set (Fig. 4D). Our results show that CER decreases\\nas a function of the amount of training data: R=0.93, p<10−7.\\nADJ\\nADP\\nDET\\nNOUN\\n VERB\\n0.0\\n0.4\\n0.8\\nCER\\n***\\n***\\n***\\n***\\n***\\nA\\nOOV\\nRare\\nFrequent\\nWord Frequency Quartile\\n0.0\\n0.4\\n0.8\\n**\\n***\\n***\\n***\\n***\\nB\\n0.0\\n0.5\\n1.0\\nNormalized Frequency\\n0.0\\n0.4\\n0.8\\nspace\\na\\nb\\nc\\nd\\ne\\nh\\ni\\nj\\nl\\nm\\nn\\no\\np\\nr\\ns\\nt\\nu\\nv\\nx\\ny\\nC\\n0\\n5\\n10\\n15\\n20\\nTotal time (h)\\n0.2\\n0.4\\n0.6\\n0.8\\nD\\nFigure 4 Analysis of character- and word-level performance.\\nThe results presented are specific to MEG data processed using the Conv+Trans model. A. Character-error-rate (CER)\\nis evaluated across different part-of-speech categories to evaluate how performance varies across adjectives (ADJ),\\nnouns, verbs, determiners (DET), and prepositions (ADP). B. CER as a function of word frequency. Out-of-vocabulary\\n(OOV) decoding is used to test whether Brain2Qwerty can decode words absent from the training set. C. CER as a\\nfunction of character frequency. D. CER as a function of recording time included in the training set.\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 5}, page_content='Figure 5 Impact of keyboard layout and typing errors.\\nThe results presented are specific to MEG data processed using the Conv+Trans model. A. Keyboard Distance Effect.\\nConfusion rate is analyzed against normalized keyboard distance. B. Clustering Analysis. K-means clustering of the\\nmodel embeddings with 2 (top) and 10 (bottom) clusters respectively. C. Keypress Intervals Analysis. Comparison\\nof keypress interval for correct keystrokes and typing errors, focusing on both preceding and subsequent characters.\\nThe sum of the two intervals is displayed. D. Typing Mistakes Performance Differences. Performance comparison for\\ncorrect characters versus typing errors using the Conv+Trans model (left) and the Conv model (right).\\n2.7\\nImpact of Keyboard Layout\\nIf Brain2Qwerty relies on brain activity from the motor cortex (as opposed to some amodal representations of\\nlanguage), then we expect its decoding errors to relate to the specific layout of the QWERTY keyboard. To\\ntest this hypothesis, we evaluated the confusion patterns of incorrectly predicted characters by analyzing the\\nkeyboard distance between decoded and actual key presses. The results show a strong Pearson correlation\\nbetween physical distance and confusion rate: R=0.73, p=0.02 (Fig. 5A). To complement this analysis, we\\nfurther perform a clustering of the last-layer embeddings of the convolutional module using scikit-learn’s\\nK-means clustering algorithm. When trained on two clusters, this unsupervised model fully separates the\\nleft-hand and right-hand keys. When using up to 10 clusters, the resulting partition remains consistent with\\nthe keyboard layout (Fig. 5B). This shows that the spatial layout of the keyboard is well encoded in the\\nhigh-dimensional representations learned by our model. Overall, these results show that the decoder’s mistakes\\ntend to be confused with the keys that are physically close to the target letter on the QWERTY keyboard,\\nsuggesting that the decoder primarily relies on motor representations.\\n2.8\\nImpact of Typing Errors\\nOur protocol does not allow participants to correct their mistakes. Typing errors account for 3.9% of the\\nkeystrokes and are present in 65% of the sentences. Furthermore, they are associated with a specific behavior\\n(Fig. 5C): the time taken to type a character – as measured by the inter-key interval – doubles between\\ncorrectly- (50±7 ms) and incorrectly-typed characters (114±12 ms, p=10−7). This well-known phenomenon\\n(Logan and Crump, 2010) likely reflects hesitation or monitoring of mistakes. To evaluate the impact of\\ntyping error on decoding performance, we separately evaluated CER for correctly- and incorrectly-typed\\ncharacters (Fig. 5D). The results show that with our Conv+Trans model, correctly-typed characters lead to a\\nbetter CER (38%) than incorrectly-typed characters (65%, p=10−7). This result, however, may be partly\\ndriven by the contextualization enabled by the transformer. To minimize the impact of sentence context on\\nthis error analysis, we thus evaluate the performance of the Convolutional Module (Fig. 5D, right). Again,\\ncorrectly-typed characters leads to a better CER (52%) than incorrectly-typed characters (71%, p=10−7).\\nThis result suggests that decoding performance diminishes when motor processes are inaccurately executed.\\n3\\nDiscussion\\nThe present study introduces a new method to decode the production of sentences from non-invasive brain\\nrecordings. With MEG, our Brain2Qwerty model achieves a character-error-rate (CER) of 32±0.6% on average\\nacross subjects, with the best-performing participants reaching a CER as low as 19%. Our analyses indicate\\nthat this decoding benefits from two main factors. First, the use of MEG signals instead of EEG signals\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 6}, page_content='resulted in a two-fold improvement. Second, our deep learning architecture, combined with a pretrained\\ncharacter-level language model, substantially outperforms standard models.\\nThis work directly stems from the recent progress in decoding natural language from non-invasive recordings\\nof brain activity. In particular, Défossez et al. (2023) showed that the perception of natural speech segments\\ncould be decoded, from MEG signals, with up to 41% top-10 accuracy (chance level=0.1%). Similarly, Tang\\net al. (2023) showed that the meaning of perceived sentences could be decoded from functional Magnetic\\nResonance Imaging (fMRI). Our Brain2Qwerty model shares several elements with these approaches, notably\\nthrough the use of both a subject layer (Défossez et al., 2023) and the use of a language model (Tang et al.,\\n2023), although here restricted to a pretrained 9-gram character-level model. However, these two studies,\\nwhich focus on decoding the perception of language rather than its production, remained limited in their\\ndownstream clinical applications.\\nStudies that directly decode text production from non-invasive recordings remain rare. For example, Crell\\nand Müller-Putz (2024) employed EEG to decode only 10 letters and achieved a character-error-rate of\\n75.8%, substantially higher than our 68% in the EEG setting with 29 characters. Similarly, EEG-based BCI\\nbenchmarks currently emphasize the constraints of poor signal quality and the variability across subjects\\n(Chevallier et al., 2024). Our EEG findings are consistent with these observations. Beyond its mere metric\\nperformance, our approach is more efficient than traditional protocols used in non-invasive BCIs, such as the\\nP300-speller (Marchetti and Priftis, 2014), SSVEP (Cheng et al., 2002), and fMRI-localizer (Owen et al.,\\n2006). These methods have historically relied on handcrafted signal processing and shallow classifiers. In\\ncontrast, our approach is based on a task that is comparatively easier to use.\\nWhile the decoding performance of Brain2Qwerty narrows the gap between non-invasive and invasive BCIs,\\nthis gap remains significant. In particular, for speech decoding, Metzger et al. (2023) achieved a rate of\\n79 words per minute and reported a CER of 15.2% on a dataset with 372 unique words, a vocabulary size\\ncomparable to ours. Willett et al. (2021) demonstrated typing speeds of 90 characters per minute with a\\nCER below 6% and an offline CER under 1% when using a correction model. Both approaches rely on\\nintracortical setups and require extensive recording sessions (11 hours per participant for Willett et al. (2021)).\\nConsequently an important research avenue will be to scale and adapt those tasks for MEG experiments.\\nSeveral challenges remain to be solved before the present method could be adapted to clinical applications.\\nFirst, our model does not operate in real time. In particular, the transformer and language model here operate\\nat the sentence level and thus require the trial to conclude before an output can be produced. In addition,\\nthe input of Brain2Qwerty requires the MEG segments to be aligned to keystrokes. Overall, a real-time\\narchitecture, akin to what is done with electromyography (Sivakumar et al., 2024) and speech recognition\\n(Défossez et al., 2023), remains necessary to make the present proof-of-concept applicable in real time.\\nSecond, our study was conducted exclusively with healthy participants and with a strictly supervised model.\\nTraining indeed requires knowing both the timing and identity of each character. While this setup may\\nbe suitable for patients with neurodegenerative conditions who may still possess motor abilities, it is not\\napplicable to locked-in individuals, who are completely unable to perform a typing task on a keyboard.\\nAddressing this challenge may involve either adapting our typing task into an imagination task or designing\\nAI systems capable of robust generalization across participants (Scotti et al., 2024).\\nFinally, while MEG outperforms EEG, current MEG systems, including the one used in the present study, are\\nnot wearable. This, however, may be resolved by the development of new MEG sensors based on optically\\npumped magnetometers (OPMs) (Shah and Wakai, 2013; Schofield et al., 2022; Brickwedde et al., 2024).\\nOverall, the present results serve as a stepping stone toward developing safer and more accessible non-invasive\\nbrain-computer interfaces, ultimately enabling solutions for individuals who have partially or completely lost\\nthe ability to communicate.\\n4\\nMethods\\nWe aim to decode language production from non-invasive brain recordings. To achieve this, participants\\ntyped sentences on a keyboard while their brain activity was recorded using EEG or MEG. These two devices\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 7}, page_content='measure neural activity at a millisecond level, with EEG capturing electric fields and MEG detecting magnetic\\nfields that are both generated by cortical neurons and recorded from sensors distributed across the scalp.\\n4.1\\nExperimental Protocol\\nCohort.\\nWe recruited 35 healthly adult volunteers to participate in our study at the Basque Center on\\nCognition, Brain and Language (BCBL) in Spain. This group was composed of 23% of men and 77% of women\\nwith an average age of 31.6±5.2 years. All participants were right-handed and skilled at typing. Participants\\nhad to type words they were hearing on a keyboard covered by a cardboard box. They were selected if their\\ntyping accuracy was above or equal to 80%. They are all native Spanish speakers with no declared prior\\nhistory of neurological or psychiatric disorders. Their brain activity was recorded with either EEG or MEG\\nfor 0.88±0.02 and 0.93±0.01 hours respectively, amounting to a total of 17.7 and 21.5 hours of typing. Five\\nparticipants took part in both EEG and MEG sessions. One participant was excluded from the MEG study\\ndue to the presence of a metallic component during the recording. Participants gave their informed consent\\nand were compensated 12 euros per hour for their participation. This study was approved by the local ethics\\ncommittee. The same dataset is used to investigate the underlying neural mechanisms driving this task in a\\ncompanion paper (Zhang et al., 2025).\\nDevices.\\nThe MEG system is a Megin system with 306 channels (102 magneto-meters and 204 planar\\ngradiometers) recording at a sampling rate of 1 kHz, with an online high-pass filter set at 0.1 Hz and a low-pass\\nfilter at 330 Hz. The EEG system is an actiCAP slim from BrainVision1, with 64 channels (61 EEG channels\\nand 3 ocular channels), a BRAINAMP DC amplifier, and sampled at 1 kHz with an online high-pass filter set\\nat 0.02 Hz.\\nStandard keyboards contain electronic and metallic parts that generate artifacts in the MEG. Consequently,\\nwe used a custom Magnetic-Resonance-compatible QWERTY keyboard from HybridMojo (LLC), and further\\nmodified it to replace the standard metallic springs with non-ferromagnetic silver-spring mechanisms.\\nTask.\\nParticipants were seated in front of a projected screen (100 cm for MEG and 70 cm for EEG away\\nfrom their eyes), and with our custom keyboard placed on a stable platform. The distance between M/EEG\\nsensors and the keyboard was 70 cm. This setup ensured participants could type in a natural position. Each\\ntrial consisted of three steps: read, wait, type. First, a sentence was presented on the screen, with a rapid\\nserial visual presentation protocol (RSVP; i.e. one word at a time). Each word was presented in a black font,\\nin all upper-case, on a 50% gray background for a random duration between 465 and 665 ms without intervals\\nbetween words. Second, after the disappearance of the last word of each sentence, a black fixation cross was\\ndisplayed on the screen for 1.5 seconds. Third, the disappearance of the fixation cross signaled the start of\\nthe typing phase. No letters were presented on the screen during typing. Nevertheless, we added minimal\\nvisual feedback: a small black square at the center of the screen rotated clockwise by 10 degrees on every\\nkeystroke. This feedback ensured that eye movements were not correlated with linguistic features, as it is\\nusually the case in left-to-right reading. Each session consisted of two blocks of 64 sentences each. The first\\nfour sentences of each session were training sentences and were different from the 128 unique sentences in the\\nprotocol. During the first two training sentences, participants received visual feedback while typing. The\\nother two sentences were used to train them on the task (typing with minimal visual feedback).\\nInstruction and stimuli.\\nParticipants were instructed to type the sentence that was presented in RSVP\\nas accurately as possible without using backspaces to correct errors and while fixating on the center of the\\nscreen. To avoid using diacritic marks that occur in Spanish (e.g., é, á, í, ó, ú, ü, and ñ), all words were\\npresented in upper case, and participants were instructed to think of writing in upper case, and without\\naccents. Participants pressed the return key at the end of the trial. Each session consisted of 128 unique\\nSpanish sentences. All sentences were declarative Spanish sentences that contained between 5 and 8 words.\\nThey consisted of determiners, nouns, adjectives, prepositions and verbs. This led to a total of 4K sentences\\nand 146K characters across participants for EEG, and 5.1K sentences and 193K characters for MEG.\\n1https://brainvision.com/products/acticap-slim-acticap-snap/\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 8}, page_content='MEG and EEG preprocessing.\\nIn our pipeline, the EEG and MEG recordings were bandpass filtered between\\n0.1 and 20 Hz, and resampled to 50 Hz, using MNE-Python’s default parameters (Gramfort et al., 2014).\\nThese continuous recordings were then segmented into 0.5 s time windows around each key press from -0.2 s to\\n+0.3 s. We applied baseline correction by subtracting the average channel-wise value in the (-0.2, 0) interval\\nfrom each window. To ensure that the data was on the same scale, a RobustScaler was applied across\\ntime from scikit-learn (Pedregosa et al., 2018) followed by a clamping operation. The scaler removes the\\nmedian and scales the data according to the interquantile range.\\nText preprocessing.\\nWe removed sentences that contained strictly more than 10 typographical errors (less\\nthan 5% for both EEG and MEG). To determine whether the participants performed typographical errors, we\\nfirst used the SequenceMatcher from difflib\\n2 to align the typed sentence to the original sentence using\\na Gestalt pattern matching algorithm (Ratcliff and Metzener, 1988) that determines the minimal number of\\ncharacter-edits. This approach yields, for each keystroke event, two variables: the key pressed and the key\\nthat should have been pressed (target). Unless stated otherwise, all analyses are based on the target key. On\\naverage, participants typed 152.0±3.2 characters per minute, leading to an average sentence production time\\nof 5.7±0.2 s. 3.6 ± 0.7% of the keystrokes were typographical errors.\\nTrain/validation/test splits.\\nTo ensure that our AI model does not memorize sentences, we divided the unique\\nsentences into train (80%), validation (10%) and test splits (10%). As sentences may be similar (e.g. same\\nsentence with an additional component), we adopted a maximally diverse splitting strategy by implementing\\na custom data splitter based on sklearn’s AgglomerativeClustering (Pedregosa et al., 2018). This\\nmethod clusters unique sentences into groups of similar data points, where similarity is determined using\\nTerm Frequency-Inverse Document Frequency (TF-IDF) (Jones, 1972) cosine similarity with a threshold of\\n0.5 (i.e., sentences with a similarity score above 0.5 are grouped in the same split). These clusters are then\\nassigned to the training, validation and testing splits with relative ratios of 80/10/10. This approach prevents\\nthe model from artificially improving test performance by memorizing similar sentences across participants.\\nThroughout the study, we presented our results using a fixed splitting seed, except in Fig. 3 where we\\nevaluated performance across sentences. For this figure, we aggregated results from multiple models with\\ndifferent splitting seeds to ensure broader sentence coverage, similar to a cross-validation approach.\\n4.2\\nDecoder\\nThe goal for our decoding model is to predict each keystroke based on 0.5 s windows of M/EEG signals.\\nFormally, the objective is to learn a mapping from brain recordings to class probabilities: f : Rs×t →[0,1]C,\\nwhere s represents the number of M/EEG sensors, t denotes the number of M/EEG time samples in the\\nwindow, and C is the number of available keys. We consider C = 29 distinct classes, which include all the\\nletters of the Latin alphabet as well as three special classes: one for space, another for numbers, and the last\\nfor all other special characters.\\n4.2.1\\nArchitecture\\nOur Brain2Qwerty model is composed of three successive modules (Fig. 1).\\nConvolutional Module.\\nThe first building block is a modified convolutional model originally introduced by\\nDéfossez et al. (2023). This model consists of four main parts. The first component employs a spatial attention\\nmechanism to encode the relative positions of the sensors. The second introduces a subject-specific linear\\nlayer to account for differences between subjects. The third component is a convolutional neural network\\narchitecture, comprising 8 sequential blocks that employ a kernel size of 3 and a dilation period of 3, and\\nincorporate skip connections, dropout regularization, and GELU activation functions. Finally, the temporal\\ndimension is pooled with a single-head self-attention layer. Thus, for each window X ∈Rs×t, the CNN outputs\\nz ∈Rh, with h = 2, 048. For clarity, this module will hereafter be referred to as Convolutional Module (Conv).\\n2https://docs.python.org/3/library/difflib.html\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 9}, page_content='Transformer Module.\\nThe outputs of the Convolutional Module are then input to a transformer Module\\n(Trans). The receptive field of the transformer is restricted to a unique sentence: Z ∈Rn×h. The transformer\\nis used to refine the keystroke predictions by exploiting contextual information and consists of 4 layers with 2\\nattention heads per layer, maintaining consistent input and output dimensions. Finally, a linear layer projects\\nthe transformer’s outputs to obtain the logits of each character ˜Y ∈Rn×C.\\nLanguage Model.\\nFinally, the output of the transformer ˜Y is input to a language model, so as to leverage the\\nstatistical regularities of natural language. For this, we used a 9-gram character-level model constructed using\\nthe KenLM library (Heafield, 2011) and pretrained on the Spanish Wikipedia Corpus (Wikimedia, 2005).\\nThis library optimizes both speed and memory efficiency by employing a prefix tree structure. At inference\\ntime, the language model is input with a sequence of predicted characters, and causally predicts the most\\nlikely next character given the preceding predicted ones.\\nFormally, let i ∈[1, 2, ..., n] denote the position of the character in a sequence of up to n characters and m = 9\\nbe the order of the m-gram model. The probability of the next character ˆci given its preceding predictions\\nˆci−m, . . . , ˆci−1 is estimated as a weighted combination between the transformer’s logits and the probabilities\\nof the language model:\\nP(ˆci|ˆci−m, . . . , ˆci−1) = Ptrans(ˆci) + α · Plm(ˆci|ˆci−m, . . . , ˆci−1),\\nwhere Ptrans(ˆci) = log\\n\\x10\\nsoftmax\\nh\\n( ˜Y)\\ni\\ni\\n\\x11\\nrepresents the contribution from the core model, Plm(ˆci|ˆci−m, . . . , ˆci−1)\\nrepresents the probabilities from the language model, and α is the language model weight. We use a beam\\nsearch of size 30 and a language model weight of 5 found using a grid search, ensuring a tradeoff between\\ninference time and decoding accuracy. This language modeling module outputs a sequence of n characters that\\naim to regularize the predictions of the transformer with the statistics of natural language. Brain2Qwerty’s\\nfinal prediction is denoted as ˆY ∈Rn×C.\\n4.2.2\\nTraining\\nThe Convolutional and Transformer modules are trained jointly with an unweighted cross-entropy loss in an\\nend-to-end manner across all subjects, with the same hyperparameters for EEG and MEG recordings. This\\nleads to a total of ∼400M parameters (258M for Conv, 138M for Trans). The model is trained for 100 epochs\\nwith a batch size of 128 using the AdamW optimizer (Loshchilov and Hutter, 2019) with early stopping. We\\nuse the OneCycleLR scheduler (Smith and Topin, 2018) (weight decay=10−4; pct_start=0.1), warming up\\nthe learning rate to 10−4 over the first 10 epochs then decaying linearly. Training was conducted on a single\\nNVIDIA Tesla V100 Volta GPU with 32 GB of memory. The total runtime for training one model is ∼12\\nhours.\\n4.2.3\\nEvaluation\\nHand Error Rate (HER).\\nFor analysis purposes and comparison with the classic BCI literature (e.g. Lebedev\\nand Nicolelis (2006)), we first consider HER. This metric estimates whether the target and the predicted\\ncharacters correspond to the same left/right-hand split of the keyboard. Specifically, keys to the left of\\nY, H, and B are assigned to the left-hand category, while those to the right (including Y, H, and B) are\\nassigned to the right-hand category. For this evaluation, special characters, numbers and space are excluded,\\nas participants may use both hands to type them.\\nCharacter-error-rate (CER).\\nCER is based on the Levenshtein distance which quantifies the minimum number\\nof single-character edits required to transform the predicted sequence of keystrokes to the target sentence. A\\nCER of 0 indicates perfect character-level accuracy. The formula for CER is given by CER = (s + d + a) × 1\\nn,\\nwhere s, d and a stand for substitutions, deletions, and additions in the n-character sentence, respectively.\\nUnless stated otherwise, we compute the CER at the sentence level using the Levenshtein3 python library,\\nand report the average CER across sentences.\\n3https://pypi.org/project/Levenshtein/\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 10}, page_content='4.2.4\\nModel comparison\\nStatistics.\\nFor statistical comparisons, we employed the non-parametric tests provided by the scipy\\npackage (Virtanen et al., 2020). For comparison across models within the same subjects, we used the Wilcoxon\\ntest. For comparison across subjects (e.g. for EEG versus MEG), we used the Mann Whitney U test. For\\ntime-course decoding, we further applied a false-discovery-rate (FDR) correction for multiple comparisons\\nacross time samples.\\nBaseline Models.\\nWe consider two baseline models. The first one is a linear model implemented using the\\nRidgeClassifierCV function from the scikit-learn library (Pedregosa et al., 2018), which was trained to\\npredict characters from a single time sample of recording for each subject separately. The regularization\\nparameter α was selected through a nested cross-validation using a grid search logarithmic spanning from\\n10−2 to 108. This operation was repeated for each time sample between -0.5 and 0.5 seconds relative to the\\ncharacter onset.\\nAs a second baseline, we need a model which uses the same setup as Brain2Qwerty, i.e. where all subjects\\nare trained collectively and the temporal dimension is collapsed. We used EEGNet (Lawhern et al., 2018), a\\nhighly parameter-efficient model classically used in BCIs. We trained EEGNet with the same approach as\\nour model. For our experiments, EEGNet is configured with a depth of 6 and a dropout rate of 0.3, which\\nwere selected based on a grid search over the validation set. EEGNet does not incorporate a subject-specific\\nlinear layer, which may compromise its performance in this particular setup where all subjects were trained\\ncollectively. To compute the chance level while accounting for the imbalance across characters, we evaluated\\nthe performance of a dummy model that always predicts the most frequent character.\\nCompanion paper.\\nWe explore how the brain produce a hierarchy of language representations in a companion\\npaper (Zhang et al., 2025). Note that Fig. 1 (left) and Fig. 2B are shared between these two studies.\\n5\\nAcknowledgments\\nThe authors would like to thank Maite Kaltzakorta, Manex Lete, Jessi Jacobsen, Daniel Nieto, Jone Iraeta,\\nAraitz Garnika, Jaione Bengoetxea, Natalia Louleli, Naroa Miralles, Eñaut Zeberio, Craig Richter, Amets\\nEsnal, and Olatz Andonegui, as well as Abhishek Charnalia and Pierre-Louis Xech for their critical help.\\nThis research is supported by the Basque Government through the BERC 2022-2025 program and Funded\\nby the Spanish State Research Agency through BCBL Severo Ochoa excellence accreditation CEX2020-\\n001010/AEI/10.13039/501100011033. Parts of this research were carried within the European Union’s Horizon\\n2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 945304 -\\nCofund AI4theSciences hosted by PSL University.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 11}, page_content='References\\nR. Abiri, S. Borhani, E. Sellers, Y. Jiang, and X. Zhao.\\nA comprehensive review of eeg-based brain-computer\\ninterface paradigms. Journal of Neural Engineering, 16(1):011001, Feb 2019. doi: 10.1088/1741-2552/aaf12e.\\nhttps://doi.org/10.1088/1741-2552/aaf12e.\\nM. Angrick, C. Herff, E. Mugler, M. C. Tate, M. W. Slutzky, D. J. Krusienski, and T. Schultz. Speech synthesis from\\necog using densely connected 3d convolutional neural networks. Journal of Neural Engineering, 16(3):036019, Jun\\n2019. doi: 10.1088/1741-2552/ab0c59. https://doi.org/10.1088/1741-2552/ab0c59.\\nG. K. Anumanchipalli, J. Chartier, and E. F. Chang. Speech synthesis from neural decoding of spoken sentences. Nature,\\n568:493–498, 2019. doi: 10.1038/s41586-019-1119-1. https://doi.org/10.1038/s41586-019-1119-1.\\nS. Baillet. Magnetoencephalography for brain electrophysiology and imaging. Nature Neuroscience, 20:327–339, 2017.\\ndoi: 10.1038/nn.4504. https://doi.org/10.1038/nn.4504.\\nG. Baranauskas. What limits the performance of current invasive brain machine interfaces? Frontiers in Systems\\nNeuroscience, 8:68, April 2014.\\nY. G. Bodien, J. Allanson, P. Cardone, et al. Cognitive motor dissociation in disorders of consciousness. New England\\nJournal of Medicine, 391(7):598–608, Aug 2024. doi: 10.1056/NEJMoa2400645. https://doi.org/10.1056/\\nNEJMoa2400645.\\nM. Brickwedde, P. Anders, A. A. Kühn, R. Lofredi, M. Holtkamp, A. M. Kaindl, T. Grent-’t Jong, P. Krüger, T. Sander,\\nand P. J. Uhlhaas. Applications of opm-meg for translational neuroscience: a perspective. Translational Psychiatry,\\n14(1):341, Aug 2024. doi: 10.1038/s41398-024-03047-y. https://doi.org/10.1038/s41398-024-03047-y.\\nA. Bullard, B. Hutchison, J. Lee, C. Chestek, and P. Patil. Estimating risk for future intracranial, fully implanted,\\nmodular neuroprosthetic systems: A systematic review of hardware complications in clinical deep brain stimulation\\nand experimental human intracortical arrays. Neuromodulation: Technology at the Neural Interface, 23(4):411–426,\\n2020. ISSN 1094-7159. doi: 10.1111/ner.13069. https://doi.org/10.1111/ner.13069.\\nN. S. Card, M. Wairagkar, C. Iacobacci, X. Hou, T. Singer-Clark, F. R. Willett, E. M. Kunz, C. Fan, M. Vahdati Nia,\\nD. R. Deo, A. Srinivasan, E. Y. Choi, M. F. Glasser, L. R. Hochberg, J. M. Henderson, K. Shahlaie, S. D. Stavisky,\\nand D. M. Brandman. An accurate and rapidly calibrating speech neuroprosthesis. New England Journal of Medicine,\\n391(7):609–618, Aug 2024. doi: 10.1056/NEJMoa2314132. https://doi.org/10.1056/NEJMoa2314132.\\nM. Cheng, X. Gao, S. Gao, and D. Xu. Design and implementation of a brain-computer interface with high transfer\\nrates. IEEE Transactions on Biomedical Engineering, 49(10):1181–1186, Oct 2002. doi: 10.1109/tbme.2002.803536.\\nhttps://doi.org/10.1109/tbme.2002.803536.\\nS. Chevallier, I. Carrara, B. Aristimunha, P. Guetschel, S. Sedlar, B. Lopes, S. Velut, S. Khazem, and T. Moreau. The\\nlargest eeg-based bci reproducibility study for open science: the moabb benchmark, 2024. https://arxiv.org/\\nabs/2404.15319.\\nJ. E. Chung, H. R. Joo, J. L. Fan, D. F. Liu, A. H. Barnett, S. Chen, C. Geaghan-Breiner, M. P. Karlsson, M. Karlsson,\\nK. Y. Lee, H. Liang, J. F. Magland, J. A. Pebbles, A. C. Tooker, L. F. Greengard, V. M. Tolosa, and L. M. Frank. High-\\ndensity, long-lasting, and multi-region electrophysiological recordings using polymer electrode arrays. Neuron, 101(1):\\n21–31.e5, Jan 2019. doi: 10.1016/j.neuron.2018.11.002. https://doi.org/10.1016/j.neuron.2018.11.002.\\nJ. Claassen, K. Doyle, A. Matory, C. Couch, K. Burger, A. Velazquez, J. Okonkwo, J.-R. King, S. Park, S. Agarwal,\\nD. Roh, M. Megjhani, A. Eliseyev, E. Connolly, and B. Rohaut. Detection of brain activation in unresponsive patients\\nwith acute brain injury. New England Journal of Medicine, 380(26):2497–2505, 2019. doi: 10.1056/NEJMoa1812757.\\nhttps://doi.org/10.1056/NEJMoa1812757.\\nM. Crell and G. Müller-Putz. Handwritten character classification from eeg through continuous kinematic decoding.\\nComputers in Biology and Medicine, 182:109132, 2024. ISSN 0010-4825. doi: 10.1016/j.compbiomed.2024.109132.\\nhttps://doi.org/10.1016/j.compbiomed.2024.109132.\\nT. H. Donner, M. Siegel, P. Fries, and A. K. Engel. Buildup of choice-predictive activity in human motor cortex\\nduring perceptual decision making. Current Biology, 19(18):1581–1585, Sep 2009. doi: 10.1016/j.cub.2009.07.066.\\nhttps://doi.org/10.1016/j.cub.2009.07.066.\\nA. Défossez, C. Caucheteux, J. Rapin, O. Kabeli, and J.-R. King. Decoding speech perception from non-invasive\\nbrain recordings. Nature Machine Intelligence, 5:1097–1107, 2023. doi: 10.1038/s42256-023-00714-5. https:\\n//doi.org/10.1038/s42256-023-00714-5.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 12}, page_content='Z. Fekete, A. Zátonyi, A. Kaszás, et al. Transparent neural interfaces: challenges and solutions of microengineered\\nmultimodal implants designed to measure intact neuronal populations using high-resolution electrophysiology\\nand microscopy simultaneously. Microsystems & Nanoengineering, 9:66, 2023. doi: 10.1038/s41378-023-00519-x.\\nhttps://doi.org/10.1038/s41378-023-00519-x.\\nD. Goldenholz, S. Ahlfors, M. Hämäläinen, D. Sharon, M. Ishitobi, L. Vaina, and S. Stufflebeam. Mapping the\\nsignal-to-noise ratios of cortical sources in magnetoencephalography and electroencephalography. Human Brain\\nMapping, 30(4):1077–1086, 2009. doi: 10.1002/hbm.20571. https://doi.org/10.1002/hbm.20571.\\nA. Gramfort, M. Luessi, E. Larson, D. A. Engemann, D. Strohmeier, C. Brodbeck, L. Parkkonen, and M. S. Hämäläinen.\\nMne software for processing meg and eeg data. Neuroimage, 86:446–460, Feb 2014. doi: 10.1016/j.neuroimage.2013.\\n10.027. https://doi.org/10.1016/j.neuroimage.2013.10.027.\\nK. Heafield. KenLM: Faster and smaller language model queries. In Chris Callison-Burch, Philipp Koehn, Christof\\nMonz, and Omar F. Zaidan, editors, Proceedings of the Sixth Workshop on Statistical Machine Translation, pages\\n187–197, Edinburgh, Scotland, July 2011. Association for Computational Linguistics. https://aclanthology.\\norg/W11-2123/.\\nC. Herff, L. Diener, M. Angrick, E. Mugler, M. C. Tate, M. A. Goldrick, D. J. Krusienski, M. W. Slutzky, and\\nT. Schultz. Generating natural, intelligible speech from brain activity in motor, premotor, and inferior frontal\\ncortices. Frontiers in Neuroscience, 13:1267, Nov 2019. doi: 10.3389/fnins.2019.01267. https://doi.org/10.\\n3389/fnins.2019.01267.\\nL. Hochberg, D. Bacher, B. Jarosiewicz, et al. Reach and grasp by people with tetraplegia using a neurally controlled\\nrobotic arm. Nature, 485:372–375, 2012. doi: 10.1038/nature11076. https://doi.org/10.1038/nature11076.\\nM. Hämäläinen, R. Hari, R. Ilmoniemi, J. Knuutila, and O. Lounasmaa. Magnetoencephalography—theory, instru-\\nmentation, and applications to noninvasive studies of the working human brain. Reviews of Modern Physics, 65(2):\\n413–497, 1993. doi: 10.1103/RevModPhys.65.413. https://doi.org/10.1103/RevModPhys.65.413.\\nS. Jones, K. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation,\\n28(1):11–21, 1972.\\nV. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung, and B. J. Lance. Eegnet: a compact\\nconvolutional neural network for eeg-based brain-computer interfaces. Journal of Neural Engineering, 15(5):056013,\\nOct 2018. doi: 10.1088/1741-2552/aace8c. https://doi.org/10.1088/1741-2552/aace8c.\\nM. A. Lebedev and M. A. Nicolelis. Brain-machine interfaces: past, present and future. Trends in Neurosciences, 29\\n(9):536–546, Sep 2006. doi: 10.1016/j.tins.2006.07.004. https://doi.org/10.1016/j.tins.2006.07.004.\\nC. Leuthardt, D. Moran, W., and T. Mullen. Defining surgical terminology and risk for brain-computer interface\\ntechnologies. Frontiers in Neuroscience, 15:599549, 2021. doi: 10.3389/fnins.2021.599549. https://doi.org/10.\\n3389/fnins.2021.599549.\\nG. D. Logan and M. J. C. Crump. Cognitive illusions of authorship reveal hierarchical error detection in skilled\\ntypists. Science, 330(6004):683–686, 2010. doi: 10.1126/science.1190483. https://doi.org/10.1126/science.\\n1190483.\\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019. https://arxiv.org/abs/1711.05101.\\nJ. Mak and J. Wolpaw. Clinical applications of brain-computer interfaces: current state and future prospects. IEEE\\nReviews in Biomedical Engineering, 2:187–199, 2009. doi: 10.1109/RBME.2009.2035356. https://doi.org/10.\\n1109/RBME.2009.2035356.\\nM. Marchetti and K. Priftis. Effectiveness of the p3-speller in brain-computer interfaces for amyotrophic lateral\\nsclerosis patients: a systematic review and meta-analysis. Frontiers in Neuroengineering, 7:12, May 2014. doi:\\n10.3389/fneng.2014.00012. https://doi.org/10.3389/fneng.2014.00012.\\nS. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli,\\nA. Tu-Chan, K. Ganguly, and E. F. Chang. Generalizable spelling using a speech neuroprosthesis in an individual with\\nsevere limb and vocal paralysis. Nature Communications, 13(1):6510, Nov 2022. doi: 10.1038/s41467-022-33611-3.\\nhttps://doi.org/10.1038/s41467-022-33611-3.\\nS. L. Metzger, K. T. Littlejohn, A. B. Silva, D. A. Moses, M. P. Seaton, R. Wang, M. E. Dougherty, J. R. Liu,\\nP. Wu, M. A. Berger, I. Zhuravleva, A. Tu-Chan, K. Ganguly, G. K. Anumanchipalli, and E. F. Chang.\\nA\\nhigh-performance neuroprosthesis for speech decoding and avatar control. Nature, 620(7976):1037–1046, Aug 2023.\\ndoi: 10.1038/s41586-023-06443-4. https://doi.org/10.1038/s41586-023-06443-4.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 13}, page_content='D. A. Moses, S. L. Metzger, J. R. Liu, G. K. Anumanchipalli, J. G. Makin, P. F. Sun, J. Chartier, M. E. Dougherty, P. M.\\nLiu, G. M. Abrams, A. Tu-Chan, K. Ganguly, and E. F. Chang. Neuroprosthesis for decoding speech in a paralyzed\\nperson with anarthria. New England Journal of Medicine, 385(3):217–227, Jul 2021. doi: 10.1056/NEJMoa2027540.\\nhttps://doi.org/10.1056/NEJMoa2027540.\\nA. M. Owen, M. R. Coleman, M. Boly, M. H. Davis, S. Laureys, and J. D. Pickard. Detecting awareness in the\\nvegetative state. Science, 313(5792):1402, Sep 2006. doi: 10.1126/science.1130197. https://doi.org/10.1126/\\nscience.1130197.\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, A. Müller, J. Nothman,\\nG. Louppe, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,\\nand É. Duchesnay. Scikit-learn: Machine learning in python, 2018. https://arxiv.org/abs/1201.0490.\\nS. Pinet and N. Nozari. Electrophysiological correlates of monitoring in typing with and without visual feedback.\\nJournal of Cognitive Neuroscience, 32(4):603–620, April 2020. doi: 10.1162/jocn_a_01500. https://doi.org/\\n10.1162/jocn_a_01500.\\nW. Ratcliff, J. and E. Metzener, D. Pattern matching: The gestalt approach. Dr. Dobb’s Journal, page 46, July\\n1988. https://www.drdobbs.com/database/pattern-matching-the-gestalt-approach/184407970?\\npgno=5.\\nH. Schofield, E. Boto, V. Shah, R. M. Hill, J. Osborne, M. Rea, C. Doyle, N. Holmes, R. Bowtell, D. Woolger, and\\nM. J. Brookes. Quantum enabled functional neuroimaging: the why and how of magnetoencephalography using\\noptically pumped magnetometers. Contemporary Physics, 63(3):161–179, 2022. doi: 10.1080/00107514.2023.2182950.\\nhttps://doi.org/10.1080/00107514.2023.2182950.\\nS. Scotti, P., M. Tripathy, K. T. Villanueva, C., R. Kneeland, T. Chen, A. Narang, C. Santhirasegaran, J. Xu,\\nT. Naselaris, A. Norman, K., and M. Abraham, T. Mindeye2: Shared-subject models enable fmri-to-image with 1\\nhour of data, 2024. https://arxiv.org/abs/2403.11207.\\nV. K. Shah and R. T. Wakai. A compact, high performance atomic magnetometer for biomedical applications.\\nPhysics in Medicine and Biology, 58(22):8153–8161, Nov 2013.\\ndoi: 10.1088/0031-9155/58/22/8153.\\nhttps:\\n//doi.org/10.1088/0031-9155/58/22/8153.\\nV. Sivakumar, J. Seely, A. Du, R. Bittner, S., A. Berenzweig, A. Bolarinwa, A. Gramfort, and I. Mandel, M.\\nemg2qwerty: A large dataset with baselines for touch typing using surface electromyography, 2024.\\nhttps:\\n//arxiv.org/abs/2410.20081.\\nN. Smith, L. and N. Topin. Super-convergence: Very fast training of neural networks using large learning rates, 2018.\\nhttps://arxiv.org/abs/1708.07120.\\nJ. Tang, A. LeBel, S. Jain, and A. G. Huth. Semantic reconstruction of continuous language from non-invasive\\nbrain recordings. Nature Neuroscience, 26(5):858–866, May 2023. doi: 10.1038/s41593-023-01304-9. https:\\n//doi.org/10.1038/s41593-023-01304-9.\\nP. Virtanen, R. Gommers, T. Oliphant, et al. Scipy 1.0: fundamental algorithms for scientific computing in python.\\nNature Methods, 17(3):261–272, Feb 2020. doi: 10.1038/s41592-019-0686-2. http://dx.doi.org/10.1038/\\ns41592-019-0686-2.\\nM. Wairagkar, N. S. Card, T. Singer-Clark, X. Hou, C. Iacobacci, L. R. Hochberg, D. M. Brandman, and S. D. Stavisky.\\nAn instantaneous voice synthesis neuroprosthesis, Sep 2024. https://doi.org/10.1101/2024.08.14.607690.\\nbioRxiv [Preprint].\\nWikimedia. Wikimedia downloads, 2005. https://dumps.wikimedia.org.\\nF. R. Willett, D. T. Avansino, L. R. Hochberg, J. M. Henderson, and K. V. Shenoy. High-performance brain-to-\\ntext communication via handwriting. Nature, 593(7858):249–254, May 2021. doi: 10.1038/s41586-021-03506-2.\\nhttps://doi.org/10.1038/s41586-021-03506-2.\\nF. R. Willett, E. M. Kunz, C. Fan, D. T. Avansino, G. H. Wilson, E. Y. Choi, F. Kamdar, M. F. Glasser, L. R. Hochberg,\\nS. Druckmann, K. V. Shenoy, and J. M. Henderson. A high-performance speech neuroprosthesis. Nature, 620(7976):\\n1031–1036, Aug 2023. doi: 10.1038/s41586-023-06377-x. https://doi.org/10.1038/s41586-023-06377-x.\\nT. Yasar, P. Gombkoto, A. Vyssotski, et al. Months-long tracking of neuronal ensembles spanning multiple brain areas\\nwith ultra-flexible tentacle electrodes. Nature Communications, 15:4822, 2024. doi: 10.1038/s41467-024-49226-9.\\nhttps://doi.org/10.1038/s41467-024-49226-9.\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-26T01:01:22+00:00', 'source': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'file_path': '/content/drive/MyDrive/PNPL/arXiv_Brain-to-Text Decoding_ A Non-invasive Approach via Typing.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-26T01:01:22+00:00', 'trapped': '', 'modDate': 'D:20250226010122Z', 'creationDate': 'D:20250226010122Z', 'page': 14}, page_content='W. Yi, S. Qiu, K. Wang, H. Qi, L. Zhang, P. Zhou, F. He, and D. Ming. Evaluation of eeg oscillatory patterns and\\ncognitive process during simple and compound limb motor imagery. PLoS One, 9(12):e114853, Dec 2014. doi:\\n10.1371/journal.pone.0114853. https://doi.org/10.1371/journal.pone.0114853.\\nM. Zhang, J. Lévy, S. d’Ascoli, J. Rapin, F.-X. Alario, P. Bourdillon, S. Pinet, and J.-R. King. From thought to action:\\nHow the hierarchy of neural dynamics supports language production. 2025.\\nC. Zhou, Y. Tian, G. Li, et al. Through-polymer, via technology-enabled, flexible, lightweight, and integrated devices\\nfor implantable neural probes. Microsystems & Nanoengineering, 10:54, 2024. doi: 10.1038/s41378-024-00691-8.\\nhttps://doi.org/10.1038/s41378-024-00691-8.\\n15')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MZBV9VT2QX4",
        "outputId": "ac058014-2d1b-49ac-c92c-92b55edb1534"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "\n",
        "# Chunk the docs\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "vectorstore.save_local(\"faiss_index\")"
      ],
      "metadata": {
        "id": "z00MjeMO0ilE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# system prompt template\n",
        "system_template = \"\"\"You are an expert research assistant specialized in extracting, summarizing, and synthesizing information from scientific research papers.\n",
        "Use the provided context retrieved from the documents to answer accurately and concisely.\n",
        "If the information is not available in the context, politely state that the answer cannot be found.\n",
        "Always respond in a clear, professional, and informative manner suitable for academic or technical audiences.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# 2. Create system and human message prompt templates\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "\n",
        "# 3. Combine into a ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# 4. Initialize LLM with prompt\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    google_api_key=API_KEY,\n",
        "    prompt=chat_prompt  # Pass the prompt template here\n",
        ")\n",
        "\n",
        "# 5. Create RetrievalQA chain with retriever\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# 6. Run a query\n",
        "query = \"Summarize the key findings across all PDFs.\"\n",
        "result = qa_chain({\"query\": query})\n",
        "\n",
        "print(result['result'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh2H4gaK4hcO",
        "outputId": "a5369600-5152-4f07-808d-9d505189e391"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Unexpected argument 'prompt' provided to ChatGoogleGenerativeAI.\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: UserWarning: WARNING! prompt is not default parameter.\n",
            "                prompt was transferred to model_kwargs.\n",
            "                Please confirm that prompt is what you intended.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided text, the key findings related to the studies reviewed are:\n",
            "\n",
            "*   **Diverse Applications & Comparisons:** Artificial Neural Networks (ANNs) have been applied to MEG research in a wide and diverse range of ways. Many studies explicitly compare ANN-based methods to more traditional MEG analysis techniques, often to justify the adoption of neural networks.\n",
            "*   **Critical Limitations:** Despite their potential, ANNs in MEG analysis currently face several critical limitations, notably issues with interpretability and data scarcity.\n",
            "*   **Need for Advancement:** To fully leverage ANNs in MEG, further advancements are necessary, including more robust foundation models that can better handle limited datasets, enhance generalizability, and improve model transparency.\n",
            "*   **Importance of Validation & Consistency:** Validation is a key element for determining the reliability and applicability of proposed networks. However, the breadth of tasks and data types makes direct comparisons across studies challenging, highlighting a need for greater consistency in reporting training durations, regularization strategies, and evaluation metrics for future benchmarking.\n",
            "*   **Shift to Data-Driven Approaches:** MEG researchers are increasingly using a variety of data-driven approaches for feature identification, moving away from methods that rely heavily on a priori choices.\n",
            "*   **Overall Challenges:** The field faces a broad spectrum of challenges and limitations stemming from the diverse applications of ANNs in MEG research.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query List for Research Investigation\n",
        "\n",
        "\"\"\"\n",
        "- Summarize the main research objective or hypothesis of the papers.\n",
        "- List the key findings or results presented.\n",
        "- What methods or experimental techniques were used?\n",
        "- Identify and explain the main datasets or materials utilized.\n",
        "- Summarize the conclusions and implications of the studies.\n",
        "- Extract definitions of important technical terms or concepts.\n",
        "- What are the limitations or challenges mentioned?\n",
        "- Describe any future work or open questions proposed.\n",
        "- What are the applications or real-world impacts discussed?\n",
        "- List the key authors and their affiliations.\n",
        "- Compare and contrast methodologies between different papers.\n",
        "- Identify any hypotheses that were disproved or unsupported.\n",
        "- What related work or prior research is cited as important?\n",
        "- Extract any statistical results or quantitative measurements.\n",
        "- What are the ethical considerations, if any, discussed?\n",
        "- Summarize any algorithms or models introduced.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "H92NAd5C3FWq",
        "outputId": "9919c276-69d5-4313-b009-fa0dd4afc37e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n- Summarize the main research objective or hypothesis of the papers.\\n- List the key findings or results presented.\\n- What methods or experimental techniques were used?\\n- Identify and explain the main datasets or materials utilized.\\n- Summarize the conclusions and implications of the studies.\\n- Extract definitions of important technical terms or concepts.\\n- What are the limitations or challenges mentioned?\\n- Describe any future work or open questions proposed.\\n- What are the applications or real-world impacts discussed?\\n- List the key authors and their affiliations.\\n- Compare and contrast methodologies between different papers.\\n- Identify any hypotheses that were disproved or unsupported.\\n- What related work or prior research is cited as important?\\n- Extract any statistical results or quantitative measurements.\\n- What are the ethical considerations, if any, discussed?\\n- Summarize any algorithms or models introduced.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}